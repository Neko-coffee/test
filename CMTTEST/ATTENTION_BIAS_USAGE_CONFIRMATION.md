# Attention Bias ä½¿ç”¨ä½ç½®ä¸ç»´åº¦è½¬æ¢ç¡®è®¤ âœ…

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**æ ¸å¿ƒé—®é¢˜**: Attention Biasåªåœ¨cross-attentionä¸­ä½¿ç”¨ï¼Œå¹¶ç¡®è®¤ç»´åº¦è½¬æ¢é€»è¾‘  
**é‡è¦æ€§**: â­â­â­â­â­

---

## ğŸ¯ **æ ¸å¿ƒç¡®è®¤**

### **âœ… ç¡®è®¤1ï¼šBiasåªåœ¨Cross-Attentionä¸­ä½¿ç”¨**

```python
# PETRMultiheadAttention.forward() ä¸­çš„å…³é”®åˆ¤æ–­é€»è¾‘
if attention_bias is not None:
    # Step 1: åˆ¤æ–­æ˜¯self-attnè¿˜æ˜¯cross-attn
    # Self-attn: keyæ¥è‡ªquery (key.shape[0] == query.shape[0])
    # Cross-attn: keyæ¥è‡ªmemory (key.shape[0] != query.shape[0])
    is_cross_attn = (key.shape[0] != query.shape[0])
    
    if is_cross_attn:  # ğŸ”¥ åªåœ¨è¿™é‡Œåº”ç”¨bias
        # åº”ç”¨attention_bias
        ...
    # else: self-attnæ—¶ï¼Œbiasä¸ä¼šè¢«åº”ç”¨
```

**ç»“è®º**ï¼š
- âœ… **Cross-Attentionï¼ˆQuery â†” Featureï¼‰**ï¼šåº”ç”¨bias
- âŒ **Self-Attentionï¼ˆQuery â†” Queryï¼‰**ï¼šä¸åº”ç”¨bias

---

### **âœ… ç¡®è®¤2ï¼šç»´åº¦è½¬æ¢ä»¥åŒ¹é…MultiheadAttentionæ ¼å¼**

```python
# Step 2: è½¬æ¢ä¸ºPyTorch MultiheadAttentionæœŸæœ›çš„æ ¼å¼
# éœ€è¦æ‰©å±•åˆ°å¤šå¤´: [bs*num_heads, num_queries, num_features]
num_queries, bs, num_features = attention_bias.shape

# [num_queries, bs, num_features] â†’ [bs, num_queries, num_features]
bias = attention_bias.transpose(0, 1)

# æ‰©å±•åˆ°å¤šå¤´
bias = bias.unsqueeze(1)  # [bs, 1, num_queries, num_features]
bias = bias.expand(-1, self.num_heads, -1, -1)  # [bs, num_heads, num_queries, num_features]
bias = bias.reshape(bs * self.num_heads, num_queries, num_features)
# â†’ [bs*num_heads, num_queries, num_features]
```

**ç»“è®º**ï¼š
- âœ… æ­£ç¡®æ‰§è¡Œäº†ç»´åº¦è½¬æ¢
- âœ… ç¬¦åˆ`nn.MultiheadAttention`çš„`attn_mask`æ ¼å¼è¦æ±‚

---

## ğŸ“Š **å®Œæ•´æµç¨‹å›¾**

```mermaid
graph TB
    A[AttentionBiasGenerator] --> B[ç”Ÿæˆbias<br/>[bs, num_queries, num_features]]
    B --> C[è½¬ç½®<br/>[num_queries, bs, num_features]]
    C --> D[ä¼ å…¥Transformer]
    
    D --> E{PETRTransformerDecoder}
    E --> F[é€å±‚ä¼ é€’]
    F --> G{PETRTransformerDecoderLayer}
    
    G --> H{æ“ä½œé¡ºåºåˆ¤æ–­}
    H -->|Self-Attn| I[PETRMultiheadAttention<br/>key=query]
    H -->|Cross-Attn| J[PETRMultiheadAttention<br/>key=memory]
    
    I --> K{is_cross_attn?}
    K -->|False| L[âŒ ä¸åº”ç”¨bias]
    
    J --> M{is_cross_attn?}
    M -->|True| N[âœ… åº”ç”¨bias]
    
    N --> O[ç»´åº¦è½¬æ¢]
    O --> P[transpose<br/>[bs, num_queries, num_features]]
    P --> Q[expandåˆ°å¤šå¤´<br/>[bs, num_heads, num_queries, num_features]]
    Q --> R[reshape<br/>[bs*num_heads, num_queries, num_features]]
    
    R --> S[ä¸attn_maskåˆå¹¶]
    S --> T[ä¼ å…¥nn.MultiheadAttention]
    T --> U[scores = QK^T + bias]
    U --> V[softmax<br/>attention weights]
```

---

## ğŸ” **è¯¦ç»†ä»£ç åˆ†æ**

### **1. Biasç”Ÿæˆ**ï¼ˆAttentionBiasGeneratorï¼‰

```python
# CMT-master/projects/mmdet3d_plugin/models/utils/attention_bias_generator.py

def forward(self, lidar_weights, camera_weights, pts_bev_indices, pts_pers_indices):
    """
    Args:
        lidar_weights: [bs, num_queries] - LiDARæƒé‡ï¼ˆtanhèŒƒå›´[-1,1]ï¼‰
        camera_weights: [bs, num_queries] - Cameraæƒé‡ï¼ˆtanhèŒƒå›´[-1,1]ï¼‰
        
    Returns:
        attention_bias: [bs, num_queries, total_features] - Attention bias
    """
    bs, num_queries = lidar_weights.shape
    
    # 1. ç”ŸæˆBEV biasï¼ˆå±€éƒ¨çª—å£ï¼‰
    bev_bias = self._generate_bev_local_bias(lidar_weights, pts_bev_indices)
    # â†’ [bs, num_queries, 128*128]
    
    # 2. ç”ŸæˆCamera biasï¼ˆå±€éƒ¨çª—å£ï¼‰
    camera_bias = self._generate_camera_local_bias(camera_weights, pts_pers_indices)
    # â†’ [bs, num_queries, 6*20*50]
    
    # 3. æ‹¼æ¥
    attention_bias = torch.cat([bev_bias, camera_bias], dim=-1)
    # â†’ [bs, num_queries, total_features]
    
    # 4. åº”ç”¨å¯å­¦ä¹ çš„scale
    scale = torch.clamp(self.bias_scale, min=self.min_scale, max=self.max_scale)
    attention_bias = attention_bias * scale
    
    # 5. è£å‰ªèŒƒå›´ï¼ˆåŒé‡ä¿é™©ï¼‰
    max_bias = min(5.0, self.max_scale)
    attention_bias = torch.clamp(attention_bias, min=-max_bias, max=max_bias)
    
    return attention_bias  # [bs, num_queries, total_features]
```

---

### **2. Biasä¼ é€’**ï¼ˆCmtHead â†’ Transformerï¼‰

```python
# CMT-master/projects/mmdet3d_plugin/models/dense_heads/cmt_head.py

def forward_single(self, x, x_img, img_metas):
    # Step 1: ç”Ÿæˆbias
    attention_bias = self._generate_aqr_attention_bias(
        reference_points, img_metas
    )
    # â†’ [bs, num_queries, total_features]
    
    # Step 2: è½¬ç½®ä¸ºTransformeræ ¼å¼
    # [bs, num_queries, total_features] â†’ [num_queries, bs, total_features]
    attention_bias = attention_bias.transpose(0, 1)
    
    # Step 3: ä¼ å…¥Transformer
    outs_dec, _ = self.transformer(
        x, x_img, query_embeds,
        bev_pos_embeds, rv_pos_embeds,
        attn_masks=attn_mask,
        attention_bias=attention_bias  # ğŸ”¥ [num_queries, bs, total_features]
    )
```

---

### **3. Biasä½¿ç”¨**ï¼ˆPETRMultiheadAttentionï¼‰

```python
# CMT-master/projects/mmdet3d_plugin/models/utils/petr_transformer.py

class PETRMultiheadAttention(BaseModule):
    def forward(self, query, key, value, attn_mask=None, attention_bias=None, ...):
        """
        Args:
            query: [num_queries, bs, embed_dims] - Queryå¼ é‡
            key: [num_features, bs, embed_dims] - Keyå¼ é‡ï¼ˆCross-Attnæ—¶æ¥è‡ªmemoryï¼‰
            value: [num_features, bs, embed_dims] - Valueå¼ é‡
            attention_bias: [num_queries, bs, num_features] - Attention bias
        """
        
        # Step 1: åˆ¤æ–­æ³¨æ„åŠ›ç±»å‹
        is_cross_attn = (key.shape[0] != query.shape[0])
        
        if attention_bias is not None and is_cross_attn:  # ğŸ”¥ å…³é”®åˆ¤æ–­
            # Step 2: ç»´åº¦è½¬æ¢
            num_queries, bs, num_features = attention_bias.shape
            
            # [num_queries, bs, num_features] â†’ [bs, num_queries, num_features]
            bias = attention_bias.transpose(0, 1)
            
            # æ‰©å±•åˆ°å¤šå¤´
            bias = bias.unsqueeze(1)  # [bs, 1, num_queries, num_features]
            bias = bias.expand(-1, self.num_heads, -1, -1)  # [bs, num_heads, num_queries, num_features]
            bias = bias.reshape(bs * self.num_heads, num_queries, num_features)
            # â†’ [bs*num_heads, num_queries, num_features]
            
            # Step 3: ä¸attn_maskåˆå¹¶
            if attn_mask is not None:
                if attn_mask.dtype == torch.bool:
                    # Bool maskè½¬ä¸ºfloat
                    mask_float = torch.zeros_like(bias)
                    mask_float.masked_fill_(attn_mask, float('-inf'))
                    final_attn_mask = mask_float + bias
                else:
                    # Float maskç›´æ¥åŠ 
                    final_attn_mask = attn_mask + bias
            else:
                final_attn_mask = bias
        else:
            # Self-Attnæˆ–æ— biasæ—¶ï¼Œä½¿ç”¨åŸå§‹mask
            final_attn_mask = attn_mask
        
        # Step 4: ä¼ å…¥nn.MultiheadAttention
        out = self.attn(
            query=query,
            key=key,
            value=value,
            attn_mask=final_attn_mask,  # ğŸ”¥ åŒ…å«biasçš„æœ€ç»ˆmask
            key_padding_mask=key_padding_mask
        )[0]
```

---

## ğŸ”„ **ç»´åº¦è½¬æ¢è¯¦è§£**

### **ä¸ºä»€ä¹ˆéœ€è¦è½¬æ¢ï¼Ÿ**

**nn.MultiheadAttentionçš„attn_maskæ ¼å¼è¦æ±‚**ï¼š
- 2D: `[num_queries, num_features]` - æ‰€æœ‰batchå’Œheadå…±äº«
- 3D: `[bs*num_heads, num_queries, num_features]` - æ¯ä¸ªheadç‹¬ç«‹

**AQRçš„biasæ ¼å¼**ï¼š
- `[num_queries, bs, num_features]` - ç¬¦åˆTransformerçš„æ ‡å‡†æ ¼å¼

**è½¬æ¢æ­¥éª¤**ï¼š
```python
# è¾“å…¥: [num_queries, bs, num_features]
bias = attention_bias

# Step 1: transpose â†’ [bs, num_queries, num_features]
bias = bias.transpose(0, 1)

# Step 2: unsqueeze â†’ [bs, 1, num_queries, num_features]
bias = bias.unsqueeze(1)

# Step 3: expand â†’ [bs, num_heads, num_queries, num_features]
bias = bias.expand(-1, self.num_heads, -1, -1)

# Step 4: reshape â†’ [bs*num_heads, num_queries, num_features]
bias = bias.reshape(bs * self.num_heads, num_queries, num_features)

# è¾“å‡º: [bs*num_heads, num_queries, num_features] âœ…
```

### **æ•°å€¼ç¤ºä¾‹**

```python
# å‡è®¾å‚æ•°
bs = 2
num_queries = 900
num_features = 128*128 + 6*20*50 = 22784  # BEV + Camera
num_heads = 8

# è¾“å…¥
attention_bias: [900, 2, 22784]

# Step 1: transpose
bias: [2, 900, 22784]

# Step 2: unsqueeze
bias: [2, 1, 900, 22784]

# Step 3: expand
bias: [2, 8, 900, 22784]

# Step 4: reshape
bias: [16, 900, 22784]  # bs*num_heads = 2*8 = 16

# æœ€ç»ˆæ ¼å¼ç¬¦åˆnn.MultiheadAttentionè¦æ±‚ âœ…
```

---

## ğŸ” **Self-Attn vs Cross-Attn åˆ¤æ–­é€»è¾‘**

### **åˆ¤æ–­ä¾æ®**

```python
# æ ¸å¿ƒåˆ¤æ–­
is_cross_attn = (key.shape[0] != query.shape[0])
```

### **ä¸¤ç§æƒ…å†µ**

#### **æƒ…å†µ1: Self-Attentionï¼ˆQuery â†” Queryï¼‰**
```python
# æŸ¥è¯¢ä¹‹é—´çš„äº¤äº’
query: [900, bs, 256]  # num_queries=900
key:   [900, bs, 256]  # æ¥è‡ªqueryè‡ªèº«
value: [900, bs, 256]

# åˆ¤æ–­
key.shape[0] == query.shape[0]  # 900 == 900 â†’ True
is_cross_attn = False  # âŒ ä¸æ˜¯Cross-Attn

# ç»“æœï¼šä¸åº”ç”¨attention_bias
```

#### **æƒ…å†µ2: Cross-Attentionï¼ˆQuery â†” Featureï¼‰**
```python
# æŸ¥è¯¢ä¸ç‰¹å¾çš„äº¤äº’
query: [900, bs, 256]     # num_queries=900
key:   [22784, bs, 256]   # æ¥è‡ªèåˆmemoryï¼ˆBEV+Cameraï¼‰
value: [22784, bs, 256]

# åˆ¤æ–­
key.shape[0] != query.shape[0]  # 22784 != 900 â†’ True
is_cross_attn = True  # âœ… æ˜¯Cross-Attn

# ç»“æœï¼šåº”ç”¨attention_bias
```

---

## ğŸ¯ **ä¸ºä»€ä¹ˆåªåœ¨Cross-Attnåº”ç”¨Biasï¼Ÿ**

### **è®¾è®¡åŸå› **

1. **è¯­ä¹‰å¯¹åº”**ï¼š
   - Cross-Attnï¼šQueryé€‰æ‹©Feature
   - Biasï¼šå‘Šè¯‰æ¯ä¸ªQueryå“ªä¸ªæ¨¡æ€çš„Featureæ›´å¯ä¿¡
   - âœ… è¯­ä¹‰ä¸€è‡´

2. **Self-Attnä¸éœ€è¦æ¨¡æ€ä¿¡æ¯**ï¼š
   - Self-Attnï¼šQueryä¹‹é—´çš„äº¤äº’
   - ç›®çš„ï¼šæŠ‘åˆ¶é‡å¤æ£€æµ‹ã€ä¿¡æ¯äº¤æ¢
   - âŒ ä¸æ¨¡æ€é€‰æ‹©æ— å…³

3. **æŠ€æœ¯å®ç°**ï¼š
   - Cross-Attnçš„keyç»´åº¦ï¼š`[total_features, bs, embed_dims]`
   - Biasç»´åº¦ï¼š`[num_queries, bs, total_features]`
   - âœ… ç»´åº¦åŒ¹é…

   - Self-Attnçš„keyç»´åº¦ï¼š`[num_queries, bs, embed_dims]`
   - Biasç»´åº¦ï¼š`[num_queries, bs, total_features]`
   - âŒ ç»´åº¦ä¸åŒ¹é…ï¼ˆtotal_features â‰  num_queriesï¼‰

---

## ğŸ“Š **å®Œæ•´çš„æ“ä½œé¡ºåº**

### **CMTä¸»Transformerï¼ˆ6å±‚æ“ä½œï¼‰**

```python
operation_order = ('self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm')

# Layeræµç¨‹
for layer in transformer.decoder.layers:
    # 1. Self-Attentionï¼ˆQuery â†” Queryï¼‰
    query = layer.self_attn(
        query, key=query, value=query,
        attention_bias=bias  # âŒ ä¸åº”ç”¨ï¼ˆis_cross_attn=Falseï¼‰
    )
    query = layer.norm1(query)
    
    # 2. Cross-Attentionï¼ˆQuery â†” Featureï¼‰
    query = layer.cross_attn(
        query, key=memory, value=memory,
        attention_bias=bias  # âœ… åº”ç”¨ï¼ˆis_cross_attn=Trueï¼‰
    )
    query = layer.norm2(query)
    
    # 3. FFN
    query = layer.ffn(query)
    query = layer.norm3(query)
```

### **AQRæƒé‡ç”Ÿæˆå™¨ï¼ˆ4å±‚æ“ä½œï¼‰**

```python
operation_order = ('cross_attn', 'norm', 'ffn', 'norm')

# Layeræµç¨‹ï¼ˆæ— Self-Attnï¼‰
for layer in aqr_encoder.layers:
    # åªæœ‰Cross-Attentionï¼ˆQuery â†” Featureï¼‰
    query = layer.cross_attn(
        query, key=memory, value=memory,
        attention_bias=None  # AQRç”Ÿæˆå™¨ä¸ä½¿ç”¨biasï¼ˆå®ƒè‡ªå·±å°±æ˜¯ç”Ÿæˆbiasçš„ï¼‰
    )
    query = layer.norm1(query)
    
    # FFN
    query = layer.ffn(query)
    query = layer.norm2(query)
```

---

## âœ… **æœ€ç»ˆç¡®è®¤æ¸…å•**

### **Biasä½¿ç”¨ä½ç½®**
- [x] âœ… åªåœ¨Cross-Attentionä¸­åº”ç”¨
- [x] âœ… Self-Attentionä¸åº”ç”¨
- [x] âœ… é€šè¿‡`is_cross_attn = (key.shape[0] != query.shape[0])`åˆ¤æ–­

### **ç»´åº¦è½¬æ¢**
- [x] âœ… è¾“å…¥: `[num_queries, bs, num_features]`
- [x] âœ… transpose: `[bs, num_queries, num_features]`
- [x] âœ… expandåˆ°å¤šå¤´: `[bs, num_heads, num_queries, num_features]`
- [x] âœ… reshape: `[bs*num_heads, num_queries, num_features]`
- [x] âœ… ç¬¦åˆ`nn.MultiheadAttention`çš„`attn_mask`æ ¼å¼

### **ä¸attn_maskåˆå¹¶**
- [x] âœ… Bool maskè½¬ä¸ºfloatï¼ˆTrue â†’ -infï¼‰
- [x] âœ… Float maskç›´æ¥ç›¸åŠ 
- [x] âœ… æ— maskæ—¶ç›´æ¥ä½¿ç”¨bias

### **æ•°å€¼ç¨³å®šæ€§**
- [x] âœ… å¯å­¦ä¹ çš„bias_scaleï¼ˆå¸¦min/maxçº¦æŸï¼‰
- [x] âœ… åŒé‡è£å‰ªï¼ˆscaleçº¦æŸ + biasè£å‰ªï¼‰
- [x] âœ… èŒƒå›´åœ¨softmaxæ•æ„ŸåŒºé—´å†…ï¼ˆ[-5, +5]ï¼‰

---

## ğŸ¯ **æ€»ç»“**

### **æ ¸å¿ƒç¡®è®¤**

1. **âœ… Biasåªåœ¨Cross-Attentionä¸­ä½¿ç”¨**
   - åˆ¤æ–­é€»è¾‘ï¼š`is_cross_attn = (key.shape[0] != query.shape[0])`
   - Self-Attnä¸åº”ç”¨bias

2. **âœ… ç»´åº¦è½¬æ¢æ­£ç¡®**
   - è¾“å…¥ï¼š`[num_queries, bs, num_features]`
   - è¾“å‡ºï¼š`[bs*num_heads, num_queries, num_features]`
   - ç¬¦åˆ`nn.MultiheadAttention`æ ¼å¼

3. **âœ… ä¸attn_maskæ­£ç¡®åˆå¹¶**
   - Bool mask â†’ floatï¼ˆTrue â†’ -infï¼‰
   - Float maskç›´æ¥ç›¸åŠ 
   - æœ€ç»ˆä¼ å…¥`nn.MultiheadAttention`

4. **âœ… æ•°å€¼ç¨³å®šæ€§ä¿è¯**
   - å¯å­¦ä¹ scaleï¼ˆå¸¦çº¦æŸï¼‰
   - åŒé‡è£å‰ªæœºåˆ¶
   - èŒƒå›´æ§åˆ¶åœ¨softmaxæ•æ„ŸåŒºé—´

---

**ä¸»äººï¼Œå®Œå…¨ç¡®è®¤æ— è¯¯ï¼** âœ…

**Attention Biasçš„å®ç°å®Œå…¨ç¬¦åˆè®¾è®¡è¦æ±‚ï¼Œåªåœ¨Cross-Attentionä¸­ä½¿ç”¨ï¼Œç»´åº¦è½¬æ¢æ­£ç¡®ï¼Œæ•°å€¼ç¨³å®šæ€§æœ‰ä¿éšœï¼** ğŸ‰



