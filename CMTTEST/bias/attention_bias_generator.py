# Copyright (c) 2024 CMT-AQR Team. All rights reserved.
"""
Attention Bias Generator for Local Spatial Modulation
å±€éƒ¨ç©ºé—´æ³¨æ„åŠ›biasç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
1. æ ¹æ®queryçš„ç©ºé—´æŠ•å½±ä½ç½®ç”Ÿæˆå±€éƒ¨çª—å£
2. å°†per-queryæƒé‡æ‰©æ•£åˆ°å±€éƒ¨çª—å£å†…çš„ç‰¹å¾
3. ç”Ÿæˆç»†ç²’åº¦çš„attention biasçŸ©é˜µç”¨äºTransformer

è®¾è®¡æ€è·¯ï¼š
- ä¸ç›´æ¥ä¿®æ”¹ç‰¹å¾å›¾ï¼Œè€Œæ˜¯å½±å“attentionè®¡ç®—
- ä¿æŒç‰¹å¾è¯­ä¹‰ä¸å˜ï¼Œåªè°ƒæ•´queryå¯¹ä¸åŒåŒºåŸŸçš„å…³æ³¨ç¨‹åº¦
- ä½¿ç”¨å±€éƒ¨çª—å£è€Œéå…¨å±€ï¼Œæä¾›ç©ºé—´å…ˆéªŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from mmcv.runner import BaseModule, force_fp32
import numpy as np


class AttentionBiasGenerator(BaseModule):
    """
    å±€éƒ¨æ³¨æ„åŠ›Biasç”Ÿæˆå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è¾“å…¥ï¼šper-queryæƒé‡ + ç©ºé—´ä½ç½®
    - è¾“å‡ºï¼š[bs, num_queries, num_features] çš„biasçŸ©é˜µ
    
    å®ç°ç­–ç•¥ï¼š
    - å‘é‡åŒ–è®¡ç®—ï¼Œé¿å…å¾ªç¯
    - å±€éƒ¨çª—å£æ§åˆ¶biasèŒƒå›´
    - æ”¯æŒBEVå’ŒCameraä¸¤ç§ç‰¹å¾å›¾
    """
    
    def __init__(self,
                 bev_feature_shape=(180, 180),
                 pers_feature_shape=(6, 40, 100),
                 window_size=15,
                 bias_scale=2.5,
                 learnable_scale=False,
                 min_scale=0.5,
                 max_scale=5.0,
                 use_local_bias=True,
                 fp16=False,
                 init_cfg=None):
        """
        Args:
            bev_feature_shape (tuple): BEVç‰¹å¾å›¾å°ºå¯¸ (H, W)
            pers_feature_shape (tuple): é€è§†ç‰¹å¾å›¾å°ºå¯¸ (num_views, H, W)
            window_size (int): å±€éƒ¨çª—å£å¤§å°ï¼ˆæ­£æ–¹å½¢çª—å£çš„è¾¹é•¿ï¼‰
            bias_scale (float): biasç¼©æ”¾å› å­çš„åˆå§‹å€¼
            learnable_scale (bool): æ˜¯å¦è®©bias_scaleå¯å­¦ä¹ 
            min_scale (float): bias_scaleçš„æœ€å°å€¼ï¼ˆé˜²æ­¢é€€åŒ–ï¼‰
            max_scale (float): bias_scaleçš„æœ€å¤§å€¼ï¼ˆé˜²æ­¢softmaxé¥±å’Œï¼‰
            use_local_bias (bool): True=å±€éƒ¨çª—å£bias, False=å…¨å±€bias
            fp16 (bool): æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦å­˜å‚¨biasçŸ©é˜µ
            init_cfg (dict): åˆå§‹åŒ–é…ç½®
        """
        super(AttentionBiasGenerator, self).__init__(init_cfg=init_cfg)
        
        self.bev_h, self.bev_w = bev_feature_shape
        self.num_views, self.pers_h, self.pers_w = pers_feature_shape
        self.window_size = window_size
        self.use_local_bias = use_local_bias
        self.fp16 = fp16
        self.learnable_scale = learnable_scale
        self.min_scale = min_scale
        self.max_scale = max_scale
        
        # ğŸ”¥ å¯å­¦ä¹ çš„bias_scale
        if learnable_scale:
            self.bias_scale = nn.Parameter(torch.tensor(bias_scale))
        else:
            self.register_buffer('bias_scale', torch.tensor(bias_scale))
        
        # é¢„è®¡ç®—çª—å£åç§»é‡ï¼ˆåŠ é€Ÿï¼‰
        self._init_window_offsets()
        
        print(f"âœ… AttentionBiasGenerator initialized:")
        print(f"   BEV shape: {bev_feature_shape}")
        print(f"   Pers shape: {pers_feature_shape}")
        print(f"   Window size: {window_size} ({'local' if use_local_bias else 'global'})")
        print(f"   Bias scale: {bias_scale} ({'learnable' if learnable_scale else 'fixed'})")
        if learnable_scale:
            print(f"   Scale range: [{min_scale}, {max_scale}]")
        print(f"   FP16: {fp16}")
    
    def _init_window_offsets(self):
        """é¢„è®¡ç®—çª—å£åç§»é‡"""
        half_window = self.window_size // 2
        offsets = torch.arange(-half_window, half_window + 1)
        
        # 2Dç½‘æ ¼åç§»ï¼ˆç”¨äºBEVï¼‰
        y_offsets, x_offsets = torch.meshgrid(offsets, offsets, indexing='ij')
        self.register_buffer('y_offsets', y_offsets.reshape(-1))  # [window_size^2]
        self.register_buffer('x_offsets', x_offsets.reshape(-1))  # [window_size^2]
        
        # 1Dç´¢å¼•åç§»ï¼ˆç”¨äºå±•å¹³åçš„ç‰¹å¾ï¼‰
        window_offsets_bev = y_offsets * self.bev_w + x_offsets
        self.register_buffer('window_offsets_bev', window_offsets_bev.reshape(-1))  # [window_size^2]
        
        window_offsets_pers = y_offsets * self.pers_w + x_offsets
        self.register_buffer('window_offsets_pers', window_offsets_pers.reshape(-1))  # [window_size^2]
    
    @force_fp32(apply_to=('lidar_weights', 'camera_weights'))
    def forward(self,
                lidar_weights,      # [bs, num_queries] LiDARæ¨¡æ€æƒé‡
                camera_weights,     # [bs, num_queries] Cameraæ¨¡æ€æƒé‡
                pts_bev_indices,    # [bs, num_queries] BEVç‰¹å¾å›¾ä½ç½®ç´¢å¼•
                pts_pers_indices):  # [bs, num_queries, 3] é€è§†ç‰¹å¾å›¾ä½ç½®ç´¢å¼• (view, h, w)
        """
        ç”Ÿæˆå±€éƒ¨attention bias
        
        Args:
            lidar_weights: [bs, num_queries] AQRç”Ÿæˆçš„LiDARæƒé‡
            camera_weights: [bs, num_queries] AQRç”Ÿæˆçš„Cameraæƒé‡
            pts_bev_indices: [bs, num_queries] queryåœ¨BEVç‰¹å¾å›¾ä¸­çš„ä½ç½®ï¼ˆ1Dç´¢å¼•ï¼‰
            pts_pers_indices: [bs, num_queries, 3] queryåœ¨é€è§†ç‰¹å¾å›¾ä¸­çš„ä½ç½®ï¼ˆview, h, wï¼‰
        
        Returns:
            attention_bias: [bs, num_queries, total_features]
                å…¶ä¸­ total_features = bev_h*bev_w + num_views*pers_h*pers_w
                
        ç¤ºä¾‹ï¼š
            bs=2, num_queries=900
            bev_features = 180*180 = 32400
            pers_features = 6*40*100 = 24000
            total_features = 56400
            
            è¾“å‡ºï¼š[2, 900, 56400]
        """
        batch_size, num_queries = lidar_weights.shape
        device = lidar_weights.device
        
        # 1. ç”ŸæˆBEV bias
        bev_bias = self._generate_bev_bias(
            lidar_weights,      # [bs, num_queries]
            pts_bev_indices     # [bs, num_queries]
        )  # â†’ [bs, num_queries, bev_h*bev_w]
        
        # 2. ç”ŸæˆCamera bias
        camera_bias = self._generate_camera_bias(
            camera_weights,     # [bs, num_queries]
            pts_pers_indices    # [bs, num_queries, 3]
        )  # â†’ [bs, num_queries, num_views*pers_h*pers_w]
        
        # 3. æ‹¼æ¥æˆå®Œæ•´çš„biasçŸ©é˜µ
        attention_bias = torch.cat([bev_bias, camera_bias], dim=-1)
        # â†’ [bs, num_queries, total_features]
        
        # 4. ğŸ”¥ åº”ç”¨ç¼©æ”¾å› å­ï¼ˆå¸¦çº¦æŸï¼‰
        if self.learnable_scale:
            # ğŸ”¥ Step 1: Clamp scaleåˆ°å®‰å…¨èŒƒå›´
            scale = torch.clamp(self.bias_scale, min=self.min_scale, max=self.max_scale)
            
            # ğŸ”¥ Step 2: ç›‘æ§scaleï¼ˆè®­ç»ƒæ—¶å¶å°”æ‰“å°ï¼‰
            if self.training and torch.rand(1).item() < 0.001:  # 0.1%æ¦‚ç‡
                current_scale = scale.item()
                print(f"ğŸ“Š Bias scale: {current_scale:.3f} (range: [{self.min_scale}, {self.max_scale}])")
                if current_scale > 0.9 * self.max_scale:
                    print(f"   âš ï¸ Scaleæ¥è¿‘ä¸Šé™ï¼")
        else:
            scale = self.bias_scale
        
        attention_bias = attention_bias * scale
        # ğŸ”¥ æ³¨æ„ï¼šæƒé‡ç°åœ¨æ˜¯[-1, 1]ï¼Œæ‰€ä»¥biasæ˜¯[-scale, +scale]
        
        # 5. ğŸ”¥ åŒé‡ä¿é™©ï¼šè£å‰ªæœ€ç»ˆbiasèŒƒå›´
        # å³ä½¿scaleè¢«çº¦æŸï¼Œä»ç„¶clampä¸€æ¬¡ç¡®ä¿ä¸ä¼šè¶…å‡ºsoftmaxæ•æ„ŸåŒºé—´
        # Softmaxæ•æ„ŸåŒºé—´ï¼š[-3, +3]æœ€ä¼˜ï¼Œ[-5, +5]å®‰å…¨
        max_bias = min(5.0, self.max_scale)  # å–max_scaleå’Œ5.0çš„è¾ƒå°å€¼
        attention_bias = torch.clamp(attention_bias, min=-max_bias, max=max_bias)
        
        # 6. è½¬æ¢ä¸ºfp16ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.fp16:
            attention_bias = attention_bias.half()
        
        return attention_bias
    
    def _generate_bev_bias(self, weights, positions):
        """
        ç”ŸæˆBEVç‰¹å¾å›¾çš„å±€éƒ¨bias
        
        Args:
            weights: [bs, num_queries] æƒé‡
            positions: [bs, num_queries] 1Dä½ç½®ç´¢å¼•
        
        Returns:
            bias: [bs, num_queries, bev_h*bev_w]
        """
        batch_size, num_queries = weights.shape
        device = weights.device
        total_features = self.bev_h * self.bev_w
        
        if not self.use_local_bias:
            # å…¨å±€biasï¼šæ¯ä¸ªqueryå¯¹æ‰€æœ‰BEVç‰¹å¾æ–½åŠ ç›¸åŒbias
            return weights.unsqueeze(-1).expand(batch_size, num_queries, total_features)
        
        # === å±€éƒ¨biasï¼šå‘é‡åŒ–å®ç° ===
        
        # 1. è®¡ç®—æ‰€æœ‰queryçš„å±€éƒ¨çª—å£ç´¢å¼•
        # positions: [bs, num_queries]
        # window_offsets_bev: [window_size^2]
        query_indices = positions.unsqueeze(-1) + self.window_offsets_bev.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        # 2. è¾¹ç•Œæ£€æŸ¥ï¼ˆ2Dç½‘æ ¼ï¼‰
        query_y = positions // self.bev_w  # [bs, num_queries]
        query_x = positions % self.bev_w   # [bs, num_queries]
        
        window_y = query_y.unsqueeze(-1) + self.y_offsets.unsqueeze(0).unsqueeze(0)
        window_x = query_x.unsqueeze(-1) + self.x_offsets.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        valid_y = (window_y >= 0) & (window_y < self.bev_h)
        valid_x = (window_x >= 0) & (window_x < self.bev_w)
        valid_mask = valid_y & valid_x  # [bs, num_queries, window_size^2]
        
        # 3. åˆ›å»ºbiasçŸ©é˜µ
        bias = torch.zeros(batch_size, num_queries, total_features, 
                          device=device, dtype=weights.dtype)
        
        # 4. æ‰©å±•æƒé‡åˆ°çª—å£
        weights_expanded = weights.unsqueeze(-1).expand(-1, -1, self.window_size**2)
        # â†’ [bs, num_queries, window_size^2]
        
        # 5. åº”ç”¨valid mask
        weights_masked = torch.where(valid_mask, weights_expanded, 
                                     torch.zeros_like(weights_expanded))
        
        # 6. å‘é‡åŒ–å¡«å……ï¼ˆä½¿ç”¨scatter_addï¼‰
        # å°†çª—å£å†…çš„ç´¢å¼•clipåˆ°æœ‰æ•ˆèŒƒå›´
        query_indices_clamped = query_indices.clamp(0, total_features - 1)
        
        # é€batchå¤„ç†ï¼ˆé¿å…scatter_addçš„ç»´åº¦é—®é¢˜ï¼‰
        for b in range(batch_size):
            bias[b].scatter_add_(
                dim=1,  # åœ¨featureç»´åº¦scatter
                index=query_indices_clamped[b],  # [num_queries, window_size^2]
                src=weights_masked[b]             # [num_queries, window_size^2]
            )
        
        return bias
    
    def _generate_camera_bias(self, weights, positions):
        """
        ç”ŸæˆCameraé€è§†ç‰¹å¾å›¾çš„å±€éƒ¨bias
        
        Args:
            weights: [bs, num_queries] æƒé‡
            positions: [bs, num_queries, 3] 3Dä½ç½®ç´¢å¼• (view, h, w)
        
        Returns:
            bias: [bs, num_queries, num_views*pers_h*pers_w]
        """
        batch_size, num_queries = weights.shape
        device = weights.device
        total_features = self.num_views * self.pers_h * self.pers_w
        
        if not self.use_local_bias:
            # å…¨å±€bias
            return weights.unsqueeze(-1).expand(batch_size, num_queries, total_features)
        
        # === å±€éƒ¨biasï¼šå‘é‡åŒ–å®ç° ===
        
        # 1. è§£æ3Dä½ç½®
        view_indices = positions[..., 0].long()  # [bs, num_queries]
        h_indices = positions[..., 1].long()     # [bs, num_queries]
        w_indices = positions[..., 2].long()     # [bs, num_queries]
        
        # 2. è®¡ç®—1Dç´¢å¼•
        positions_1d = (view_indices * self.pers_h * self.pers_w + 
                       h_indices * self.pers_w + 
                       w_indices)  # [bs, num_queries]
        
        # 3. è®¡ç®—çª—å£ç´¢å¼•ï¼ˆåªåœ¨åŒä¸€è§†è§’å†…ï¼‰
        query_indices = positions_1d.unsqueeze(-1) + self.window_offsets_pers.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        # 4. è¾¹ç•Œæ£€æŸ¥ï¼ˆ2Dç½‘æ ¼ + è§†è§’ä¸€è‡´æ€§ï¼‰
        window_h = h_indices.unsqueeze(-1) + self.y_offsets.unsqueeze(0).unsqueeze(0)
        window_w = w_indices.unsqueeze(-1) + self.x_offsets.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        valid_h = (window_h >= 0) & (window_h < self.pers_h)
        valid_w = (window_w >= 0) & (window_w < self.pers_w)
        
        # ç¡®ä¿çª—å£ä¸è·¨è§†è§’ï¼ˆæ£€æŸ¥çª—å£ç´¢å¼•æ˜¯å¦åœ¨åŒä¸€è§†è§’å†…ï¼‰
        window_view = query_indices // (self.pers_h * self.pers_w)
        valid_view = (window_view == view_indices.unsqueeze(-1))
        
        valid_mask = valid_h & valid_w & valid_view  # [bs, num_queries, window_size^2]
        
        # 5. åˆ›å»ºbiasçŸ©é˜µ
        bias = torch.zeros(batch_size, num_queries, total_features,
                          device=device, dtype=weights.dtype)
        
        # 6. æ‰©å±•æƒé‡åˆ°çª—å£
        weights_expanded = weights.unsqueeze(-1).expand(-1, -1, self.window_size**2)
        weights_masked = torch.where(valid_mask, weights_expanded,
                                     torch.zeros_like(weights_expanded))
        
        # 7. å‘é‡åŒ–å¡«å……
        query_indices_clamped = query_indices.clamp(0, total_features - 1)
        
        for b in range(batch_size):
            bias[b].scatter_add_(
                dim=1,
                index=query_indices_clamped[b],
                src=weights_masked[b]
            )
        
        return bias
    
    def get_memory_usage(self, batch_size, num_queries):
        """
        ä¼°ç®—å†…å­˜å ç”¨
        
        Args:
            batch_size: batchå¤§å°
            num_queries: queryæ•°é‡
        
        Returns:
            dict: å†…å­˜ä½¿ç”¨ä¿¡æ¯
        """
        total_features = self.bev_h * self.bev_w + self.num_views * self.pers_h * self.pers_w
        
        # biasçŸ©é˜µå¤§å°
        bias_elements = batch_size * num_queries * total_features
        bias_memory_fp32 = bias_elements * 4 / (1024**2)  # MB
        bias_memory_fp16 = bias_elements * 2 / (1024**2)  # MB
        
        return {
            'total_features': total_features,
            'bias_shape': (batch_size, num_queries, total_features),
            'memory_fp32_mb': bias_memory_fp32,
            'memory_fp16_mb': bias_memory_fp16,
            'current_dtype': 'fp16' if self.fp16 else 'fp32',
            'estimated_memory_mb': bias_memory_fp16 if self.fp16 else bias_memory_fp32
        }


# === é»˜è®¤é…ç½® ===
DEFAULT_ATTENTION_BIAS_CONFIG = dict(
    type='AttentionBiasGenerator',
    bev_feature_shape=(180, 180),
    pers_feature_shape=(6, 40, 100),
    window_size=15,                # æ¨èå€¼ï¼šä¸LAMçš„camera windowä¸€è‡´
    bias_scale=1.0,                # åˆå§‹å»ºè®®ï¼š1.0ï¼ˆæ— ç¼©æ”¾ï¼‰
    use_local_bias=True,           # æ¨èï¼šTrueï¼ˆå±€éƒ¨biasï¼‰
    fp16=True                      # æ¨èï¼šTrueï¼ˆèŠ‚çœå†…å­˜ï¼‰
)

