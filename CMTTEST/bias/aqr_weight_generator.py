# ------------------------------------------------------------------------
# AQR Weight Generator - åŸºäºMoMEçš„AQRæ”¹é€ ä¸ºæƒé‡å›¾æ¸²æŸ“æœºåˆ¶
# æ ¸å¿ƒä¿®æ”¹ï¼šä»ç¦»æ•£æ¨¡æ€é€‰æ‹©æ”¹ä¸ºè¿ç»­æƒé‡ç”Ÿæˆ
# ------------------------------------------------------------------------
import copy
import numpy as np
import torch
import torch.nn as nn
import warnings
from einops import rearrange
from mmcv.cnn import xavier_init
from mmcv.cnn.bricks.transformer import build_transformer_layer_sequence
from mmcv.runner.base_module import BaseModule
from mmdet.models.utils.builder import TRANSFORMER


@TRANSFORMER.register_module()
class AQRWeightGenerator(BaseModule):
    """
    AQRæƒé‡ç”Ÿæˆå™¨ - åŸºäºMoMEçš„AQRæœºåˆ¶æ”¹é€ 
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ä¿ç•™MoMEçš„3DæŠ•å½±å’Œå±€éƒ¨æ³¨æ„åŠ›æ©ç ï¼ˆLAMï¼‰æœºåˆ¶
    2. å°†ç¦»æ•£çš„æ¨¡æ€é€‰æ‹©æ”¹ä¸ºè¿ç»­çš„æƒé‡ç”Ÿæˆ
    3. ä¸ºæ¯ä¸ªQueryç”ŸæˆLiDARå’ŒCameraçš„æƒé‡å€¼
    
    Args:
        embed_dims (int): åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤256
        encoder_config (dict): ç¼–ç å™¨é…ç½®
        window_sizes (list): å±€éƒ¨æ³¨æ„åŠ›çª—å£å¤§å° [camera_window, lidar_window]
        use_type_embed (bool): æ˜¯å¦ä½¿ç”¨æ¨¡æ€ç±»å‹åµŒå…¥
        pc_range (list): ç‚¹äº‘èŒƒå›´
    """
    
    def __init__(self,
                 embed_dims=256,
                 encoder_config=None,
                 window_sizes=[15, 5],
                 use_type_embed=True,
                 pc_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0],
                 bev_feature_shape=(180, 180),  # ğŸ”¥ BEVç‰¹å¾å›¾å°ºå¯¸ï¼ˆå¯é…ç½®ï¼‰
                 pers_feature_shape=(6, 40, 100),  # ğŸ”¥ é€è§†ç‰¹å¾å›¾å°ºå¯¸ï¼ˆå¯é…ç½®ï¼‰
                 init_cfg=None):
        super(AQRWeightGenerator, self).__init__(init_cfg=init_cfg)
        
        self.embed_dims = embed_dims
        self.window_sizes = window_sizes
        self.use_type_embed = use_type_embed
        self.pc_range = pc_range
        self.bev_feature_shape = bev_feature_shape  # ğŸ”¥ ä¿å­˜BEVå°ºå¯¸
        self.pers_feature_shape = pers_feature_shape  # ğŸ”¥ ä¿å­˜é€è§†å°ºå¯¸
        
        # ğŸ”¥ ä»é…ç½®ä¸­æå–å°ºå¯¸å‚æ•°
        self.bev_h, self.bev_w = bev_feature_shape
        self.num_views, self.pers_h, self.pers_w = pers_feature_shape
        
        # æ„å»ºç¼–ç å™¨ï¼ˆä¿ç•™MoMEçš„ç»“æ„ï¼‰
        if encoder_config is not None:
            self.encoder = build_transformer_layer_sequence(encoder_config)
            self.e_num_heads = self.encoder.layers[0].attentions[0].num_heads
        else:
            raise ValueError("encoder_config is required for AQRWeightGenerator")
            
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä»3ç±»åˆ†ç±»æ”¹ä¸º2ä¸ªè¿ç»­æƒé‡è¾“å‡º
        self.weight_predictor = nn.Linear(embed_dims, 2)  # [lidar_weight, camera_weight]
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šåˆå§‹åŒ–åç½®ä½¿åˆå§‹æƒé‡æ¥è¿‘0.82ï¼ˆæ¸©å’Œå¢å¼ºï¼‰
        # sigmoid(1.5) â‰ˆ 0.82 â†’ é…åˆresidual=0.7ï¼ŒåˆæœŸå¢å¼º40%ï¼Œä¿æŠ¤é¢„è®­ç»ƒç‰¹å¾
        nn.init.constant_(self.weight_predictor.bias, 1.5)  
        
        # ç±»å‹åµŒå…¥ï¼ˆä¿ç•™MoMEè®¾è®¡ï¼‰
        if self.use_type_embed:
            self.bev_type_embed = nn.Parameter(torch.randn(embed_dims))
            self.rv_type_embed = nn.Parameter(torch.randn(embed_dims))
    
    def init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if hasattr(m, 'weight') and m.weight.dim() > 1:
                xavier_init(m, distribution='uniform')
        
        # ğŸ”¥ å†æ¬¡ç¡®ä¿weight_predictorçš„åç½®åˆå§‹åŒ–æ­£ç¡®
        if hasattr(self, 'weight_predictor'):
            nn.init.constant_(self.weight_predictor.bias, 1.5)
        
        self._is_init = True
    
    def project_3d_to_features(self, ref_points, img_metas):
        """
        3Då‚è€ƒç‚¹æŠ•å½±åˆ°ç‰¹å¾å›¾åæ ‡
        ä¿ç•™MoMEçš„ç²¾ç¡®æŠ•å½±é€»è¾‘
        
        Args:
            ref_points: [bs, num_queries, 3] å½’ä¸€åŒ–çš„3Då‚è€ƒç‚¹
            img_metas: å›¾åƒå…ƒæ•°æ®
            
        Returns:
            pts_bev: [bs, num_queries, 2] BEVç‰¹å¾å›¾åæ ‡ (y, x)
            pts_pers: [bs, num_queries, 3] é€è§†ç‰¹å¾å›¾åæ ‡ (view, h, w)
            pts_idx: [bs, num_queries] BEVç‰¹å¾å›¾ç´¢å¼•
            pts_pers_idx: [bs, num_queries] é€è§†ç‰¹å¾å›¾ç´¢å¼•
        """
        # åå½’ä¸€åŒ–åˆ°çœŸå®3Dåæ ‡
        _ref_points = ref_points.new_zeros(ref_points.shape)
        _ref_points[..., 0:1] = ref_points[..., 0:1] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
        _ref_points[..., 1:2] = ref_points[..., 1:2] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
        _ref_points[..., 2:3] = ref_points[..., 2:3] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2]
        
        # è·å–ç›¸æœºæŠ•å½±çŸ©é˜µ
        _matrices = np.stack([np.stack(i['lidar2img']) for i in img_metas])
        _matrices = torch.tensor(_matrices).float().to(ref_points.device)
        batch, _num_queries, _ = _ref_points.shape
        
        # 3D â†’ 2D é€è§†æŠ•å½±
        _position_4d = torch.cat([_ref_points, torch.ones((batch, _num_queries, 1), device=ref_points.device)], dim=-1)
        pts_2d = torch.einsum('bni,bvij->bvnj', _position_4d, _matrices.transpose(2,3))
        pts_2d[..., 2] = torch.clip(pts_2d[..., 2], min=1e-5, max=99999)
        pts_2d[..., 0] /= pts_2d[..., 2]
        pts_2d[..., 1] /= pts_2d[..., 2]
        
        # FOVæ£€æŸ¥
        fov_inds = ((pts_2d[..., 0] < img_metas[0]['img_shape'][0][1]) &
                   (pts_2d[..., 0] >= 0) &
                   (pts_2d[..., 1] < img_metas[0]['img_shape'][0][0]) &
                   (pts_2d[..., 1] >= 0))
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆè§†è§’
        _, num_views, _, _ = pts_2d.shape
        _fov_inds = torch.cat([torch.full((fov_inds.shape[0], 1, fov_inds.shape[-1]), False, device=ref_points.device), fov_inds], dim=1)
        first_valid_view = _fov_inds.to(torch.float32).argmax(dim=1)
        first_valid_view[first_valid_view == 0] = -1
        first_valid_view[first_valid_view != -1] -= 1
        
        # æ„å»ºé€è§†ç‰¹å¾åæ ‡
        batch_indices = torch.arange(batch, device=ref_points.device).unsqueeze(1).expand(-1, _num_queries)
        point_indices = torch.arange(_num_queries, device=ref_points.device).unsqueeze(0).expand(batch, -1)
        mask = first_valid_view != -1
        
        selected_pts = pts_2d[batch_indices[mask], first_valid_view[mask], point_indices[mask]]
        pts_pers = torch.full((batch, _num_queries, 2), float('nan'), device=ref_points.device)
        pts_pers[batch_indices[mask], point_indices[mask]] = selected_pts[..., [1, 0]]  # H,W
        
        # ç‰¹å¾å›¾å°ºåº¦è°ƒæ•´ï¼ˆğŸ”¥ ä½¿ç”¨é…ç½®çš„å°ºå¯¸ï¼Œè‡ªåŠ¨é€‚åº”ä¸åŒåˆ†è¾¨ç‡ï¼‰
        ori_H, ori_W = img_metas[0]['img_shape'][0][:2]
        feat_H, feat_W = self.pers_h, self.pers_w  # ğŸ”¥ ä»é…ç½®è·å–ï¼Œè€Œéç¡¬ç¼–ç 
        ratio = torch.tensor(feat_H / ori_H, device=ref_points.device)
        pts_pers = torch.cat([first_valid_view.unsqueeze(-1), pts_pers], dim=-1)
        pts_pers[:, :, 1:] = torch.floor(pts_pers[:, :, 1:] * ratio)
        pts_pers[pts_pers[:, :, 0] == -1] = 0.0
        
        # BEVæŠ•å½±ï¼ˆä½¿ç”¨å¯é…ç½®çš„BEVå°ºå¯¸ï¼‰
        pts_bev = torch.floor((_ref_points[..., :2] + 54.0) * (self.bev_h / 108))[:, :, [1, 0]]  # y,x
        
        # è®¡ç®—ç´¢å¼•ï¼ˆä½¿ç”¨å¯é…ç½®çš„å°ºå¯¸ï¼‰
        pts_idx = pts_bev[:, :, 0] * self.bev_w + pts_bev[:, :, 1]
        pts_pers_idx = pts_pers[:, :, 0] * self.pers_h * self.pers_w + pts_pers[:, :, 1] * self.pers_w + pts_pers[:, :, 2]
        
        return pts_bev, pts_pers, pts_idx, pts_pers_idx
    
    def generate_local_attention_masks(self, pts_idx, pts_pers_idx):
        """
        ç”Ÿæˆå±€éƒ¨æ³¨æ„åŠ›æ©ç ï¼ˆLAMï¼‰
        ä¿ç•™MoMEçš„ç²¾ç¡®æ©ç ç”Ÿæˆé€»è¾‘
        
        Args:
            pts_idx: [bs, num_queries] BEVç‰¹å¾å›¾ç´¢å¼•
            pts_pers_idx: [bs, num_queries] é€è§†ç‰¹å¾å›¾ç´¢å¼•
            
        Returns:
            fusion_attention_mask: [bs*num_heads, num_queries, total_elements] èåˆæ³¨æ„åŠ›æ©ç 
        """
        batch_size, num_queries = pts_idx.shape
        
        # === Cameraæ³¨æ„åŠ›æ©ç ï¼ˆä½¿ç”¨å¯é…ç½®å°ºå¯¸ï¼‰===
        total_elements_cam = self.num_views * self.pers_h * self.pers_w
        H, W = self.pers_h, self.pers_w
        stride_view = self.pers_h * self.pers_w
        stride_h = self.pers_w
        window_size = self.window_sizes[0]  # camera window
        
        # ç”Ÿæˆçª—å£åç§»
        offsets = torch.arange(-(window_size // 2), window_size // 2 + 1, device=pts_idx.device)
        window_offsets = offsets.unsqueeze(1) * stride_h + offsets.unsqueeze(0)
        window_offsets = window_offsets.view(-1)
        
        # åº”ç”¨çª—å£åç§»
        indices = pts_pers_idx.unsqueeze(-1) + window_offsets.unsqueeze(0).unsqueeze(0)
        
        # æœ‰æ•ˆæ€§æ£€æŸ¥
        query_rows = (pts_pers_idx % (H * W)) // W
        query_cols = pts_pers_idx % W
        index_rows = (indices % (H * W)) // W
        index_cols = indices % W
        
        valid_row = (index_rows - query_rows.unsqueeze(-1)).abs() <= window_size // 2
        valid_column = (index_cols - query_cols.unsqueeze(-1)).abs() <= window_size // 2
        valid = valid_row & valid_column
        
        indices = torch.clamp(indices, 0, total_elements_cam - 1).long()
        
        # ç”Ÿæˆcameraæ³¨æ„åŠ›æ©ç 
        img_attention_mask = torch.ones(batch_size, num_queries, total_elements_cam, dtype=torch.bool, device=pts_idx.device)
        valid_indices = valid.nonzero(as_tuple=True)
        batch_indices = torch.arange(batch_size, device=pts_idx.device).long().view(-1, 1, 1)
        query_indices_range = torch.arange(num_queries, device=pts_idx.device).long().view(1, -1, 1)
        
        img_attention_mask[
            batch_indices.expand_as(indices)[valid_indices[0], valid_indices[1], valid_indices[2]],
            query_indices_range.expand_as(indices)[valid_indices[0], valid_indices[1], valid_indices[2]],
            indices[valid_indices]
        ] = False
        
        # === LiDARæ³¨æ„åŠ›æ©ç ï¼ˆä½¿ç”¨å¯é…ç½®å°ºå¯¸ï¼‰===
        total_elements_lidar = self.bev_h * self.bev_w
        row_stride = self.bev_w
        window_size = self.window_sizes[1]  # lidar window
        
        offsets = torch.arange(-(window_size // 2), window_size // 2 + 1, device=pts_idx.device)
        # ğŸ”¥ å…¼å®¹PyTorch 1.9ï¼šä¸ä½¿ç”¨indexingå‚æ•°
        y_offsets, x_offsets = torch.meshgrid(offsets, offsets)
        window_offsets = (y_offsets * row_stride + x_offsets).reshape(-1)
        
        indices = pts_idx.unsqueeze(-1) + window_offsets.unsqueeze(0).unsqueeze(0)
        valid_indices = (indices >= 0) & (indices < total_elements_lidar)
        
        # è¾¹ç•Œæ£€æŸ¥
        query_columns = pts_idx % row_stride
        window_columns = (indices % row_stride).float() - query_columns.unsqueeze(-1).float()
        valid_indices &= (window_columns.abs() <= window_size // 2)
        
        # ç”Ÿæˆlidaræ³¨æ„åŠ›æ©ç 
        lidar_attention_mask = torch.ones(batch_size, num_queries, total_elements_lidar, dtype=torch.bool, device=pts_idx.device)
        
        batch_indices = torch.arange(batch_size, device=pts_idx.device).view(-1, 1, 1).expand_as(indices)
        query_indices_range = torch.arange(num_queries, device=pts_idx.device).view(1, -1, 1).expand_as(indices)
        
        valid_mask = valid_indices & (indices < total_elements_lidar)
        lidar_attention_mask[
            batch_indices[valid_mask],
            query_indices_range[valid_mask],
            indices[valid_mask].long()
        ] = False
        
        # èåˆæ©ç 
        fusion_attention_mask = torch.cat([lidar_attention_mask, img_attention_mask], dim=-1)
        fusion_attention_mask = fusion_attention_mask.unsqueeze(1).repeat(1, self.e_num_heads, 1, 1).flatten(0, 1)
        
        return fusion_attention_mask
    
    def forward(self, query_embed, memory, pos_embed, ref_points, img_metas, reg_branch=None):
        """
        å‰å‘ä¼ æ’­ï¼šç”Ÿæˆæ¯ä¸ªQueryçš„LiDARå’ŒCameraæƒé‡
        
        Args:
            query_embed: [num_queries, bs, embed_dims] æŸ¥è¯¢åµŒå…¥
            memory: [total_elements, bs, embed_dims] è®°å¿†ç‰¹å¾
            pos_embed: [total_elements, bs, embed_dims] ä½ç½®ç¼–ç 
            ref_points: [bs, num_queries, 3] å½’ä¸€åŒ–å‚è€ƒç‚¹
            img_metas: å›¾åƒå…ƒæ•°æ®
            reg_branch: å›å½’åˆ†æ”¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            lidar_weights: [bs, num_queries] LiDARæ¨¡æ€æƒé‡ [0, 1]
            camera_weights: [bs, num_queries] Cameraæ¨¡æ€æƒé‡ [0, 1]
            weight_loss: æƒé‡æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰
            projection_info: dict æŠ•å½±ä¿¡æ¯ï¼ŒåŒ…å«pts_bev, pts_persç­‰
        """
        # 1. 3DæŠ•å½±å’Œä½ç½®æ˜ å°„
        pts_bev, pts_pers, pts_idx, pts_pers_idx = self.project_3d_to_features(ref_points, img_metas)
        
        # 2. ç”Ÿæˆå±€éƒ¨æ³¨æ„åŠ›æ©ç 
        fusion_attention_mask = self.generate_local_attention_masks(pts_idx, pts_pers_idx)
        
        # 3. ç¼–ç å™¨å¤„ç†ï¼ˆä¿ç•™MoMEé€»è¾‘ï¼‰
        target = torch.zeros_like(query_embed)
        target = self.encoder(
            query=target,
            key=memory,
            value=memory,
            query_pos=query_embed,
            key_pos=pos_embed,
            attn_masks=[fusion_attention_mask],
            reg_branch=reg_branch
        )
        
        # è·å–æœ€åä¸€å±‚è¾“å‡º
        if target.shape[0] == 0:
            target = target.squeeze(0).transpose(1, 0)
        else:
            target = target[-1].transpose(1, 0)  # [bs, num_queries, embed_dims]
        
        # 4. ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç”Ÿæˆè¿ç»­æƒé‡è€Œéç¦»æ•£é€‰æ‹©
        weights = self.weight_predictor(target)  # [bs, num_queries, 2]
        weights = torch.tanh(weights)  # ğŸ”¥ æ”¹ç”¨tanhï¼šèŒƒå›´[-1, 1]ï¼Œæ”¯æŒè´Ÿbiasï¼ˆæŠ‘åˆ¶ï¼‰
        
        lidar_weights = weights[..., 0]   # [bs, num_queries] âˆˆ [-1, 1]
        camera_weights = weights[..., 1]  # [bs, num_queries] âˆˆ [-1, 1]
        
        # 5. ğŸ”¥ éµå¾ªè€å¸ˆå»ºè®®ï¼šç§»é™¤æ‰€æœ‰ç›´æ¥ç›‘ç£ï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯å­¦ä¹ 
        weight_loss = None
        # æ³¨é‡Šæ‰æ‰€æœ‰æƒé‡ç›‘ç£æŸå¤±ï¼Œè®©æ£€æµ‹æŸå¤±è‡ªç„¶åœ°è®­ç»ƒAQRæ¨¡å—
        # if self.training:
        #     # æ‰€æœ‰æƒé‡ç›‘ç£éƒ½è¢«ç§»é™¤ï¼Œé‡‡ç”¨çº¯ç«¯åˆ°ç«¯å­¦ä¹ 
        #     weight_loss = self._compute_regularization_loss(weights)
        
        # 6. ç»„ç»‡æŠ•å½±ä¿¡æ¯
        projection_info = {
            'pts_bev': pts_bev,           # [bs, num_queries, 2] BEVåæ ‡
            'pts_pers': pts_pers,         # [bs, num_queries, 3] é€è§†åæ ‡
            'pts_idx': pts_idx,           # [bs, num_queries] BEVç´¢å¼•
            'pts_pers_idx': pts_pers_idx  # [bs, num_queries] é€è§†ç´¢å¼•
        }
        
        return lidar_weights, camera_weights, weight_loss, projection_info
    
    # ğŸ”¥ ä»¥ä¸‹å‡½æ•°å·²ç§»é™¤ï¼Œéµå¾ªè€å¸ˆçš„ç«¯åˆ°ç«¯å­¦ä¹ å»ºè®®
    # æ³¨é‡Šä¿ç•™ä¾›å‚è€ƒï¼Œä½†ä¸å†ä½¿ç”¨
    """
    ç§»é™¤åŸå› ï¼šæ ¹æ®è€å¸ˆçš„æŒ‡å¯¼ï¼ŒAQRæƒé‡ç”Ÿæˆå™¨åº”è¯¥å®Œå…¨é€šè¿‡æœ€ç»ˆçš„æ£€æµ‹æŸå¤±æ¥å­¦ä¹ ï¼Œ
    è€Œä¸éœ€è¦ä»»ä½•ç›´æ¥çš„æƒé‡ç›‘ç£ã€‚è¿™æ›´ç¬¦åˆæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯è®­ç»ƒç†å¿µã€‚
    
    åŸæœ‰çš„ç›‘ç£å‡½æ•°ï¼š
    - _compute_weight_loss_from_modalmask
    - _compute_weight_loss_from_supervision  
    - _compute_regularization_loss
    - _compute_adversarial_regularization
    
    è¿™äº›éƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œå› ä¸ºï¼š
    1. æœ€ç»ˆçš„æ£€æµ‹æŸå¤±ä¼šè‡ªç„¶åœ°åå‘ä¼ æ’­åˆ°AQRæ¨¡å—
    2. æ¨¡å‹ä¼šéšå¼åœ°å­¦ä¼šç”Ÿæˆæœ€ä¼˜æƒé‡
    3. æƒé‡å¥½åçš„å”¯ä¸€æ ‡å‡†æ˜¯æœ€ç»ˆæ£€æµ‹æ•ˆæœ
    """


# é»˜è®¤é…ç½®
DEFAULT_AQR_CONFIG = dict(
    type='AQRWeightGenerator',
    embed_dims=256,
    encoder_config=dict(
        type='PETRTransformerDecoder',  # ğŸ”¥ ç»Ÿä¸€ä½¿ç”¨PETR
        return_intermediate=True,
        num_layers=1,
        transformerlayers=dict(
            type='PETRTransformerDecoderLayer',
            with_cp=False,
            attn_cfgs=[  # ğŸ”¥ PETRéœ€è¦åˆ—è¡¨æ ¼å¼
                dict(
                    type='MultiheadAttention',
                    embed_dims=256,
                    num_heads=4,  # ğŸ”¥ ä¸MoMEä¿æŒä¸€è‡´ï¼šAQRä½¿ç”¨4å¤´
                    dropout=0.1
                ),
            ],
            ffn_cfgs=dict(
                type='FFN',
                embed_dims=256,
                feedforward_channels=1024,
                num_fcs=2,
                ffn_drop=0.1,
                act_cfg=dict(type='ReLU', inplace=True)
            ),
            feedforward_channels=1024,
                operation_order=('cross_attn', 'norm', 'ffn', 'norm')  # ğŸ”¥ ä¸MoMEä¿æŒä¸€è‡´ï¼Œåªæœ‰cross_attn
        )
    ),
    window_sizes=[15, 5],  # [camera_window, lidar_window]
    use_type_embed=True,
    pc_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0],
    bev_feature_shape=(180, 180),  # ğŸ”¥ é»˜è®¤BEVç‰¹å¾å›¾å°ºå¯¸ï¼ˆvoxel_size=0.075ï¼‰
    pers_feature_shape=(6, 40, 100)  # ğŸ”¥ é»˜è®¤é€è§†ç‰¹å¾å›¾å°ºå¯¸ï¼ˆ1600x640ï¼‰
)
