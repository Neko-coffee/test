# AQR Queryæ•°é‡å’ŒCameraæƒé‡å›¾Bugåˆ†ææŠ¥å‘Š ğŸ”

## ğŸ¯ **é—®é¢˜1ï¼šçª—å£æ³¨æ„åŠ›è®¡ç®—ä½¿ç”¨çš„Queryæ•°é‡**

### **ç­”æ¡ˆï¼šä½¿ç”¨å…¨éƒ¨1730ä¸ªQuery** âœ…

```python
# ä»£ç ä½ç½®ï¼šaqr_weight_generator.py ç¬¬250-314è¡Œ

def forward(self, query_embed, memory, pos_embed, ref_points, img_metas, reg_branch=None):
    """
    Args:
        query_embed: [num_queries, bs, embed_dims]  # ğŸ”¥ è¿™é‡Œæ˜¯1730ä¸ªQuery
        memory: [total_elements, bs, embed_dims]
        ref_points: [bs, num_queries, 3]  # ğŸ”¥ è¿™é‡Œä¹Ÿæ˜¯1730ä¸ªQuery
    """
    
    # Step 1: 3DæŠ•å½±ï¼ˆå…¨éƒ¨1730ä¸ªQueryï¼‰
    pts_bev, pts_pers, pts_idx, pts_pers_idx = self.project_3d_to_features(ref_points, img_metas)
    # è¾“å‡ºï¼š[bs, 1730, ...]
    
    # Step 2: ç”Ÿæˆå±€éƒ¨æ³¨æ„åŠ›æ©ç ï¼ˆå…¨éƒ¨1730ä¸ªQueryï¼‰
    fusion_attention_mask = self.generate_local_attention_masks(pts_idx, pts_pers_idx)
    # è¾“å‡ºï¼š[bs*num_heads, 1730, total_elements]
    
    # Step 3: ç¼–ç å™¨å¤„ç†ï¼ˆå…¨éƒ¨1730ä¸ªQueryï¼‰
    target = self.encoder(
        query=target,  # [1730, bs, embed_dims]
        key=memory,
        value=memory,
        attn_masks=[fusion_attention_mask],  # ğŸ”¥ 1730ä¸ªQueryçš„æ©ç 
    )
    
    # Step 4: ç”Ÿæˆæƒé‡ï¼ˆå…¨éƒ¨1730ä¸ªQueryï¼‰
    weights = self.weight_predictor(target)  # [bs, 1730, 2]
    lidar_weights = weights[..., 0]   # [bs, 1730]
    camera_weights = weights[..., 1]  # [bs, 1730]
    
    return lidar_weights, camera_weights, weight_loss, projection_info
```

---

### **è¯¦ç»†è§£é‡Š**

#### **ä¸ºä»€ä¹ˆä½¿ç”¨å…¨éƒ¨1730ä¸ªQueryï¼Ÿ**

```python
åŸå› 1ï¼šAQRåœ¨CMT Transformerä¹‹å‰æ‰§è¡Œ
  æµç¨‹ï¼š
    ç‰¹å¾æå– â†’ AQRè°ƒåˆ¶ â†’ CMT Transformer â†’ æ£€æµ‹å¤´
    
  åœ¨AQRé˜¶æ®µï¼š
    - reference_pointså·²ç»åŒ…å«DN Query
    - shape: [bs, 1730, 3]
    - AQRå¿…é¡»ä¸ºæ‰€æœ‰Queryç”Ÿæˆæƒé‡

åŸå› 2ï¼šDN Queryä¹Ÿéœ€è¦æƒé‡
  DN Queryçš„ä½œç”¨ï¼š
    - å¸®åŠ©è®­ç»ƒæ”¶æ•›
    - ä¹Ÿéœ€è¦ä»ç‰¹å¾ä¸­æå–ä¿¡æ¯
    - ä¹Ÿéœ€è¦AQRä¸ºå…¶åˆ†é…æ¨¡æ€æƒé‡
    
åŸå› 3ï¼šä»£ç å®ç°
  # cmt_head.py ç¬¬623-628è¡Œ
  reference_points = self.reference_points.weight  # [900, 3]
  reference_points, attn_mask, mask_dict = self.prepare_for_dn(...)  # [bs, 1730, 3]
  
  # AQRè°ƒåˆ¶
  if self.enable_aqr:
      x, x_img = self._apply_aqr_modulation(x, x_img, reference_points, img_metas)
      # reference_pointså·²ç»æ˜¯[bs, 1730, 3]äº†
```

---

#### **çª—å£æ³¨æ„åŠ›æ©ç çš„ç”Ÿæˆ**

```python
# aqr_weight_generator.py ç¬¬170-248è¡Œ

def generate_local_attention_masks(self, pts_idx, pts_pers_idx):
    """
    ä¸º1730ä¸ªQueryç”Ÿæˆå±€éƒ¨æ³¨æ„åŠ›æ©ç 
    
    Args:
        pts_idx: [bs, 1730] BEVç‰¹å¾å›¾ç´¢å¼•
        pts_pers_idx: [bs, 1730] é€è§†ç‰¹å¾å›¾ç´¢å¼•
    
    Returns:
        fusion_attention_mask: [bs*num_heads, 1730, total_elements]
    """
    batch_size, num_queries = pts_idx.shape  # num_queries = 1730
    
    # Cameraçª—å£ï¼ˆwindow_size=15ï¼‰
    camera_mask = self._generate_camera_window(pts_pers_idx, window_size=15)
    # [bs, 1730, camera_elements]
    
    # LiDARçª—å£ï¼ˆwindow_size=5ï¼‰
    lidar_mask = self._generate_lidar_window(pts_idx, window_size=5)
    # [bs, 1730, lidar_elements]
    
    # èåˆæ©ç 
    fusion_mask = torch.cat([lidar_mask, camera_mask], dim=-1)
    # [bs, 1730, total_elements]
    
    return fusion_mask
```

---

#### **æƒé‡ç”Ÿæˆå’Œä½¿ç”¨**

```python
# ç”Ÿæˆ1730ä¸ªQueryçš„æƒé‡
lidar_weights: [bs, 1730]
camera_weights: [bs, 1730]

# æ¸²æŸ“åˆ°ç‰¹å¾å›¾
weight_map_bev = self.weight_renderer.render_bev_weights(
    lidar_weights,  # [bs, 1730]
    pts_bev         # [bs, 1730, 2]
)
# è¾“å‡ºï¼š[bs, 128, 128]

weight_map_pers = self.weight_renderer.render_perspective_weights(
    camera_weights,  # [bs, 1730]
    pts_pers         # [bs, 1730, 3]
)
# è¾“å‡ºï¼š[bs, 6, 20, 50]
```

---

### **å…³é”®ç»“è®º**

```python
âœ… AQRçª—å£æ³¨æ„åŠ›ä½¿ç”¨å…¨éƒ¨1730ä¸ªQuery
âœ… åŒ…æ‹¬900ä¸ªåŸå§‹Query + 830ä¸ªDN Query
âœ… æ¯ä¸ªQueryéƒ½æœ‰è‡ªå·±çš„å±€éƒ¨æ³¨æ„åŠ›çª—å£
âœ… æ¯ä¸ªQueryéƒ½ç”Ÿæˆè‡ªå·±çš„LiDARå’ŒCameraæƒé‡

åŸå› ï¼š
  1. AQRåœ¨CMT Transformerä¹‹å‰æ‰§è¡Œ
  2. æ­¤æ—¶reference_pointså·²ç»åŒ…å«DN Query
  3. DN Queryä¹Ÿéœ€è¦æ¨¡æ€æƒé‡æ¥æå–ç‰¹å¾
  4. ä»£ç å®ç°ä¸Šæ— æ³•åŒºåˆ†åŸå§‹Queryå’ŒDN Query
```

---

## ğŸ› **é—®é¢˜2ï¼šCameraæƒé‡å›¾æœ€å¤§å€¼70.83çš„Bugåˆ†æ**

### **Bugç¡®è®¤ï¼šæ˜¯ä»£ç é—®é¢˜ï¼** âš ï¸âš ï¸âš ï¸

```python
BUGä½ç½®ï¼šweight_renderer.py ç¬¬122-169è¡Œ

é—®é¢˜ï¼šrender_perspective_weights() ç¼ºå°‘åå¤„ç†è°ƒç”¨ï¼

# âŒ å½“å‰ä»£ç ï¼ˆæœ‰Bugï¼‰
def render_perspective_weights(self, query_weights, pts_pers, feature_shape=None):
    ...
    for view_idx in range(num_views):
        view_weight_map = self._render_to_single_view(...)
        weight_map[:, view_idx] = view_weight_map
    
    return weight_map  # âŒ ç›´æ¥è¿”å›ï¼Œæ²¡æœ‰åå¤„ç†ï¼

# âœ… æ­£ç¡®ä»£ç ï¼ˆå¯¹æ¯”BEVï¼‰
def render_bev_weights(self, query_weights, pts_bev, feature_shape=None):
    ...
    if self.render_method == 'gaussian':
        weight_map = self._render_gaussian(...)
    
    # âœ… æœ‰åå¤„ç†ï¼
    weight_map = self._postprocess_weight_map(weight_map)
    
    return weight_map
```

---

### **Bugå½±å“åˆ†æ**

```python
# åå¤„ç†å‡½æ•°ï¼ˆç¬¬374-394è¡Œï¼‰
def _postprocess_weight_map(self, weight_map):
    """
    å…³é”®åŠŸèƒ½ï¼š
    1. è¿‡æ»¤å°äºé˜ˆå€¼çš„æƒé‡
    2. è£å‰ªåˆ°[0, 1.5]èŒƒå›´
    """
    weight_map[weight_map < self.min_weight_threshold] = 0
    
    if self.normalize_weights:
        weight_map = torch.clamp(weight_map, min=0, max=1.5)  # ğŸ”¥ å…³é”®ï¼
    
    return weight_map

# å½“å‰çŠ¶æ€
BEVæƒé‡å›¾ï¼š
  - è°ƒç”¨äº†_postprocess_weight_map âœ…
  - maxè¢«è£å‰ªåˆ°1.5 âœ…
  - å®é™…max: 1.500 âœ…

Cameraæƒé‡å›¾ï¼š
  - æ²¡æœ‰è°ƒç”¨_postprocess_weight_map âŒ
  - maxæ²¡æœ‰è¢«è£å‰ª âŒ
  - å®é™…max: 70.829 âŒï¼ˆå¤šä¸ªQueryæƒé‡å åŠ å¯¼è‡´ï¼‰
```

---

### **ä¸ºä»€ä¹ˆCameraä¼šåˆ°70.83ï¼Ÿ**

```python
åŸå› ï¼šé«˜æ–¯æ ¸å åŠ  + æ— è£å‰ª

å‡è®¾åœºæ™¯ï¼š
  - 1730ä¸ªQuery
  - Cameraç‰¹å¾å›¾å°ï¼ˆ6Ã—20Ã—50 = 6000åƒç´ ï¼‰
  - Queryè¦†ç›–ç‡é«˜ï¼ˆ1730/6000 â‰ˆ 28.8%ï¼‰
  
æŸä¸ªçƒ­ç‚¹åƒç´ ï¼š
  Query 1æŠ•å½±åˆ°è¿™é‡Œï¼šæƒé‡0.85ï¼Œé«˜æ–¯æ ¸ä¸­å¿ƒå€¼1.0 â†’ è´¡çŒ®0.85
  Query 2ä¹ŸæŠ•å½±åˆ°è¿™é‡Œï¼šæƒé‡0.78ï¼Œé«˜æ–¯æ ¸ä¸­å¿ƒå€¼1.0 â†’ è´¡çŒ®0.78
  Query 3ä¹ŸæŠ•å½±åˆ°è¿™é‡Œï¼šæƒé‡0.92ï¼Œé«˜æ–¯æ ¸ä¸­å¿ƒå€¼1.0 â†’ è´¡çŒ®0.92
  ...
  Query 80ä¹ŸæŠ•å½±åˆ°è¿™é‡Œï¼šæƒé‡0.81ï¼Œé«˜æ–¯æ ¸ä¸­å¿ƒå€¼1.0 â†’ è´¡çŒ®0.81
  
  æ€»æƒé‡ = 0.85 + 0.78 + 0.92 + ... + 0.81 â‰ˆ 70.83 âŒ

å¦‚æœæœ‰åå¤„ç†ï¼š
  æ€»æƒé‡ = clamp(70.83, 0, 1.5) = 1.5 âœ…
```

---

### **Bugä¿®å¤æ–¹æ¡ˆ**

#### **æ–¹æ¡ˆ1ï¼šç›´æ¥æ·»åŠ åå¤„ç†è°ƒç”¨ï¼ˆæ¨èï¼‰** â­â­â­â­â­

```python
# ä¿®æ”¹ï¼šweight_renderer.py ç¬¬122-169è¡Œ

def render_perspective_weights(self, query_weights, pts_pers, feature_shape=None):
    """
    å°†æŸ¥è¯¢æƒé‡æ¸²æŸ“åˆ°é€è§†ç‰¹å¾å›¾ä¸Š
    """
    if feature_shape is None:
        feature_shape = self.pers_feature_shape
    
    batch_size, num_queries = query_weights.shape
    num_views, H, W = feature_shape
    
    # åˆå§‹åŒ–æƒé‡å›¾
    weight_map = torch.zeros(batch_size, num_views, H, W, 
                           device=query_weights.device, dtype=query_weights.dtype)
    
    # åˆ†è§†è§’å¤„ç†
    for view_idx in range(num_views):
        view_mask = (pts_pers[:, :, 0] == view_idx) & (~torch.isnan(pts_pers[:, :, 1]))
        
        if not view_mask.any():
            continue
        
        view_coords = pts_pers[:, :, 1:3][view_mask]
        view_weights = query_weights[view_mask]
        
        if len(view_coords) == 0:
            continue
        
        batch_indices = torch.arange(batch_size, device=query_weights.device)[:, None].expand(-1, num_queries)[view_mask]
        
        view_weight_map = self._render_to_single_view(
            view_weights, view_coords, batch_indices, batch_size, (H, W)
        )
        
        weight_map[:, view_idx] = view_weight_map
    
    # ğŸ”¥ æ·»åŠ è¿™ä¸€è¡Œï¼
    # å¯¹æ¯ä¸ªè§†è§’åˆ†åˆ«è¿›è¡Œåå¤„ç†
    for view_idx in range(num_views):
        weight_map[:, view_idx] = self._postprocess_weight_map(weight_map[:, view_idx])
    
    return weight_map
```

---

#### **æ–¹æ¡ˆ2ï¼šä¿®æ”¹åå¤„ç†å‡½æ•°æ”¯æŒ4Då¼ é‡** â­â­â­â­

```python
# ä¿®æ”¹ï¼šweight_renderer.py ç¬¬374-394è¡Œ

def _postprocess_weight_map(self, weight_map):
    """
    æƒé‡å›¾åå¤„ç†ï¼ˆæ”¯æŒ3Då’Œ4Då¼ é‡ï¼‰
    
    Args:
        weight_map: [bs, H, W] æˆ– [bs, num_views, H, W]
    """
    # åº”ç”¨æœ€å°é˜ˆå€¼
    weight_map[weight_map < self.min_weight_threshold] = 0
    
    # è£å‰ªåˆ°åˆç†èŒƒå›´
    if self.normalize_weights:
        weight_map = torch.clamp(weight_map, min=0, max=1.5)
    
    return weight_map

# ä¿®æ”¹ï¼šrender_perspective_weights
def render_perspective_weights(self, query_weights, pts_pers, feature_shape=None):
    ...
    # åˆ†è§†è§’å¤„ç†
    for view_idx in range(num_views):
        ...
        weight_map[:, view_idx] = view_weight_map
    
    # ğŸ”¥ æ·»åŠ åå¤„ç†
    weight_map = self._postprocess_weight_map(weight_map)
    
    return weight_map
```

---

### **ä¿®å¤åçš„é¢„æœŸæ•ˆæœ**

```python
ä¿®å¤å‰ï¼š
  Cameraæƒé‡å›¾ï¼š
    mean: 0.201530
    std: 1.203692
    min: 0.000000
    max: 70.829094  âŒ
  
  Cameraç›¸å¯¹å˜åŒ–ï¼š33.3% âš ï¸

ä¿®å¤åï¼š
  Cameraæƒé‡å›¾ï¼š
    mean: ~0.15-0.20ï¼ˆç•¥å¾®ä¸‹é™ï¼‰
    std: ~0.25-0.35ï¼ˆå¤§å¹…ä¸‹é™ï¼‰
    min: 0.000000
    max: 1.500000  âœ…
  
  Cameraç›¸å¯¹å˜åŒ–ï¼š~10-15% âœ…

æ€§èƒ½æå‡ï¼š
  - ç‰¹å¾è°ƒåˆ¶æ›´æ¸©å’Œ
  - Transformeræ›´å®¹æ˜“é€‚åº”
  - é¢„æœŸmAPæå‡1-2%
  - è®­ç»ƒæ›´ç¨³å®š
```

---

## ğŸ”§ **ç«‹å³ä¿®å¤æ­¥éª¤**

### **Step 1ï¼šä¿®æ”¹weight_renderer.py**

```python
# æ–‡ä»¶ï¼šCMT-master/projects/mmdet3d_plugin/models/utils/weight_renderer.py
# ä½ç½®ï¼šç¬¬122-169è¡Œ

# åœ¨returnä¹‹å‰æ·»åŠ ï¼š
def render_perspective_weights(self, query_weights, pts_pers, feature_shape=None):
    ...
    # åˆ†è§†è§’å¤„ç†
    for view_idx in range(num_views):
        ...
        weight_map[:, view_idx] = view_weight_map
    
    # ğŸ”¥ æ·»åŠ åå¤„ç†ï¼ˆé€è§†è§’ï¼‰
    for view_idx in range(num_views):
        weight_map[:, view_idx] = self._postprocess_weight_map(weight_map[:, view_idx])
    
    return weight_map
```

---

### **Step 2ï¼šéªŒè¯ä¿®å¤**

```python
# è®­ç»ƒå‡ ä¸ªiterationåæ£€æŸ¥debugè¾“å‡º

æœŸæœ›çœ‹åˆ°ï¼š
  weight_map_pers_stats:
    mean: 0.15-0.20
    std: 0.25-0.35
    min: 0.000000
    max: 1.500000  âœ…ï¼ˆä¸å†æ˜¯70.8ï¼‰
    
  modulation_effect_pers:
    relative_change: 0.10-0.15  âœ…ï¼ˆä¸å†æ˜¯33.3%ï¼‰
```

---

### **Step 3ï¼šé‡æ–°è®­ç»ƒ**

```python
ä¿®å¤åå»ºè®®ï¼š
  1. ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆæˆ–ä»é¢„è®­ç»ƒæƒé‡ï¼‰
  2. è§‚å¯Ÿå‰å‡ ä¸ªepochçš„æ€§èƒ½
  3. é¢„æœŸåˆæœŸæ€§èƒ½æ›´ç¨³å®š
  4. é¢„æœŸ5-10 epochsåæ€§èƒ½æ›´å¥½
```

---

## ğŸ“Š **æ€»ç»“**

### **é—®é¢˜1ï¼šQueryæ•°é‡**

```python
âœ… çª—å£æ³¨æ„åŠ›ä½¿ç”¨å…¨éƒ¨1730ä¸ªQuery
âœ… åŒ…æ‹¬900ä¸ªåŸå§‹ + 830ä¸ªDN Query
âœ… è¿™æ˜¯æ­£ç¡®çš„è®¾è®¡
```

---

### **é—®é¢˜2ï¼šCameraæƒé‡å›¾Bug**

```python
âŒ Bugç¡®è®¤ï¼šrender_perspective_weightsç¼ºå°‘åå¤„ç†
âŒ å¯¼è‡´Cameraæƒé‡å›¾max=70.8ï¼ˆåº”è¯¥æ˜¯1.5ï¼‰
âŒ å¯¼è‡´Cameraç›¸å¯¹å˜åŒ–33.3%ï¼ˆåº”è¯¥æ˜¯10-15%ï¼‰

âœ… ä¿®å¤æ–¹æ¡ˆï¼šæ·»åŠ _postprocess_weight_mapè°ƒç”¨
âœ… é¢„æœŸæ•ˆæœï¼šCameraè°ƒåˆ¶æ›´æ¸©å’Œï¼Œæ€§èƒ½æå‡1-2%
âœ… ä¿®å¤éš¾åº¦ï¼šç®€å•ï¼ˆåªéœ€æ·»åŠ å‡ è¡Œä»£ç ï¼‰
```

---

## ğŸ¯ **è¡ŒåŠ¨å»ºè®®**

**ä¼˜å…ˆçº§1ï¼šç«‹å³ä¿®å¤Cameraæƒé‡å›¾Bug** ğŸ”¥ğŸ”¥ğŸ”¥
  - å·¥ä½œé‡ï¼š5åˆ†é’Ÿ
  - å½±å“ï¼šæ˜¾è‘—ï¼ˆç›¸å¯¹å˜åŒ–ä»33%é™åˆ°10-15%ï¼‰
  - é£é™©ï¼šæä½

**ä¼˜å…ˆçº§2ï¼šé‡æ–°è®­ç»ƒéªŒè¯** â­â­â­â­
  - ä»é¢„è®­ç»ƒæƒé‡å¼€å§‹
  - è§‚å¯Ÿå‰5ä¸ªepochs
  - é¢„æœŸæ€§èƒ½æ¢å¤æ›´å¿«

**ä¼˜å…ˆçº§3ï¼šç»§ç»­è®­ç»ƒåˆ°10-20 epochs** â­â­â­
  - é¢„æœŸè¶…è¶ŠåŸæ¨¡å‹
  - mAP: 68-72%
  - NDS: 71-75%

---

**ç”Ÿæˆæ—¶é—´**: 2025-10-12
**åˆ†æè€…**: AI Assistant ğŸ¾

