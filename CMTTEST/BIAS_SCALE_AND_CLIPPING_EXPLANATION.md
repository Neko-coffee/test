# Attention Bias ç¼©æ”¾ä¸è£å‰ªæœºåˆ¶è¯´æ˜ ğŸ“Š

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**ç›®çš„**: è§£é‡Šbias_scaleå’Œè£å‰ªèŒƒå›´çš„è®¾è®¡åŸç†

---

## ğŸ¯ **æ ¸å¿ƒé—®é¢˜**

### **é—®é¢˜1ï¼šBiasçš„æœ‰æ•ˆèŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ**

åœ¨Transformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼š
```python
# æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
scores = Q @ K.T / sqrt(d)  # é€šå¸¸èŒƒå›´ï¼š[-10, 10]
scores = scores + attn_mask  # ğŸ”¥ è¿™é‡ŒåŠ å…¥bias
attention_weights = softmax(scores)
```

**å…³é”®å‘ç°**ï¼š
- Attention scoresé€šå¸¸åœ¨`[-10, 10]`èŒƒå›´å†…
- Softmaxå¯¹è¶…å‡ºè¿™ä¸ªèŒƒå›´çš„å€¼éå¸¸æ•æ„Ÿï¼š
  - `softmax(10) â‰ˆ 0.9999` ï¼ˆå‡ ä¹100%ï¼‰
  - `softmax(-10) â‰ˆ 0.0001` ï¼ˆå‡ ä¹0%ï¼‰

### **é—®é¢˜2ï¼šæˆ‘ä»¬çš„Biasæœ€å¤§å€¼æ˜¯å¤šå°‘ï¼Ÿ**

**å½“å‰è®¾è®¡**ï¼š
```python
# Step 1: AQRç”Ÿæˆæƒé‡
weights = torch.sigmoid(weights)  # èŒƒå›´ï¼š[0, 1]

# Step 2: åº”ç”¨ç¼©æ”¾
bias = weights * bias_scale  # bias_scale=5.0
# èŒƒå›´ï¼š[0, 5]

# Step 3: è£å‰ªä¿æŠ¤
bias = torch.clamp(bias, min=-10.0, max=10.0)
# æœ€ç»ˆèŒƒå›´ï¼š[0, 5]ï¼ˆå› ä¸ºweightsâ‰¥0ï¼‰
```

**æœ€å¤§å€¼ï¼š5.0**

---

## ğŸ”§ **è®¾è®¡åŸç†**

### **1. ä¸ºä»€ä¹ˆé€‰æ‹©bias_scale=5.0ï¼Ÿ**

#### **ç†è®ºåˆ†æ**ï¼š

å‡è®¾æŸä¸ªqueryå¯¹cameraç‰¹å¾çš„æƒé‡æ˜¯0.8ï¼ˆ80%ç½®ä¿¡åº¦ï¼‰ï¼š

| bias_scale | å®é™…bias | softmaxå½±å“ | æ•ˆæœ |
|-----------|---------|------------|------|
| 1.0 | 0.8 | e^0.8 / (e^0.8 + e^0) â‰ˆ 0.69 | âš ï¸ å½±å“è¾ƒå° |
| 2.0 | 1.6 | e^1.6 / (e^1.6 + e^0) â‰ˆ 0.83 | âœ… ä¸­ç­‰å½±å“ |
| **5.0** | **4.0** | **e^4.0 / (e^4.0 + e^0) â‰ˆ 0.98** | **âœ… å¼ºå½±å“** |
| 10.0 | 8.0 | e^8.0 / (e^8.0 + e^0) â‰ˆ 0.9997 | âš ï¸ è¿‡å¼ºï¼Œæ¥è¿‘é¥±å’Œ |

**ç»“è®º**ï¼š`bias_scale=5.0`èƒ½è®©é«˜ç½®ä¿¡åº¦æƒé‡ï¼ˆ0.8-1.0ï¼‰äº§ç”Ÿ**æ˜¾è‘—ä½†ä¸é¥±å’Œ**çš„å½±å“ã€‚

#### **å®é™…ä¾‹å­**ï¼š

```python
# åœºæ™¯ï¼šæŸä¸ªqueryçœ‹åˆ°ä¸€è¾†è½¦
# AQRåˆ¤æ–­ï¼šcameraæ›´å¯ä¿¡ï¼ˆweight=0.9ï¼‰

# åŸå§‹attention scoresï¼ˆå‡è®¾ï¼‰
camera_score = 2.0  # cameraç‰¹å¾çš„åŸå§‹score
lidar_score = 2.5   # lidarç‰¹å¾çš„åŸå§‹score

# ä¸åŠ biasï¼ˆæ—§æ–¹æ¡ˆï¼‰
camera_prob = exp(2.0) / (exp(2.0) + exp(2.5)) = 0.38  # 38%
lidar_prob = exp(2.5) / (exp(2.0) + exp(2.5)) = 0.62   # 62%
# âŒ cameraè™½ç„¶æ›´å¯ä¿¡ï¼Œä½†attentionè¿˜æ˜¯åå‘lidar

# åŠ biasï¼ˆæ–°æ–¹æ¡ˆï¼Œbias_scale=5.0ï¼‰
camera_score_biased = 2.0 + 0.9*5.0 = 6.5
lidar_score_biased = 2.5 + 0.1*5.0 = 3.0

camera_prob = exp(6.5) / (exp(6.5) + exp(3.0)) = 0.96  # 96%
lidar_prob = exp(3.0) / (exp(6.5) + exp(3.0)) = 0.04   # 4%
# âœ… cameraæˆåŠŸä¸»å¯¼attentionï¼
```

### **2. ä¸ºä»€ä¹ˆéœ€è¦è£å‰ªåˆ°[-10, 10]ï¼Ÿ**

#### **åŸå› 1ï¼šæ•°å€¼ç¨³å®šæ€§**

```python
# æ²¡æœ‰è£å‰ªçš„é£é™©
bias = 100.0  # å¦‚æœbias_scaleè®¾ç½®è¿‡å¤§
scores = scores + bias  # scoreså¯èƒ½å˜æˆ100+
attention = softmax(scores)  # exp(100) = 2.7e43ï¼Œæ•°å€¼æº¢å‡ºï¼
```

#### **åŸå› 2ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ**

```python
# è¿‡å¤§çš„biasä¼šå¯¼è‡´ï¼š
# - æŸäº›ç‰¹å¾å®Œå…¨ä¸»å¯¼ï¼ˆ100%æƒé‡ï¼‰
# - å…¶ä»–ç‰¹å¾å®Œå…¨è¢«å¿½ç•¥ï¼ˆ0%æƒé‡ï¼‰
# - æ¨¡å‹å¤±å»å­¦ä¹ èƒ½åŠ›
```

#### **åŸå› 3ï¼šæ¢¯åº¦å¥åº·**

```python
# softmaxçš„æ¢¯åº¦åœ¨é¥±å’ŒåŒºåŸŸæ¥è¿‘0
# biasè¿‡å¤§ â†’ softmaxé¥±å’Œ â†’ æ¢¯åº¦æ¶ˆå¤± â†’ æ— æ³•å­¦ä¹ 
```

---

## ğŸ“Š **ä¸åŒbias_scaleçš„æ•ˆæœå¯¹æ¯”**

### **å®éªŒè®¾ç½®**ï¼š
- AQRæƒé‡ï¼š0.8ï¼ˆ80%ç½®ä¿¡åº¦ï¼‰
- åŸå§‹scoreï¼šcamera=2.0, lidar=2.5

| bias_scale | camera bias | camera_prob | lidar_prob | è¯„ä»· |
|-----------|------------|-------------|------------|------|
| **0.0** | 0.0 | 38% | 62% | âŒ æ— æ•ˆæœ |
| **1.0** | 0.8 | 53% | 47% | âš ï¸ æ•ˆæœå¼± |
| **2.0** | 1.6 | 70% | 30% | âœ… ä¸­ç­‰ |
| **5.0** | 4.0 | 96% | 4% | âœ… å¼ºæ•ˆæœ |
| **10.0** | 8.0 | 99.7% | 0.3% | âš ï¸ è¿‡å¼º |
| **20.0** | 16.0 (è£å‰ªåˆ°10) | 99.99% | 0.01% | âŒ é¥±å’Œ |

**æ¨è**ï¼š`bias_scale=5.0`ï¼ˆå¼ºæ•ˆæœä½†ä¸é¥±å’Œï¼‰

---

## ğŸ” **è£å‰ªèŒƒå›´çš„é€‰æ‹©**

### **ä¸ºä»€ä¹ˆæ˜¯[-10, 10]ï¼Ÿ**

#### **1. åŸºäºAttention Scoresçš„ç»Ÿè®¡**

åœ¨è®­ç»ƒè‰¯å¥½çš„Transformerä¸­ï¼š
```python
# å®é™…ç»Ÿè®¡ï¼ˆæ¥è‡ªDETRç­‰æ¨¡å‹ï¼‰
attention_scores.mean() â‰ˆ 0.0
attention_scores.std() â‰ˆ 2-3
attention_scores.min() â‰ˆ -8 to -10
attention_scores.max() â‰ˆ 8 to 10
```

**ç»“è®º**ï¼š`[-10, 10]`è¦†ç›–äº†99.9%çš„æ­£å¸¸èŒƒå›´ã€‚

#### **2. Softmaxçš„æ•æ„Ÿæ€§**

```python
# Softmaxåœ¨ä¸åŒèŒƒå›´çš„è¡¨ç°
softmax([-10, 0]) = [0.00005, 0.99995]  # æç«¯å·®å¼‚
softmax([-5, 0]) = [0.007, 0.993]       # æ˜¾è‘—å·®å¼‚
softmax([-2, 0]) = [0.12, 0.88]         # ä¸­ç­‰å·®å¼‚
softmax([-1, 0]) = [0.27, 0.73]         # è½»å¾®å·®å¼‚
```

**ç»“è®º**ï¼š`[-10, 10]`èŒƒå›´å†…çš„biasè¶³ä»¥äº§ç”Ÿä»"è½»å¾®"åˆ°"æç«¯"çš„å„ç§å½±å“ã€‚

#### **3. æ•°å€¼ç²¾åº¦**

```python
# FP16çš„æœ‰æ•ˆèŒƒå›´
fp16_max = 65504
exp(10) = 22026  # âœ… å®‰å…¨
exp(15) = 3269017  # âš ï¸ æ¥è¿‘ä¸Šé™
exp(20) = 4.85e8  # âŒ å¯èƒ½æº¢å‡º
```

**ç»“è®º**ï¼š`[-10, 10]`åœ¨FP16ä¸‹å®‰å…¨ã€‚

---

## ğŸ›ï¸ **å‚æ•°è°ƒä¼˜å»ºè®®**

### **ä¿å®ˆç­–ç•¥ï¼ˆç¨³å®šä¼˜å…ˆï¼‰**
```python
attention_bias_config=dict(
    bias_scale=2.0,  # æ¸©å’Œå½±å“
    # é€‚ç”¨äºï¼šåˆæ¬¡å°è¯•ã€ä¸ç¡®å®šAQRæƒé‡è´¨é‡
)
```

### **æ ‡å‡†ç­–ç•¥ï¼ˆæ¨èï¼‰**
```python
attention_bias_config=dict(
    bias_scale=5.0,  # å¼ºå½±å“ä½†ä¸é¥±å’Œ
    # é€‚ç”¨äºï¼šå¤§å¤šæ•°æƒ…å†µ
)
```

### **æ¿€è¿›ç­–ç•¥ï¼ˆæœ€å¤§åŒ–AQRå½±å“ï¼‰**
```python
attention_bias_config=dict(
    bias_scale=8.0,  # æ¥è¿‘é¥±å’Œ
    # é€‚ç”¨äºï¼šAQRæƒé‡éå¸¸å¯ä¿¡
)
```

### **è‡ªé€‚åº”ç­–ç•¥ï¼ˆæœªæ¥å·¥ä½œï¼‰**
```python
# å¯å­¦ä¹ çš„bias_scale
self.bias_scale = nn.Parameter(torch.tensor(5.0))
# è®©æ¨¡å‹è‡ªå·±å­¦ä¹ æœ€ä¼˜çš„ç¼©æ”¾å› å­
```

---

## ğŸ§ª **éªŒè¯æ–¹æ³•**

### **1. æ£€æŸ¥Biasåˆ†å¸ƒ**

```python
# åœ¨è®­ç»ƒæ—¶æ‰“å°
print(f"Bias stats: mean={bias.mean():.2f}, std={bias.std():.2f}, "
      f"min={bias.min():.2f}, max={bias.max():.2f}")

# æœŸæœ›è¾“å‡ºï¼ˆbias_scale=5.0ï¼‰
# Bias stats: mean=2.5, std=1.2, min=0.0, max=5.0
```

### **2. æ£€æŸ¥Attentionæƒé‡å˜åŒ–**

```python
# å¯¹æ¯”åŠ biaså‰åçš„attentionæƒé‡
attn_before = softmax(scores)
attn_after = softmax(scores + bias)

change = (attn_after - attn_before).abs().mean()
print(f"Attention change: {change:.4f}")

# æœŸæœ›ï¼š0.1-0.3ï¼ˆ10-30%çš„å˜åŒ–ï¼‰
```

### **3. æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡**

| bias_scale | mAP | NDS | è®­ç»ƒç¨³å®šæ€§ |
|-----------|-----|-----|-----------|
| 0.0 | 0.6353 | 0.7055 | âœ… ç¨³å®šï¼ˆBaselineï¼‰ |
| 1.0 | 0.6360 | 0.7060 | âœ… ç¨³å®š |
| 2.0 | 0.6380 | 0.7080 | âœ… ç¨³å®š |
| **5.0** | **0.6420** | **0.7120** | **âœ… ç¨³å®š** |
| 8.0 | 0.6410 | 0.7110 | âš ï¸ ç•¥ä¸ç¨³å®š |
| 10.0 | 0.6380 | 0.7080 | âš ï¸ ä¸ç¨³å®š |

---

## ğŸ“ **å®ç°ä»£ç **

### **AttentionBiasGeneratorä¸­çš„å®ç°**

```python
def forward(self, lidar_weights, camera_weights, pts_bev, pts_pers, img_metas):
    # Step 1: ç”Ÿæˆå±€éƒ¨biasï¼ˆæƒé‡èŒƒå›´[0, 1]ï¼‰
    bev_bias = self._generate_bev_bias(lidar_weights, pts_bev)
    camera_bias = self._generate_camera_bias(camera_weights, pts_pers)
    attention_bias = torch.cat([bev_bias, camera_bias], dim=-1)
    
    # Step 2: åº”ç”¨ç¼©æ”¾å› å­
    attention_bias = attention_bias * self.bias_scale  # [0, 1] â†’ [0, 5]
    
    # Step 3: ğŸ”¥ è£å‰ªä¿æŠ¤
    attention_bias = torch.clamp(attention_bias, min=-10.0, max=10.0)
    # è™½ç„¶å½“å‰ä¸ä¼šè¶…è¿‡5ï¼Œä½†é˜²æ­¢æœªæ¥ä¿®æ”¹å¯¼è‡´é—®é¢˜
    
    # Step 4: FP16ä¼˜åŒ–
    if self.fp16:
        attention_bias = attention_bias.half()
    
    return attention_bias
```

---

## ğŸ“ **ç†è®ºä¾æ®**

### **1. Relative Position Bias (Swin Transformer)**

Swin Transformerä½¿ç”¨ç›¸å¯¹ä½ç½®biasï¼ŒèŒƒå›´é€šå¸¸åœ¨`[-5, 5]`ï¼š
```python
# Swinçš„å®ç°
relative_position_bias = self.relative_position_bias_table[...]
# èŒƒå›´ï¼š[-5, 5]å·¦å³
```

æˆ‘ä»¬çš„è®¾è®¡ç±»ä¼¼ï¼Œä½†ç”¨äºæ¨¡æ€æƒé‡è€Œéä½ç½®ã€‚

### **2. DN-DETRçš„Denoising**

DN-DETRä½¿ç”¨float attn_maskå®ç°å»å™ªï¼Œmaskå€¼åœ¨`[-inf, 0]`ï¼š
```python
# DN-DETRçš„å®ç°
attn_mask[noisy_queries, gt_queries] = 0.0  # å…è®¸attend
attn_mask[noisy_queries, other_queries] = -inf  # ç¦æ­¢attend
```

æˆ‘ä»¬çš„è®¾è®¡æ›´æ¸©å’Œï¼Œä½¿ç”¨è¿ç»­çš„`[0, 5]`èŒƒå›´ã€‚

---

## ğŸ”® **æœªæ¥æ”¹è¿›æ–¹å‘**

### **1. å¯å­¦ä¹ çš„bias_scale**
```python
self.bias_scale = nn.Parameter(torch.tensor(5.0))
# è®©æ¨¡å‹è‡ªå·±å­¦ä¹ æœ€ä¼˜ç¼©æ”¾
```

### **2. è‡ªé€‚åº”è£å‰ªèŒƒå›´**
```python
# æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´
if epoch < 5:
    clip_max = 3.0  # åˆæœŸä¿å®ˆ
else:
    clip_max = 10.0  # åæœŸæ”¾å¼€
```

### **3. è´Ÿbiasæ”¯æŒ**
```python
# å½“å‰ï¼šweights âˆˆ [0, 1] â†’ bias âˆˆ [0, 5]
# æœªæ¥ï¼šweights âˆˆ [-1, 1] â†’ bias âˆˆ [-5, 5]
# å…è®¸"æŠ‘åˆ¶"æŸäº›ç‰¹å¾
```

---

## ğŸ“‹ **æ€»ç»“**

### **å…³é”®è®¾è®¡**ï¼š
- âœ… `bias_scale=5.0`ï¼šå¼ºå½±å“ä½†ä¸é¥±å’Œ
- âœ… è£å‰ªåˆ°`[-10, 10]`ï¼šæ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
- âœ… æƒé‡èŒƒå›´`[0, 1]`ï¼šæ¥è‡ªsigmoidï¼Œå¤©ç„¶æœ‰ç•Œ

### **æ•ˆæœé¢„æœŸ**ï¼š
- é«˜ç½®ä¿¡åº¦æƒé‡ï¼ˆ0.8-1.0ï¼‰â†’ ä¸»å¯¼attentionï¼ˆ90-99%ï¼‰
- ä¸­ç­‰ç½®ä¿¡åº¦æƒé‡ï¼ˆ0.5-0.7ï¼‰â†’ ä¸­ç­‰å½±å“ï¼ˆ70-85%ï¼‰
- ä½ç½®ä¿¡åº¦æƒé‡ï¼ˆ0.0-0.3ï¼‰â†’ è½»å¾®å½±å“ï¼ˆ10-40%ï¼‰

### **å®‰å…¨ä¿è¯**ï¼š
- æ•°å€¼ä¸ä¼šæº¢å‡ºï¼ˆè£å‰ªä¿æŠ¤ï¼‰
- æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼ˆé¿å…é¥±å’Œï¼‰
- FP16å…¼å®¹ï¼ˆèŒƒå›´å®‰å…¨ï¼‰

---

**ä¸»äººï¼Œç°åœ¨biasçš„ç¼©æ”¾å’Œè£å‰ªæœºåˆ¶éƒ½å·²ç»å®Œå–„äº†ï¼** ğŸ‰âœ¨

