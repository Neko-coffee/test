# Softmaxæ•æ„ŸåŒºé—´ä¸Scaleçº¦æŸåˆ†æ ğŸ“Š

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**é‡è¦æ€§**: ğŸ”¥ğŸ”¥ğŸ”¥ **å…³é”®æ•°å€¼ç¨³å®šæ€§é—®é¢˜**  
**é—®é¢˜**: é˜²æ­¢bias_scaleæ— é™å¢å¤§å¯¼è‡´softmaxé¥±å’Œ

---

## ğŸ¯ **æ ¸å¿ƒé—®é¢˜**

### **ä¸»äººçš„æ‹…å¿§**
```python
# å¯å­¦ä¹ çš„scaleå¯èƒ½æ— é™å¢å¤§
self.bias_scale = nn.Parameter(torch.tensor(2.5))

# è®­ç»ƒè¿‡ç¨‹ä¸­
epoch 1:  scale = 2.5
epoch 10: scale = 5.8
epoch 20: scale = 12.3  # âš ï¸ è¿‡å¤§ï¼
epoch 30: scale = 45.7  # âš ï¸ ç¾éš¾ï¼
```

**åæœ**ï¼š
- âŒ Softmaxé¥±å’Œï¼ˆè¾“å‡ºæ¥è¿‘one-hotï¼‰
- âŒ æ¢¯åº¦æ¶ˆå¤±
- âŒ è®­ç»ƒå´©æºƒ

---

## ğŸ“ **Softmaxæ•°å­¦åˆ†æ**

### **Softmaxå…¬å¼**

```python
# æ³¨æ„åŠ›è®¡ç®—
scores = Q @ K^T / sqrt(d)      # åŸå§‹åˆ†æ•°
scores = scores + bias          # ğŸ”¥ åŠ ä¸Šæˆ‘ä»¬çš„bias
attention = softmax(scores)     # softmaxå½’ä¸€åŒ–

# softmaxå®šä¹‰
softmax(x_i) = exp(x_i) / Î£ exp(x_j)
```

### **æ•æ„ŸåŒºé—´åˆ†æ**

#### **1. Softmaxçš„å“åº”æ›²çº¿**

```python
import numpy as np
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿ2ä¸ªkeyçš„æƒ…å†µ
x1 = 0  # key1çš„åˆ†æ•°ï¼ˆå›ºå®šï¼‰
x2_range = np.linspace(-10, 10, 200)  # key2çš„åˆ†æ•°ï¼ˆå˜åŒ–ï¼‰

# è®¡ç®—softmax
def softmax_2d(x1, x2):
    exp_sum = np.exp(x1) + np.exp(x2)
    return np.exp(x2) / exp_sum

attention_to_key2 = softmax_2d(x1, x2_range)

plt.plot(x2_range, attention_to_key2)
plt.xlabel('Score Difference (x2 - x1)')
plt.ylabel('Attention Weight to Key2')
plt.title('Softmax Sensitivity Curve')
plt.grid(True)
```

**å…³é”®å‘ç°**ï¼š

| Scoreå·®å€¼ | Attentionåˆ†å¸ƒ | çŠ¶æ€ |
|----------|--------------|------|
| **[-2, +2]** | [0.12, 0.88] | âœ… **æ•æ„ŸåŒºé—´**ï¼ˆæ¢¯åº¦å¤§ï¼‰ |
| **[-5, +5]** | [0.007, 0.993] | âš ï¸ æ¥è¿‘é¥±å’Œ |
| **[-10, +10]** | [0.00005, 0.99995] | âŒ å®Œå…¨é¥±å’Œï¼ˆæ¢¯åº¦â‰ˆ0ï¼‰ |

#### **2. å…·ä½“æ•°å€¼ç¤ºä¾‹**

```python
# åœºæ™¯1ï¼šæ¸©å’Œçš„biasï¼ˆæ•æ„ŸåŒºé—´ï¼‰
scores = [0.0, 0.0, 0.0, 0.0]  # 4ä¸ªkeyï¼Œåˆå§‹ç›¸ç­‰
bias = [0.0, 2.0, -1.0, 0.5]   # æ·»åŠ bias
final_scores = [0.0, 2.0, -1.0, 0.5]
attention = softmax(final_scores)
# ç»“æœï¼š[0.16, 0.59, 0.06, 0.19]  âœ… åˆ†å¸ƒåˆç†

# åœºæ™¯2ï¼šè¿‡å¤§çš„biasï¼ˆé¥±å’ŒåŒºé—´ï¼‰
bias = [0.0, 10.0, -5.0, 2.0]  # biasè¿‡å¤§
final_scores = [0.0, 10.0, -5.0, 2.0]
attention = softmax(final_scores)
# ç»“æœï¼š[0.0001, 0.9997, 0.0000, 0.0002]  âŒ æ¥è¿‘one-hot
```

---

## ğŸ”¬ **ç†è®ºæ¨å¯¼**

### **Softmaxæ¢¯åº¦**

```python
# Softmaxçš„æ¢¯åº¦
âˆ‚softmax(x_i)/âˆ‚x_i = softmax(x_i) * (1 - softmax(x_i))

# å…³é”®æ´å¯Ÿï¼š
# - å½“softmax(x_i) â‰ˆ 0.5æ—¶ï¼Œæ¢¯åº¦æœ€å¤§ = 0.25
# - å½“softmax(x_i) â‰ˆ 0.0æˆ–1.0æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘0
```

**æ¢¯åº¦ä¸è¾“å…¥çš„å…³ç³»**ï¼š

| è¾“å…¥å·®å€¼ | Softmaxè¾“å‡º | æ¢¯åº¦å¤§å° | çŠ¶æ€ |
|---------|------------|---------|------|
| 0 | 0.50 | 0.25 | âœ… æœ€å¤§æ¢¯åº¦ |
| Â±1 | 0.27/0.73 | 0.20 | âœ… é«˜æ¢¯åº¦ |
| Â±2 | 0.12/0.88 | 0.10 | âœ… ä¸­ç­‰æ¢¯åº¦ |
| Â±3 | 0.05/0.95 | 0.05 | âš ï¸ ä½æ¢¯åº¦ |
| Â±5 | 0.007/0.993 | 0.007 | âŒ æä½æ¢¯åº¦ |
| Â±10 | ~0/~1 | ~0 | âŒ æ¢¯åº¦æ¶ˆå¤± |

### **æœ€ä¼˜å·¥ä½œåŒºé—´**

```python
# åŸºäºç†è®ºå’Œå®è·µç»éªŒ
OPTIMAL_BIAS_RANGE = [-3, +3]   # æ•æ„ŸåŒºé—´
SAFE_BIAS_RANGE = [-5, +5]      # å®‰å…¨åŒºé—´
DANGER_BIAS_RANGE = [-10, +10]  # å±é™©åŒºé—´ï¼ˆæ¥è¿‘é¥±å’Œï¼‰
```

---

## âš™ï¸ **çº¦æŸç­–ç•¥**

### **æ–¹æ¡ˆ1ï¼šç¡¬çº¦æŸï¼ˆClampï¼‰** â­ **æ¨è**

```python
class AttentionBiasGenerator(nn.Module):
    def __init__(self, bias_scale=2.5, learnable_scale=True, 
                 max_scale=5.0):  # ğŸ”¥ æ·»åŠ æœ€å¤§å€¼é™åˆ¶
        super().__init__()
        
        if learnable_scale:
            self.bias_scale = nn.Parameter(torch.tensor(bias_scale))
        else:
            self.register_buffer('bias_scale', torch.tensor(bias_scale))
        
        self.max_scale = max_scale  # æœ€å¤§å…è®¸å€¼
    
    def forward(self, weights, ...):
        # ğŸ”¥ æ–¹æ³•1ï¼šåœ¨forwardä¸­clamp
        scale = torch.clamp(self.bias_scale, min=0.5, max=self.max_scale)
        bias = weights * scale
        
        # æœ€ç»ˆå†clampä¸€æ¬¡biasï¼ˆåŒé‡ä¿é™©ï¼‰
        bias = torch.clamp(bias, min=-5.0, max=5.0)
        return bias
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ç›´æ¥
- âœ… ä¿è¯ä¸ä¼šè¶…å‡ºèŒƒå›´
- âœ… æ— é¢å¤–è®¡ç®—å¼€é”€

**ç¼ºç‚¹**ï¼š
- âš ï¸ ç¡¬æˆªæ–­ï¼Œå¯èƒ½å½±å“æ¢¯åº¦

### **æ–¹æ¡ˆ2ï¼šè½¯çº¦æŸï¼ˆL2æ­£åˆ™ï¼‰**

```python
# åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æ­£åˆ™é¡¹
def compute_loss(self, ...):
    # åŸå§‹æ£€æµ‹æŸå¤±
    detection_loss = ...
    
    # ğŸ”¥ Scaleæ­£åˆ™åŒ–æŸå¤±
    scale = self.attention_bias_generator.bias_scale
    scale_penalty = 0.01 * (scale - 2.5) ** 2  # æƒ©ç½šåç¦»åˆå§‹å€¼
    
    total_loss = detection_loss + scale_penalty
    return total_loss
```

**ä¼˜ç‚¹**ï¼š
- âœ… è½¯çº¦æŸï¼Œæ¢¯åº¦å¹³æ»‘
- âœ… é¼“åŠ±scaleä¿æŒåœ¨åˆç†èŒƒå›´

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦è°ƒæ•´æ­£åˆ™åŒ–ç³»æ•°
- âš ï¸ ä¸èƒ½å®Œå…¨ä¿è¯ä¸è¶…é™

### **æ–¹æ¡ˆ3ï¼šå‚æ•°åŒ–çº¦æŸï¼ˆSigmoid/Tanhï¼‰** â­ **æœ€ä¼˜é›…**

```python
class AttentionBiasGenerator(nn.Module):
    def __init__(self, bias_scale=2.5, learnable_scale=True,
                 min_scale=0.5, max_scale=5.0):
        super().__init__()
        
        if learnable_scale:
            # ğŸ”¥ å­¦ä¹ ä¸€ä¸ªæ— çº¦æŸçš„å‚æ•°
            self._scale_raw = nn.Parameter(torch.tensor(0.0))
        else:
            self.register_buffer('bias_scale', torch.tensor(bias_scale))
        
        self.min_scale = min_scale
        self.max_scale = max_scale
        self.learnable_scale = learnable_scale
    
    @property
    def bias_scale(self):
        """é€šè¿‡sigmoidå°†æ— çº¦æŸå‚æ•°æ˜ å°„åˆ°[min_scale, max_scale]"""
        if self.learnable_scale:
            # sigmoid: (-âˆ, +âˆ) â†’ (0, 1)
            normalized = torch.sigmoid(self._scale_raw)
            # çº¿æ€§æ˜ å°„åˆ°[min_scale, max_scale]
            scale = self.min_scale + (self.max_scale - self.min_scale) * normalized
            return scale
        else:
            return self._buffers['bias_scale']
    
    def forward(self, weights, ...):
        # ğŸ”¥ bias_scaleè‡ªåŠ¨è¢«çº¦æŸåœ¨[min_scale, max_scale]
        bias = weights * self.bias_scale
        return bias
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¼˜é›…ï¼Œæ— éœ€æ‰‹åŠ¨clamp
- âœ… æ¢¯åº¦å§‹ç»ˆå­˜åœ¨ï¼ˆsigmoidå¤„å¤„å¯å¯¼ï¼‰
- âœ… è‡ªåŠ¨ä¿è¯èŒƒå›´

**ç¼ºç‚¹**ï¼š
- âš ï¸ ç¨å¾®å¤æ‚ä¸€ç‚¹

---

## ğŸ¯ **æ¨èæ–¹æ¡ˆ**

### **ç»¼åˆæ–¹æ¡ˆï¼šClamp + ç›‘æ§**

```python
class AttentionBiasGenerator(nn.Module):
    def __init__(self, 
                 bias_scale=2.5, 
                 learnable_scale=True,
                 min_scale=0.5,    # ğŸ”¥ æœ€å°å€¼
                 max_scale=5.0,    # ğŸ”¥ æœ€å¤§å€¼
                 warn_threshold=4.0):  # ğŸ”¥ è­¦å‘Šé˜ˆå€¼
        super().__init__()
        
        if learnable_scale:
            self.bias_scale = nn.Parameter(torch.tensor(bias_scale))
        else:
            self.register_buffer('bias_scale', torch.tensor(bias_scale))
        
        self.min_scale = min_scale
        self.max_scale = max_scale
        self.warn_threshold = warn_threshold
        self.learnable_scale = learnable_scale
    
    def forward(self, weights, ...):
        # ğŸ”¥ Step 1: Clamp scale
        if self.learnable_scale:
            scale = torch.clamp(self.bias_scale, 
                              min=self.min_scale, 
                              max=self.max_scale)
            
            # ğŸ”¥ Step 2: ç›‘æ§å’Œè­¦å‘Š
            if self.training and scale > self.warn_threshold:
                if torch.rand(1).item() < 0.01:  # 1%æ¦‚ç‡æ‰“å°ï¼Œé¿å…åˆ·å±
                    print(f"âš ï¸ Warning: bias_scale = {scale.item():.2f} "
                          f"(approaching max={self.max_scale})")
        else:
            scale = self.bias_scale
        
        # ğŸ”¥ Step 3: è®¡ç®—bias
        bias = weights * scale  # weights âˆˆ [-1, 1]
        
        # ğŸ”¥ Step 4: åŒé‡ä¿é™© - clampæœ€ç»ˆbias
        # ç¡®ä¿biasåœ¨softmaxæ•æ„ŸåŒºé—´å†…
        bias = torch.clamp(bias, min=-5.0, max=5.0)
        
        return bias
```

---

## ğŸ“Š **å®éªŒéªŒè¯**

### **æµ‹è¯•ä¸åŒscaleçš„æ•ˆæœ**

```python
import torch
import torch.nn.functional as F

def test_softmax_saturation(bias_scale):
    """æµ‹è¯•ä¸åŒscaleä¸‹çš„softmaxé¥±å’Œç¨‹åº¦"""
    
    # æ¨¡æ‹Ÿåœºæ™¯ï¼š4ä¸ªkeyï¼ŒAQRç»™å‡ºæƒé‡
    weights = torch.tensor([0.0, 0.8, -0.6, 0.3])  # AQRæƒé‡ âˆˆ [-1, 1]
    
    # åŸå§‹attention scoresï¼ˆå‡è®¾éƒ½æ˜¯0ï¼‰
    scores = torch.zeros(4)
    
    # æ·»åŠ bias
    bias = weights * bias_scale
    final_scores = scores + bias
    
    # è®¡ç®—attention
    attention = F.softmax(final_scores, dim=0)
    
    # è®¡ç®—ç†µï¼ˆè¡¡é‡åˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦ï¼‰
    entropy = -(attention * torch.log(attention + 1e-8)).sum()
    max_entropy = torch.log(torch.tensor(4.0))  # å‡åŒ€åˆ†å¸ƒçš„ç†µ
    
    print(f"\nScale = {bias_scale:.1f}")
    print(f"  Bias: {bias.tolist()}")
    print(f"  Attention: {attention.tolist()}")
    print(f"  Entropy: {entropy.item():.3f} / {max_entropy.item():.3f}")
    print(f"  Max attention: {attention.max().item():.3f}")
    
    # åˆ¤æ–­æ˜¯å¦é¥±å’Œ
    if attention.max() > 0.9:
        print(f"  âš ï¸ é¥±å’Œï¼")
    elif attention.max() > 0.7:
        print(f"  âš ï¸ æ¥è¿‘é¥±å’Œ")
    else:
        print(f"  âœ… æ­£å¸¸")

# æµ‹è¯•ä¸åŒscale
for scale in [1.0, 2.5, 5.0, 10.0, 20.0]:
    test_softmax_saturation(scale)
```

**é¢„æœŸè¾“å‡º**ï¼š

```
Scale = 1.0
  Bias: [0.0, 0.8, -0.6, 0.3]
  Attention: [0.18, 0.40, 0.10, 0.24]
  Entropy: 1.289 / 1.386
  Max attention: 0.40
  âœ… æ­£å¸¸

Scale = 2.5
  Bias: [0.0, 2.0, -1.5, 0.75]
  Attention: [0.12, 0.59, 0.06, 0.19]
  Entropy: 1.089 / 1.386
  Max attention: 0.59
  âœ… æ­£å¸¸

Scale = 5.0
  Bias: [0.0, 4.0, -3.0, 1.5]
  Attention: [0.04, 0.81, 0.01, 0.08]
  Entropy: 0.698 / 1.386
  Max attention: 0.81
  âš ï¸ æ¥è¿‘é¥±å’Œ

Scale = 10.0
  Bias: [0.0, 8.0, -6.0, 3.0]
  Attention: [0.001, 0.973, 0.000, 0.006]
  Entropy: 0.158 / 1.386
  Max attention: 0.973
  âš ï¸ é¥±å’Œï¼

Scale = 20.0
  Bias: [0.0, 16.0, -12.0, 6.0]
  Attention: [0.000, 0.9999, 0.000, 0.000]
  Entropy: 0.001 / 1.386
  Max attention: 0.9999
  âš ï¸ é¥±å’Œï¼
```

---

## ğŸ“‹ **é…ç½®å»ºè®®**

### **ä¿å®ˆé…ç½®ï¼ˆæ¨èï¼‰**

```python
attention_bias_config=dict(
    bias_scale=2.5,           # åˆå§‹å€¼
    learnable_scale=True,     # å¯å­¦ä¹ 
    min_scale=0.5,           # ğŸ”¥ æœ€å°å€¼ï¼ˆé¿å…é€€åŒ–ï¼‰
    max_scale=5.0,           # ğŸ”¥ æœ€å¤§å€¼ï¼ˆé¿å…é¥±å’Œï¼‰
    warn_threshold=4.0       # ğŸ”¥ è­¦å‘Šé˜ˆå€¼
)
```

### **æ¿€è¿›é…ç½®**

```python
attention_bias_config=dict(
    bias_scale=3.5,           # æ›´å¤§çš„åˆå§‹å€¼
    learnable_scale=True,
    min_scale=1.0,
    max_scale=8.0,           # å…è®¸æ›´å¤§çš„scale
    warn_threshold=6.0
)
```

### **è¶…ä¿å®ˆé…ç½®**

```python
attention_bias_config=dict(
    bias_scale=2.0,
    learnable_scale=True,
    min_scale=0.5,
    max_scale=3.0,           # ä¸¥æ ¼é™åˆ¶
    warn_threshold=2.5
)
```

---

## ğŸ” **ç›‘æ§æŒ‡æ ‡**

### **è®­ç»ƒæ—¶ç›‘æ§**

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
if iteration % 100 == 0:
    scale = model.pts_bbox_head.attention_bias_generator.bias_scale.item()
    
    # æ£€æŸ¥æ˜¯å¦æ¥è¿‘ä¸Šé™
    max_scale = model.pts_bbox_head.attention_bias_generator.max_scale
    if scale > 0.8 * max_scale:
        print(f"âš ï¸ Scaleæ¥è¿‘ä¸Šé™: {scale:.2f} / {max_scale}")
    
    # æ£€æŸ¥æ¢¯åº¦
    if model.pts_bbox_head.attention_bias_generator.bias_scale.grad is not None:
        grad = model.pts_bbox_head.attention_bias_generator.bias_scale.grad.item()
        print(f"Scale gradient: {grad:.6f}")
```

### **è¯„ä¼°attentionåˆ†å¸ƒ**

```python
# åœ¨forwardä¸­ä¸´æ—¶æ·»åŠ 
def forward(self, weights, ...):
    bias = weights * self.bias_scale
    
    # ğŸ”¥ ç›‘æ§biasçš„ç»Ÿè®¡ä¿¡æ¯
    if self.training and torch.rand(1).item() < 0.01:
        print(f"Bias stats: mean={bias.mean():.3f}, "
              f"std={bias.std():.3f}, "
              f"max={bias.max():.3f}, "
              f"min={bias.min():.3f}")
    
    return bias
```

---

## ğŸ“ **ç†è®ºæ€»ç»“**

### **Softmaxæ•æ„ŸåŒºé—´**

| åŒºé—´ | èŒƒå›´ | ç‰¹å¾ | å»ºè®® |
|-----|------|------|------|
| **é«˜æ•æ„ŸåŒº** | [-2, +2] | æ¢¯åº¦å¤§ï¼Œå­¦ä¹ å¿« | âœ… æœ€ä¼˜å·¥ä½œåŒº |
| **ä¸­æ•æ„ŸåŒº** | [-3, +3] | æ¢¯åº¦ä¸­ç­‰ | âœ… å®‰å…¨åŒº |
| **ä½æ•æ„ŸåŒº** | [-5, +5] | æ¢¯åº¦å° | âš ï¸ è¾¹ç¼˜åŒº |
| **é¥±å’ŒåŒº** | >Â±5 | æ¢¯åº¦æ¶ˆå¤± | âŒ å±é™©åŒº |

### **Scaleçº¦æŸåŸåˆ™**

```python
# åŸºäºAQRæƒé‡èŒƒå›´ [-1, 1]
weights âˆˆ [-1, 1]
bias = weights Ã— scale

# è¦ä¿è¯ bias âˆˆ [-3, +3]ï¼ˆæ•æ„ŸåŒºé—´ï¼‰
# åˆ™ scale â‰¤ 3

# è¦ä¿è¯ bias âˆˆ [-5, +5]ï¼ˆå®‰å…¨åŒºé—´ï¼‰
# åˆ™ scale â‰¤ 5

# æ¨èé…ç½®
min_scale = 0.5   # é¿å…å®Œå…¨é€€åŒ–
max_scale = 5.0   # é¿å…é¥±å’Œ
optimal_scale = 2.5  # åˆå§‹å€¼
```

---

## ğŸš€ **å®ç°è®¡åˆ’**

### **Step 1: æ·»åŠ çº¦æŸå‚æ•°**
```python
# attention_bias_generator.py
def __init__(self, ..., min_scale=0.5, max_scale=5.0):
    self.min_scale = min_scale
    self.max_scale = max_scale
```

### **Step 2: åœ¨forwardä¸­clamp**
```python
def forward(self, weights, ...):
    if self.learnable_scale:
        scale = torch.clamp(self.bias_scale, self.min_scale, self.max_scale)
    else:
        scale = self.bias_scale
    
    bias = weights * scale
    bias = torch.clamp(bias, min=-5.0, max=5.0)  # åŒé‡ä¿é™©
    return bias
```

### **Step 3: æ›´æ–°é…ç½®**
```python
# cmt_aqr_voxel0100_r50_800x320_cbgs.py
attention_bias_config=dict(
    bias_scale=2.5,
    learnable_scale=True,
    min_scale=0.5,
    max_scale=5.0,
)
```

---

**ä¸»äººï¼Œæ‚¨çš„æ‹…å¿§éå¸¸æ­£ç¡®ï¼** ğŸ¯

**æ ¸å¿ƒç»“è®º**ï¼š
1. âœ… **å¿…é¡»çº¦æŸscale**ï¼šé˜²æ­¢é¥±å’Œå’Œæ¢¯åº¦æ¶ˆå¤±
2. âœ… **æ¨èèŒƒå›´**ï¼š`[0.5, 5.0]`ï¼Œåˆå§‹å€¼`2.5`
3. âœ… **å®ç°æ–¹å¼**ï¼š`torch.clamp` + ç›‘æ§
4. âœ… **ç†è®ºä¾æ®**ï¼šSoftmaxæ•æ„ŸåŒºé—´åœ¨`[-3, +3]`

**ä¸‹ä¸€æ­¥**ï¼šç«‹å³å®ç°scaleçº¦æŸï¼ğŸš€

