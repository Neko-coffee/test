# AQR Attention Bias æ­£ç¡®å®ç°æ–¹æ¡ˆ ğŸ¯

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**ç›®çš„**: åŸºäºåŒå­¦æä¾›çš„ä¼ªä»£ç ï¼Œç»“åˆCMTå®é™…æ¶æ„ï¼Œç»™å‡ºæ­£ç¡®çš„å®ç°æ–¹æ¡ˆ

---

## ğŸ“‹ **ä¼ªä»£ç åˆ†æ**

### âœ… **æ­£ç¡®çš„æ ¸å¿ƒæ€æƒ³**ï¼š
1. ä½¿ç”¨attn_maskçš„floatæ¨¡å¼ âœ…
2. åœ¨softmaxå‰åº”ç”¨bias âœ…
3. ç«¯åˆ°ç«¯å¯è®­ç»ƒ âœ…

### âŒ **éœ€è¦ä¿®æ­£çš„åœ°æ–¹**ï¼š
1. CMTä¸­BEVå’ŒCameraå·²ç»èåˆï¼Œä¸éœ€è¦å†concat
2. AQRè¾“å‡ºçš„æ˜¯per-queryæƒé‡ï¼Œéœ€è¦æ‰©å±•åˆ°ç©ºé—´
3. åº”è¯¥ä½¿ç”¨å±€éƒ¨çª—å£biasï¼Œè€Œéå…¨å±€repeat

---

## ğŸ”§ **CMTæ¶æ„ä¸‹çš„æ­£ç¡®å®ç°**

### **Step 1ï¼šCmtTransformerä¸­çš„å¤„ç†**

```python
# æ–‡ä»¶ï¼šcmt_transformer.py

@TRANSFORMER.register_module()
class CmtTransformer(BaseModule):
    
    def forward(self, x, x_img, query_embed, bev_pos_embed, rv_pos_embed, 
                attn_masks=None, attention_bias=None, reg_branch=None):
        """
        Args:
            x: [bs, c, h, w] BEVç‰¹å¾
            x_img: [bs*views, c, h, w] Cameraç‰¹å¾
            attention_bias: [bs, num_queries, num_features] AQRç”Ÿæˆçš„bias
        """
        
        bs, c, h, w = x.shape
        
        # 1. ç‰¹å¾å±•å¹³å’Œèåˆï¼ˆCMTå·²æœ‰é€»è¾‘ï¼‰
        bev_memory = rearrange(x, "bs c h w -> (h w) bs c")  # [32400, bs, 256]
        rv_memory = rearrange(x_img, "(bs v) c h w -> (v h w) bs c", bs=bs)  # [24000, bs, 256]
        
        # ğŸ”¥ å…³é”®ï¼šmemoryå·²ç»æ˜¯èåˆçš„
        memory = torch.cat([bev_memory, rv_memory], dim=0)  # [56400, bs, 256]
        #                    â†‘ å‰32400æ˜¯BEV  â†‘ å24000æ˜¯Camera
        
        # 2. ä½ç½®ç¼–ç èåˆ
        pos_embed = torch.cat([bev_pos_embed, rv_pos_embed], dim=0)
        
        # 3. ğŸ”¥ å¤„ç†attention_biasç»´åº¦
        # è¾“å…¥: [bs, num_queries, num_features=56400]
        # éœ€è¦: [num_queries, bs, num_features]
        if attention_bias is not None:
            attention_bias = attention_bias.transpose(0, 1)
            # â†’ [num_queries, bs, 56400]
            
            # attention_biasçš„ç»“æ„ï¼š
            # [:, :, :32400] â†’ BEVçš„bias
            # [:, :, 32400:] â†’ Cameraçš„bias
        
        # 4. ä¼ é€’ç»™Decoder
        query_embed = query_embed.transpose(0, 1)
        target = torch.zeros_like(query_embed)
        
        out_dec = self.decoder(
            query=target,                    # [num_queries, bs, 256]
            key=memory,                      # [56400, bs, 256]
            value=memory,                    # [56400, bs, 256]
            key_pos=pos_embed,
            query_pos=query_embed,
            key_padding_mask=mask,
            attn_masks=[attn_masks, None],   # DN maskï¼ˆself-attnç”¨ï¼‰
            attention_bias=attention_bias,   # ğŸ”¥ AQR biasï¼ˆcross-attnç”¨ï¼‰
            reg_branch=reg_branch,
        )
        
        return out_dec, memory
```

---

### **Step 2ï¼šPETRMultiheadAttentionä¸­çš„å¤„ç†**

```python
# æ–‡ä»¶ï¼špetr_transformer.py

@ATTENTION.register_module()
class PETRMultiheadAttention(BaseModule):
    
    def forward(self, query, key=None, value=None,
                identity=None, query_pos=None, key_pos=None,
                attn_mask=None,           # åŸæœ‰çš„maskï¼ˆDNç”¨ï¼‰
                attention_bias=None,      # ğŸ”¥ æ–°å¢ï¼šAQR bias
                key_padding_mask=None,
                **kwargs):
        """
        Args:
            query: [num_queries, bs, 256] æˆ– [bs, num_queries, 256]
            key: [num_features, bs, 256] æˆ– [bs, num_features, 256]
            value: åŒkey
            attention_bias: [num_queries, bs, num_features] AQR bias
        """
        
        # 1. æ ‡å‡†å¤„ç†ï¼ˆä½ç½®ç¼–ç ç­‰ï¼‰
        if key is None:
            key = query
        if value is None:
            value = key
        if identity is None:
            identity = query
        
        if query_pos is not None:
            query = query + query_pos
        if key_pos is not None:
            key = key + key_pos
        
        # 2. ğŸ”¥ å¤„ç†attention_bias
        final_attn_mask = attn_mask
        
        if attention_bias is not None:
            # attention_bias: [num_queries, bs, num_features]
            
            # Step 2.1: åˆ¤æ–­æ˜¯self-attnè¿˜æ˜¯cross-attn
            is_cross_attn = (key.shape[0] != query.shape[0])
            
            if is_cross_attn:
                # Cross-Attention: åº”ç”¨AQR bias
                
                # Step 2.2: ç»´åº¦è½¬æ¢ä¸ºMultiheadAttentionè¦æ±‚çš„æ ¼å¼
                # PyTorch MultiheadAttentionæœŸæœ›ï¼š
                # - å¦‚æœbatch_first=False: attn_mask [num_queries, num_features]
                # - å¦‚æœbatch_first=True: attn_mask [bs, num_queries, num_features]
                # - å¦‚æœ3D: attn_mask [bs*num_heads, num_queries, num_features]
                
                if self.batch_first:
                    # [num_queries, bs, num_features] â†’ [bs, num_queries, num_features]
                    bias = attention_bias.transpose(0, 1)
                else:
                    # ä¿æŒ [num_queries, bs, num_features]
                    # ä½†PyTorchæœŸæœ›2Dæˆ–3Dï¼Œè¿™é‡Œéœ€è¦å¤„ç†
                    # é€‰æ‹©ï¼šæ‰©å±•åˆ°3D [bs*num_heads, num_queries, num_features]
                    num_queries, bs, num_features = attention_bias.shape
                    
                    # æ‰©å±•åˆ°å¤šå¤´
                    bias = attention_bias.transpose(0, 1)  # [bs, num_queries, num_features]
                    bias = bias.unsqueeze(1)  # [bs, 1, num_queries, num_features]
                    bias = bias.expand(-1, self.num_heads, -1, -1)  # [bs, num_heads, num_queries, num_features]
                    bias = bias.reshape(bs * self.num_heads, num_queries, num_features)
                    # â†’ [bs*num_heads, num_queries, num_features]
                
                # Step 2.3: ä¸åŸæœ‰attn_maskåˆå¹¶
                if final_attn_mask is not None:
                    # éœ€è¦ç¡®ä¿ç»´åº¦å…¼å®¹
                    if final_attn_mask.dtype == torch.bool:
                        # Bool maskè½¬ä¸ºfloat
                        mask_float = torch.zeros_like(bias)
                        # å¹¿æ’­å¤„ç†
                        if final_attn_mask.dim() == 2:
                            # [num_queries, num_features]
                            final_attn_mask = final_attn_mask.unsqueeze(0).expand(bs * self.num_heads, -1, -1)
                        mask_float.masked_fill_(final_attn_mask, float('-inf'))
                        final_attn_mask = mask_float + bias
                    else:
                        # Float maskç›´æ¥åŠ 
                        final_attn_mask = final_attn_mask + bias
                else:
                    final_attn_mask = bias
            
            # else: Self-Attentionä¸ä½¿ç”¨attention_biasï¼Œä¿æŒåŸæœ‰attn_mask
        
        # 3. å¤„ç†batch_first
        if self.batch_first:
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)
        
        # 4. ğŸ”¥ è°ƒç”¨PyTorch MultiheadAttention
        out = self.attn(
            query=query,
            key=key,
            value=value,
            attn_mask=final_attn_mask,  # ğŸ”¥ å·²åŒ…å«AQR bias
            key_padding_mask=key_padding_mask
        )[0]
        
        # 5. æ¢å¤ç»´åº¦
        if self.batch_first:
            out = out.transpose(0, 1)
        
        # 6. æŠ•å½±å’Œdropout
        out = self.proj_drop(out)
        
        return identity + self.dropout_layer(out)
```

---

### **Step 3ï¼šAttentionBiasGeneratorç”Ÿæˆå±€éƒ¨bias**

```python
# æ–‡ä»¶ï¼šattention_bias_generator.pyï¼ˆå·²å®ç°ï¼‰

class AttentionBiasGenerator(BaseModule):
    
    def forward(self, lidar_weights, camera_weights, 
                pts_bev_indices, pts_pers_indices):
        """
        ç”Ÿæˆç©ºé—´æ„ŸçŸ¥çš„å±€éƒ¨bias
        
        Args:
            lidar_weights: [bs, num_queries] AQRç”Ÿæˆçš„LiDARæƒé‡
            camera_weights: [bs, num_queries] AQRç”Ÿæˆçš„Cameraæƒé‡
            pts_bev_indices: [bs, num_queries] queryåœ¨BEVä¸­çš„ä½ç½®
            pts_pers_indices: [bs, num_queries, 3] queryåœ¨é€è§†å›¾ä¸­çš„ä½ç½®
        
        Returns:
            attention_bias: [bs, num_queries, 56400]
                å‰32400ç»´ï¼šBEVçš„å±€éƒ¨çª—å£bias
                å24000ç»´ï¼šCameraçš„å±€éƒ¨çª—å£bias
        """
        
        # 1. ç”ŸæˆBEVå±€éƒ¨bias
        bev_bias = self._generate_bev_bias(
            lidar_weights,      # [bs, num_queries]
            pts_bev_indices     # [bs, num_queries]
        )  # â†’ [bs, num_queries, 32400]
        
        # å¯¹äºæ¯ä¸ªqueryï¼š
        # - åœ¨æŠ•å½±ä½ç½®çš„15Ã—15çª—å£å†…ï¼šbias = lidar_weights[q]
        # - çª—å£å¤–ï¼šbias = 0
        
        # 2. ç”ŸæˆCameraå±€éƒ¨bias
        camera_bias = self._generate_camera_bias(
            camera_weights,     # [bs, num_queries]
            pts_pers_indices    # [bs, num_queries, 3]
        )  # â†’ [bs, num_queries, 24000]
        
        # 3. æ‹¼æ¥
        attention_bias = torch.cat([bev_bias, camera_bias], dim=-1)
        # â†’ [bs, num_queries, 56400]
        
        return attention_bias
```

---

## ğŸ“Š **ä¸ä¼ªä»£ç çš„å¯¹æ¯”**

| æ–¹é¢ | ä¼ªä»£ç  | æˆ‘ä»¬çš„å®ç° |
|-----|-------|----------|
| **Key/Valueå¤„ç†** | åˆ†ç¦»åconcat | å·²èåˆçš„memory |
| **Biasç”Ÿæˆ** | å…¨å±€repeat | å±€éƒ¨çª—å£ |
| **Biaså½¢çŠ¶** | [B, Nq, 2] â†’ repeat | [B, Nq, 56400] |
| **ç©ºé—´ä¿¡æ¯** | æ—  | åŸºäºæŠ•å½±ä½ç½® |
| **åˆå¹¶æ–¹å¼** | attn_mask + bias âœ… | åŒæ · âœ… |

---

## ğŸ¯ **æ ¸å¿ƒå·®å¼‚**

### **ä¼ªä»£ç çš„ç†è§£ï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š
```python
# å¯¹æ¯ä¸ªqueryï¼Œä¸¤ä¸ªæ ‡é‡æƒé‡
alpha_cam = 0.7  # query #42å¯¹cameraçš„æƒé‡
alpha_lidar = 0.3  # query #42å¯¹lidarçš„æƒé‡

# å…¨å±€åº”ç”¨
bias = [0.3, 0.3, ..., 0.3,  # æ‰€æœ‰BEVç‰¹å¾éƒ½æ˜¯0.3
        0.7, 0.7, ..., 0.7]  # æ‰€æœ‰Cameraç‰¹å¾éƒ½æ˜¯0.7
```

### **æˆ‘ä»¬çš„å®ç°ï¼ˆç²¾ç»†ç‰ˆï¼‰**ï¼š
```python
# å¯¹æ¯ä¸ªqueryï¼Œç”Ÿæˆç©ºé—´æ„ŸçŸ¥çš„bias
query #42æŠ•å½±åˆ°BEV (90, 90)

# å±€éƒ¨çª—å£bias
bias = [
    0, 0, ..., 0,               # BEVçª—å£å¤–
    0.3, 0.3, ..., 0.3,         # BEVå±€éƒ¨çª—å£(15Ã—15)å†…
    0, 0, ..., 0,               # BEVçª—å£å¤–
    0, 0, ..., 0,               # Cameraçª—å£å¤–
    0.7, 0.7, ..., 0.7,         # Cameraå±€éƒ¨çª—å£å†…
    0, 0, ..., 0                # Cameraçª—å£å¤–
]
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç©ºé—´å…ˆéªŒï¼šåªå¢å¼ºqueryå…³æ³¨çš„å±€éƒ¨åŒºåŸŸ
- âœ… æ›´ç²¾ç»†ï¼šä¸åŒqueryçš„biasåˆ†å¸ƒä¸åŒï¼ˆåŸºäºæŠ•å½±ä½ç½®ï¼‰
- âœ… æ›´åˆç†ï¼šç¬¦åˆå±€éƒ¨æ€§åŸåˆ™

---

## âœ… **æœ€ç»ˆæ–¹æ¡ˆæ€»ç»“**

### **ä¿ç•™ä¼ªä»£ç çš„ä¼˜ç‚¹**ï¼š
1. âœ… ä½¿ç”¨attn_maskçš„floatæ¨¡å¼
2. âœ… åœ¨MultiheadAttentionä¸­åº”ç”¨
3. âœ… ç«¯åˆ°ç«¯å¯è®­ç»ƒ

### **ä¿®æ­£çš„å…³é”®ç‚¹**ï¼š
1. âœ… ä¸éœ€è¦concat Key/Valueï¼ˆCMTå·²èåˆï¼‰
2. âœ… ä½¿ç”¨å±€éƒ¨çª—å£biasï¼ˆè€Œéå…¨å±€repeatï¼‰
3. âœ… æ­£ç¡®çš„ç»´åº¦è½¬æ¢
4. âœ… åŒºåˆ†self-attnå’Œcross-attn

### **å®ç°å¤æ‚åº¦**ï¼š
- PETRMultiheadAttentionä¿®æ”¹ï¼š~30è¡Œä»£ç 
- å…¶ä»–éƒ¨åˆ†å·²å®Œæˆ
- é¢„è®¡æ€»å·¥ä½œé‡ï¼š30-40åˆ†é’Ÿ

---

**ä¸»äººï¼Œä¼ªä»£ç çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ­£ç¡®çš„ï¼Œä½†éœ€è¦é€‚é…CMTçš„æ¶æ„ï¼æˆ‘ä»¬çš„æ–¹æ¡ˆæ›´ç²¾ç»†ã€æ›´ç¬¦åˆç©ºé—´å…ˆéªŒï¼** âœ…ğŸ¾

