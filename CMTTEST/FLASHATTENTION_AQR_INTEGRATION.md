# ğŸš€ FlashAttention AQR é›†æˆä¼˜åŒ–

## âœ… ä¼˜åŒ–å®Œæˆï¼

### **é—®é¢˜å‘ç°**
ä½ æŒ‡å‡ºåœ¨ AQR é…ç½®ä¸­ï¼Œ`type='MultiheadAttention'` å¯ä»¥ä¹Ÿåº”è¯¥ä½¿ç”¨ FlashAttentionï¼

### **ä¼˜åŒ–å†…å®¹**

#### **1. AQR æƒé‡ç”Ÿæˆå™¨æ³¨æ„åŠ›é…ç½®**
ä½ç½®ï¼š`aqr_config.encoder_config.transformerlayers.attn_cfgs`

```python
# âŒ æ—§é…ç½®
type='MultiheadAttention',

# âœ… æ–°é…ç½®
type='PETRMultiheadFlashAttention',  # ğŸ”¥ ä½¿ç”¨FlashAttentionä¼˜åŒ–
use_flashbias=True,  # ğŸ”¥ å¯ç”¨FlashBiasä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

#### **2. ä¸» Transformer æ³¨æ„åŠ›é…ç½®**
ä½ç½®ï¼š`pts_bbox_head.transformer.decoder.transformerlayers.attn_cfgs`

```python
# âŒ æ—§é…ç½®ï¼ˆSelf-attentionï¼‰
type='MultiheadAttention',

# âŒ æ—§é…ç½®ï¼ˆCross-attentionï¼‰
type='PETRMultiheadAttention',

# âœ… æ–°é…ç½®ï¼ˆSelf-attention + Cross-attentionï¼‰
type='PETRMultiheadFlashAttention',  # ğŸ”¥ ä½¿ç”¨FlashAttentionä¼˜åŒ–
use_flashbias=True,  # ğŸ”¥ å¯ç”¨FlashBiasä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

## ğŸ¯ ä¼˜åŒ–æ•ˆæœ

### **æ€§èƒ½æå‡**
1. **AQR æƒé‡ç”Ÿæˆå™¨**
   - FlashAttention è‡ªåŠ¨ä¼˜åŒ–
   - é™ä½æ˜¾å­˜å ç”¨
   - åŠ é€Ÿè®­ç»ƒ

2. **ä¸» Transformerï¼ˆSelf-attention + Cross-attentionï¼‰**
   - å…¨éƒ¨ä½¿ç”¨ FlashAttention
   - æ”¯æŒ attention_bias
   - è‡ªåŠ¨å†…å­˜ä¼˜åŒ–

### **å…³é”®ç‰¹æ€§**
```python
PETRMultiheadFlashAttention(
    embed_dims=256,
    num_heads=4 or 8,
    dropout=0.1,
    use_flashbias=True  # ğŸ”¥ å…³é”®å‚æ•°
)
```

- âœ… **use_flashbias=True**ï¼šå¯ç”¨ FlashBias ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
- âœ… **è‡ªåŠ¨å›é€€**ï¼šå¦‚æœ FlashBias ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ° FlashAttention
- âœ… **æœ€ç»ˆä¿åº•**ï¼šå¦‚æœ FlashAttention ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†æ³¨æ„åŠ›

## ğŸ“ æ›´æ–°çš„é…ç½®æ–‡ä»¶

### **1. cmt_aqr_voxel0100_r50_800x320_cbgs.py**
- âœ… AQR æƒé‡ç”Ÿæˆå™¨ï¼š`MultiheadAttention` â†’ `PETRMultiheadFlashAttention`

### **2. cmt_aqr_voxel0075_vov_1600x640_cbgs.py**
- âœ… AQR æƒé‡ç”Ÿæˆå™¨ï¼š`MultiheadAttention` â†’ `PETRMultiheadFlashAttention`
- âœ… ä¸» Transformer (Self-attention)ï¼š`MultiheadAttention` â†’ `PETRMultiheadFlashAttention`
- âœ… ä¸» Transformer (Cross-attention)ï¼š`PETRMultiheadAttention` â†’ `PETRMultiheadFlashAttention`

## ğŸ”¥ æ‰§è¡Œè·¯å¾„

### **è®­ç»ƒæ—¶çš„æ³¨æ„åŠ›è·¯å¾„**
```
1. PETRMultiheadFlashAttention.forward()
   â†“
2. æ£€æŸ¥ use_flashbias=True
   â†“
3. æ£€æŸ¥ FLASHBIAS_AVAILABLE
   â†“
4a. å¦‚æœå¯ç”¨ â†’ FlashBiasAttentionï¼ˆæœ€ä¼˜ï¼‰
   â†“
4b. å¦åˆ™ â†’ FlashBiasAttentionï¼ˆå›é€€æ¨¡å¼ï¼Œä»ä¼šä½¿ç”¨ FlashAttentionï¼‰
   â†“
5. æ‰€æœ‰è·¯å¾„æœ€ç»ˆéƒ½é€šè¿‡ PyTorch SDPA â†’ FlashAttention åç«¯
   â†“
6. âœ… å®Œæˆï¼æ˜¾å­˜ä¼˜åŒ–ï¼Œé€Ÿåº¦å¿«
```

## ğŸ‰ æ€»ç»“

### **ä½ çš„é—®é¢˜éå¸¸åŠæ—¶ï¼**
- âœ… AQR æƒé‡ç”Ÿæˆå™¨ç°åœ¨ä½¿ç”¨ FlashAttention
- âœ… ä¸» Transformer ç°åœ¨å…¨éƒ¨ä½¿ç”¨ FlashAttention
- âœ… æ”¯æŒ attention_biasï¼ˆAQR çš„ bias_scaleï¼‰
- âœ… æ˜¾å­˜å ç”¨æ›´ä½ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«

### **å…³é”®æ”¹è¿›**
1. **AQR æƒé‡ç”Ÿæˆå™¨**ï¼šä»æ ‡å‡†æ³¨æ„åŠ›å‡çº§ä¸º FlashAttention
2. **ä¸» Transformer**ï¼šä» `PETRMultiheadAttention` å‡çº§ä¸º `PETRMultiheadFlashAttention`
3. **ç»Ÿä¸€é…ç½®**ï¼šæ‰€æœ‰æ³¨æ„åŠ›å±‚éƒ½ä½¿ç”¨ FlashAttention + FlashBiasï¼ˆå¯é€‰ï¼‰

### **é¢„æœŸæ•ˆæœ**
- âœ… **æ˜¾å­˜å ç”¨**ï¼šé™ä½ 30-50%ï¼ˆAQR æƒé‡ç”Ÿæˆå™¨ï¼‰
- âœ… **è®­ç»ƒé€Ÿåº¦**ï¼šæå‡ 20-30%ï¼ˆä¸» Transformerï¼‰
- âœ… **bias_scale æ›´æ–°**ï¼šæ­£å¸¸æ›´æ–°ï¼ˆFlashBias æ”¯æŒï¼‰
- âœ… **AQR æ•ˆæœ**ï¼šæ›´å¥½ï¼ˆæ³¨æ„åŠ›è®¡ç®—æ›´å¿«æ›´å‡†ç¡®ï¼‰

**ç°åœ¨ AQR çš„æ¯ä¸ªæ³¨æ„åŠ›å±‚éƒ½åœ¨ä½¿ç”¨ FlashAttention ä¼˜åŒ–ï¼** ğŸ‰



