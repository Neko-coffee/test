# ðŸŽ¯ å±€éƒ¨Attention Biaså®žçŽ°è®¾è®¡æ–¹æ¡ˆ

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**çŠ¶æ€**: å®žçŽ°è®¾è®¡é˜¶æ®µ  
**ç›®æ ‡**: ä¸ºCMTçš„Transformer cross-attentionæ·»åŠ ç©ºé—´æ„ŸçŸ¥çš„å±€éƒ¨biasæœºåˆ¶

---

## ðŸ“ çª—å£å¤§å°è®¾è®¡

### 1. **ç‰©ç†ç©ºé—´åˆ†æž**

```
BEVç‰¹å¾å›¾è§„æ ¼ï¼š
- å°ºå¯¸ï¼š180 Ã— 180
- è¦†ç›–èŒƒå›´ï¼š108m Ã— 108mï¼ˆ-54m åˆ° +54mï¼‰
- æ¯åƒç´ ä»£è¡¨ï¼š0.6m Ã— 0.6m

ä¸åŒç±»åˆ«ç›®æ ‡çš„ç©ºé—´å°ºåº¦ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç›®æ ‡ç±»åˆ«   â”‚ çœŸå®žå°ºå¯¸ â”‚ ç‰¹å¾å›¾å°ºå¯¸ â”‚ æŽ¨èçª—å£    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Car         â”‚ 4Ã—2m     â”‚ 7Ã—3åƒç´     â”‚ 11Ã—11 å¯è¦†ç›– â”‚
â”‚ Bus/Trailer â”‚ 12Ã—3m    â”‚ 20Ã—5åƒç´    â”‚ 25Ã—25 å¯è¦†ç›– â”‚
â”‚ Pedestrian  â”‚ 1Ã—0.5m   â”‚ 2Ã—1åƒç´     â”‚ 5Ã—5 å¯è¦†ç›–   â”‚
â”‚ Barrier     â”‚ 4Ã—0.5m   â”‚ 7Ã—1åƒç´     â”‚ 11Ã—11 å¯è¦†ç›– â”‚
â”‚ Bicycle     â”‚ 2Ã—0.7m   â”‚ 3Ã—1åƒç´     â”‚ 7Ã—7 å¯è¦†ç›–   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **çª—å£å¤§å°ç­–ç•¥å¯¹æ¯”**

```python
# æ–¹æ¡ˆAï¼šä¿å®ˆçª—å£ï¼ˆå¯¹æ ‡LAMçš„lidar windowï¼‰â­â­â­
BEV_WINDOW_SIZE = 5
Camera_WINDOW_SIZE = 5
è¦†ç›–èŒƒå›´ï¼š5 Ã— 0.6m = 3.0m
ä¼˜ç‚¹ï¼šè®¡ç®—é‡å°ã€ç²¾ç¡®èšç„¦
ç¼ºç‚¹ï¼šæ— æ³•è¦†ç›–å¤§ç›®æ ‡ï¼ˆå¦‚Busï¼‰

# æ–¹æ¡ˆBï¼šä¸­ç­‰çª—å£ï¼ˆå¯¹æ ‡LAMçš„camera windowï¼‰â­â­â­â­â­ ã€æŽ¨èã€‘
BEV_WINDOW_SIZE = 15
Camera_WINDOW_SIZE = 15
è¦†ç›–èŒƒå›´ï¼š15 Ã— 0.6m = 9.0m
ä¼˜ç‚¹ï¼šèƒ½è¦†ç›–ç»å¤§å¤šæ•°ç›®æ ‡ã€ä¸ŽLAMä¸€è‡´ã€å¹³è¡¡æ€§å¥½
ç¼ºç‚¹ï¼šè¾ƒå¤§ç›®æ ‡è¾¹ç¼˜å¯èƒ½ä¸å®Œæ•´

# æ–¹æ¡ˆCï¼šå¤§çª—å£ï¼ˆæ¿€è¿›ï¼‰â­â­â­
BEV_WINDOW_SIZE = 25
Camera_WINDOW_SIZE = 25
è¦†ç›–èŒƒå›´ï¼š25 Ã— 0.6m = 15.0m
ä¼˜ç‚¹ï¼šè¦†ç›–æ‰€æœ‰ç›®æ ‡
ç¼ºç‚¹ï¼šæŽ¥è¿‘å…¨å±€ã€å¤±åŽ»å±€éƒ¨æ€§ä¼˜åŠ¿

# æ–¹æ¡ˆDï¼šè‡ªé€‚åº”çª—å£ï¼ˆç†æƒ³ä½†å¤æ‚ï¼‰â­â­â­â­
æ ¹æ®queryçš„é¢„æµ‹å°ºå¯¸åŠ¨æ€è°ƒæ•´çª—å£å¤§å°
ä¼˜ç‚¹ï¼šç†è®ºæœ€ä¼˜
ç¼ºç‚¹ï¼šå®žçŽ°å¤æ‚ã€éš¾ä»¥å‘é‡åŒ–
```

**ðŸŽ¯ æœ€ç»ˆé€‰æ‹©ï¼šæ–¹æ¡ˆBï¼ˆwindow_size=15ï¼‰**
- ä¸ŽAQRçš„LAM camera windowä¸€è‡´
- èƒ½è¦†ç›–90%ä»¥ä¸Šçš„ç›®æ ‡
- ä¿æŒåˆç†çš„è®¡ç®—å¼€é”€

---

## ðŸ—ï¸ æž¶æž„è®¾è®¡

### æ•´ä½“æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CmtHead Forward                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€â”€> 1. ç‰¹å¾æå–ï¼ˆx, x_imgï¼‰
                            â”‚
                            â”œâ”€â”€> 2. å‚è€ƒç‚¹èŽ·å–ï¼ˆreference_pointsï¼‰
                            â”‚
                            â”œâ”€â”€> 3. è®¡ç®—æŠ•å½±ä½ç½®ï¼ˆpts_bev, pts_pers_idxï¼‰
                            â”‚
                            â”œâ”€â”€> 4. ç”ŸæˆAQRæƒé‡
                            â”‚      â””â”€> AQRWeightGenerator
                            â”‚           â”œâ”€> lidar_weights [bs, num_queries]
                            â”‚           â””â”€> camera_weights [bs, num_queries]
                            â”‚
                            â”œâ”€â”€> 5. âœ¨ ç”Ÿæˆå±€éƒ¨Attention Bias âœ¨
                            â”‚      â””â”€> AttentionBiasGenerator
                            â”‚           â”œâ”€> è¾“å…¥ï¼šweights, positions, window_size
                            â”‚           â”œâ”€> è®¡ç®—å±€éƒ¨çª—å£mask
                            â”‚           â””â”€> è¾“å‡ºï¼šattention_bias [bs, num_queries, num_features]
                            â”‚
                            â”œâ”€â”€> 6. è°ƒç”¨Transformer
                            â”‚      â””â”€> self.transformer(
                            â”‚            x, x_img, query_embeds,
                            â”‚            bev_pos_embeds, rv_pos_embeds,
                            â”‚            attention_bias=bias  â† âœ¨ æ–°å‚æ•° âœ¨
                            â”‚          )
                            â”‚
                            â””â”€â”€> 7. æ£€æµ‹å¤´è¾“å‡º
```

---

## ðŸ”§ æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. **AttentionBiasGenerator æ¨¡å—**

```python
class AttentionBiasGenerator(nn.Module):
    """
    å±€éƒ¨æ³¨æ„åŠ›biasç”Ÿæˆå™¨
    
    åŠŸèƒ½ï¼š
    1. æ ¹æ®queryçš„ç©ºé—´ä½ç½®ç”Ÿæˆå±€éƒ¨çª—å£
    2. å°†per-queryæƒé‡æ‰©æ•£åˆ°å±€éƒ¨çª—å£
    3. ç”Ÿæˆ [bs, num_queries, num_features] çš„biasçŸ©é˜µ
    """
    
    def __init__(self,
                 bev_feature_shape=(180, 180),
                 pers_feature_shape=(6, 40, 100),
                 window_size=15,
                 bias_scale=1.0):
        """
        Args:
            bev_feature_shape: BEVç‰¹å¾å›¾å°ºå¯¸
            pers_feature_shape: é€è§†ç‰¹å¾å›¾å°ºå¯¸
            window_size: å±€éƒ¨çª—å£å¤§å°
            bias_scale: biasç¼©æ”¾å› å­
        """
        
    def forward(self, 
                lidar_weights,      # [bs, num_queries]
                camera_weights,     # [bs, num_queries]
                pts_bev_indices,    # [bs, num_queries] BEVä½ç½®ç´¢å¼•
                pts_pers_indices):  # [bs, num_queries, 3] é€è§†ä½ç½®ç´¢å¼•(view,h,w)
        """
        ç”Ÿæˆå±€éƒ¨attention bias
        
        Returns:
            attention_bias: [bs, num_queries, total_features]
                å…¶ä¸­ total_features = bev_h*bev_w + 6*pers_h*pers_w
        """
```

### 2. **çª—å£ç”Ÿæˆé€»è¾‘**

```python
def _generate_local_window_bias(self, 
                                 weights,        # [bs, num_queries]
                                 positions,      # [bs, num_queries] æˆ– [bs, num_queries, 3]
                                 feature_shape,  # (H, W) æˆ– (V, H, W)
                                 window_size):   # int
    """
    æ ¸å¿ƒç®—æ³•ï¼šå‘é‡åŒ–çš„å±€éƒ¨çª—å£biasç”Ÿæˆ
    
    æ­¥éª¤ï¼š
    1. è®¡ç®—çª—å£åç§»é‡ offsets
    2. ç”Ÿæˆæœ‰æ•ˆçª—å£ç´¢å¼• valid_indices
    3. åˆ›å»ºbiasçŸ©é˜µå¹¶å¡«å……
    4. åº”ç”¨æƒé‡ç¼©æ”¾
    """
    
    # ä¼ªä»£ç ç¤ºä¾‹
    batch_size, num_queries = weights.shape
    H, W = feature_shape[-2:]
    total_features = H * W
    
    # 1. ç”Ÿæˆçª—å£åç§»ï¼ˆmeshgridï¼‰
    offsets = torch.arange(-window_size//2, window_size//2+1)
    y_off, x_off = torch.meshgrid(offsets, offsets)
    window_offsets = y_off * W + x_off  # [window_size^2]
    
    # 2. è®¡ç®—æ‰€æœ‰queryçš„çª—å£ç´¢å¼•ï¼ˆå‘é‡åŒ–ï¼‰
    query_indices = positions.unsqueeze(-1) + window_offsets.view(1, 1, -1)
    # query_indices: [bs, num_queries, window_size^2]
    
    # 3. è¾¹ç•Œæ£€æŸ¥
    valid_mask = (query_indices >= 0) & (query_indices < total_features)
    
    # 4. åˆ›å»ºbiasçŸ©é˜µ
    bias = torch.zeros(batch_size, num_queries, total_features, device=weights.device)
    
    # 5. å¡«å……å±€éƒ¨çª—å£ï¼ˆä½¿ç”¨scatter_addï¼‰
    weights_expanded = weights.unsqueeze(-1).expand(-1, -1, window_size**2)
    weights_masked = torch.where(valid_mask, weights_expanded, torch.zeros_like(weights_expanded))
    
    # 6. å‘é‡åŒ–å¡«å……
    bias.scatter_add_(
        dim=2,
        index=query_indices.clamp(0, total_features-1),
        src=weights_masked
    )
    
    return bias
```

---

## ðŸ’¡ å…³é”®å®žçŽ°ç»†èŠ‚

### 1. **å‘é‡åŒ–ä¼˜åŒ–**

```python
# âŒ ä½Žæ•ˆçš„å¾ªçŽ¯å®žçŽ°
for b in range(batch_size):
    for q in range(num_queries):
        for offset in window_offsets:
            idx = positions[b, q] + offset
            if 0 <= idx < total_features:
                bias[b, q, idx] = weights[b, q]

# âœ… é«˜æ•ˆçš„å‘é‡åŒ–å®žçŽ°
query_indices = positions.unsqueeze(-1) + window_offsets.view(1, 1, -1)
valid_mask = (query_indices >= 0) & (query_indices < total_features)
weights_expanded = weights.unsqueeze(-1).expand(-1, -1, window_size**2)
bias.scatter_add_(dim=2, index=query_indices.clamp(0), src=weights_masked)
```

### 2. **è¾¹ç•Œå¤„ç†**

```python
# BEVè¾¹ç•Œæ£€æŸ¥ï¼ˆ2Dç½‘æ ¼ï¼‰
query_y = positions // W
query_x = positions % W

window_y = query_y.unsqueeze(-1) + y_offsets.view(1, 1, -1)
window_x = query_x.unsqueeze(-1) + x_offsets.view(1, 1, -1)

valid_y = (window_y >= 0) & (window_y < H)
valid_x = (window_x >= 0) & (window_x < W)
valid_mask = valid_y & valid_x

# Cameraè¾¹ç•Œæ£€æŸ¥ï¼ˆè€ƒè™‘å¤šè§†è§’ï¼‰
view_mask = (view_indices >= 0) & (view_indices < 6)
valid_mask = valid_mask & view_mask
```

### 3. **å†…å­˜ä¼˜åŒ–**

```python
# è®¡ç®—å†…å­˜å ç”¨
batch_size = 2
num_queries = 900
total_features = 180*180 + 6*40*100 = 32400 + 24000 = 56400

bias_memory = batch_size * num_queries * total_features * 4 bytes (fp32)
            = 2 * 900 * 56400 * 4 / 1024^2
            â‰ˆ 387 MB

# ä¼˜åŒ–ç­–ç•¥ï¼š
1. ä½¿ç”¨fp16å­˜å‚¨ï¼šå‡åŠè‡³ ~194 MB
2. ç¨€ç–è¡¨ç¤ºï¼šåªå­˜å‚¨å±€éƒ¨çª—å£
3. æŒ‰éœ€è®¡ç®—ï¼šåˆ†batchè®¡ç®—
```

---

## ðŸ”— é›†æˆç‚¹è®¾è®¡

### ä¿®æ”¹ CmtTransformer

```python
@TRANSFORMER.register_module()
class CmtTransformer(BaseModule):
    
    def forward(self, x, x_img, query_embed, 
                bev_pos_embed, rv_pos_embed, 
                attn_masks=None, 
                attention_bias=None,  # â† âœ¨ æ–°å¢žå‚æ•°
                reg_branch=None):
        """
        Args:
            attention_bias: [bs, num_queries, total_features] æˆ– None
        """
        
        # èžåˆmemory
        memory = torch.cat([bev_memory, rv_memory], dim=0)
        pos_embed = torch.cat([bev_pos_embed, rv_pos_embed], dim=0)
        
        # ä¼ é€’biasåˆ°decoder
        out_dec = self.decoder(
            query=target,
            key=memory,
            value=memory,
            key_pos=pos_embed,
            query_pos=query_embed,
            attn_masks=[attn_masks, None],
            attention_bias=attention_bias,  # â† âœ¨ ä¼ é€’bias
            reg_branch=reg_branch
        )
        
        return out_dec, memory
```

### ä¿®æ”¹ PETRTransformerDecoder

```python
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class PETRTransformerDecoder(TransformerLayerSequence):
    
    def forward(self, query, *args, 
                attention_bias=None,  # â† âœ¨ æ–°å¢ž
                **kwargs):
        """
        Args:
            attention_bias: [bs, num_queries, num_features]
        """
        
        # è°ƒæ•´ç»´åº¦ï¼š[bs, num_queries, num_features] â†’ [num_queries, bs, num_features]
        if attention_bias is not None:
            attention_bias = attention_bias.transpose(0, 1)
        
        # é€å±‚ä¼ é€’
        intermediate = []
        for layer in self.layers:
            query = layer(query, *args, 
                         attention_bias=attention_bias,  # â† âœ¨ ä¼ é€’
                         **kwargs)
            if self.return_intermediate:
                intermediate.append(self.post_norm(query))
        
        return torch.stack(intermediate)
```

### ä¿®æ”¹ PETRTransformerDecoderLayer

```python
@TRANSFORMER_LAYER.register_module()
class PETRTransformerDecoderLayer(BaseTransformerLayer):
    
    def forward(self, query, key=None, value=None,
                query_pos=None, key_pos=None,
                attn_masks=None, 
                attention_bias=None,  # â† âœ¨ æ–°å¢ž
                query_key_padding_mask=None,
                key_padding_mask=None,
                **kwargs):
        """
        Args:
            attention_bias: [num_queries, bs, num_features]
        """
        
        # åœ¨cross_attnæ—¶åº”ç”¨bias
        for layer in self.operation_order:
            if layer == 'cross_attn':
                query = self.attentions[attn_index](
                    query,
                    key,
                    value,
                    query_pos=query_pos,
                    key_pos=key_pos,
                    attn_mask=attn_masks[attn_index],
                    attention_bias=attention_bias,  # â† âœ¨ åº”ç”¨bias
                    key_padding_mask=key_padding_mask,
                    **kwargs
                )
                attn_index += 1
        
        return query
```

### ä¿®æ”¹ MultiheadAttention

```python
class MultiheadAttention(nn.Module):
    
    def forward(self, query, key=None, value=None,
                query_pos=None, key_pos=None,
                attn_mask=None,
                attention_bias=None,  # â† âœ¨ æ–°å¢ž
                key_padding_mask=None,
                **kwargs):
        """
        Args:
            attention_bias: [num_queries, bs, num_features]
                åœ¨è®¡ç®—attention scoresåŽåŠ åˆ°scoresä¸Š
        """
        
        # æ ‡å‡†attentionè®¡ç®—
        q = k = self.qkv_proj(query)
        k = self.qkv_proj(key) if key is not None else k
        v = self.qkv_proj(value) if value is not None else k
        
        if query_pos is not None:
            q = q + self.qkv_proj(query_pos)
        if key_pos is not None:
            k = k + self.qkv_proj(key_pos)
        
        # è®¡ç®—attention scores
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / sqrt(d_k)
        # attn_scores: [bs, num_heads, num_queries, num_features]
        
        # âœ¨ åº”ç”¨attention bias âœ¨
        if attention_bias is not None:
            # attention_bias: [num_queries, bs, num_features]
            # éœ€è¦è°ƒæ•´ç»´åº¦å¹¶æ‰©å±•åˆ°å¤šå¤´
            bias = attention_bias.transpose(0, 1)  # [bs, num_queries, num_features]
            bias = bias.unsqueeze(1)  # [bs, 1, num_queries, num_features]
            bias = bias.expand(-1, self.num_heads, -1, -1)  # [bs, num_heads, num_queries, num_features]
            
            attn_scores = attn_scores + bias  # â† âœ¨ åŠ bias
        
        # åº”ç”¨mask
        if attn_mask is not None:
            attn_scores.masked_fill_(attn_mask, float('-inf'))
        
        # Softmax + dropout
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # è®¡ç®—è¾“å‡º
        output = torch.matmul(attn_weights, v)
        
        return output
```

---

## ðŸ“Š æ€§èƒ½é¢„ä¼°

### è®¡ç®—å¤æ‚åº¦

```
åŽŸå§‹cross-attentionï¼š
- æ—¶é—´å¤æ‚åº¦ï¼šO(num_queries * num_features)
- å†…å­˜å ç”¨ï¼šO(bs * num_heads * num_queries * num_features)

æ·»åŠ å±€éƒ¨biasï¼š
- é¢å¤–æ—¶é—´ï¼šO(bs * num_queries * window_size^2)  â† çª—å£ç”Ÿæˆ
- é¢å¤–å†…å­˜ï¼šO(bs * num_queries * num_features)   â† biasçŸ©é˜µ

window_size=15 æ—¶ï¼š
- çª—å£ç”Ÿæˆï¼š2 * 900 * 15^2 = 405,000 æ¬¡æ“ä½œï¼ˆå¯å‘é‡åŒ–ï¼‰
- biasçŸ©é˜µï¼š387 MBï¼ˆfp32ï¼‰æˆ– 194 MBï¼ˆfp16ï¼‰

ç»“è®ºï¼šè®¡ç®—å¼€é”€å¯æŽ¥å—ï¼Œå†…å­˜éœ€è¦ä¼˜åŒ–
```

### ä¼˜åŒ–å»ºè®®

```python
# 1. åŠç²¾åº¦å­˜å‚¨
attention_bias = attention_bias.half()

# 2. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æžœå†…å­˜ç´§å¼ ï¼‰
with torch.cuda.amp.autocast():
    bias = self.bias_generator(...)

# 3. åˆ†å—è®¡ç®—ï¼ˆå¦‚æžœbatch_sizeå¾ˆå¤§ï¼‰
for i in range(0, batch_size, chunk_size):
    bias_chunk = self.bias_generator(
        weights[i:i+chunk_size],
        positions[i:i+chunk_size]
    )
```

---

## ðŸŽ¯ é…ç½®æŽ¥å£è®¾è®¡

```python
# åœ¨é…ç½®æ–‡ä»¶ä¸­
model = dict(
    pts_head=dict(
        type='CmtHead',
        enable_aqr=True,
        
        # AQRæƒé‡ç”Ÿæˆé…ç½®
        aqr_config=dict(
            type='AQRWeightGenerator',
            embed_dims=256,
            window_sizes=[15, 5],  # LAMçª—å£
            ...
        ),
        
        # âœ¨ Attention Biasé…ç½® âœ¨
        attention_bias_config=dict(
            enable=True,                    # æ˜¯å¦å¯ç”¨
            window_size=15,                 # å±€éƒ¨çª—å£å¤§å°
            bias_scale=1.0,                 # biasç¼©æ”¾å› å­
            use_local_bias=True,            # True=å±€éƒ¨, False=å…¨å±€
            fp16=True,                      # æ˜¯å¦ä½¿ç”¨fp16
            debug_mode=False                # è°ƒè¯•æ¨¡å¼
        ),
        
        # ä¸å†éœ€è¦å•ç‹¬çš„rendererå’Œmodulator
        # renderer_config=None,  # åºŸå¼ƒ
        # modulator_config=None, # åºŸå¼ƒ
    )
)
```

---

## âœ… å®žçŽ°æ£€æŸ¥æ¸…å•

### Phase 1: æ ¸å¿ƒæ¨¡å—å®žçŽ°
- [ ] `AttentionBiasGenerator` ç±»å®žçŽ°
  - [ ] `__init__` åˆå§‹åŒ–
  - [ ] `_generate_bev_local_bias` BEVçª—å£bias
  - [ ] `_generate_camera_local_bias` Cameraçª—å£bias
  - [ ] `forward` ä¸»å‡½æ•°
  - [ ] è¾¹ç•Œæ£€æŸ¥é€»è¾‘
  - [ ] å‘é‡åŒ–ä¼˜åŒ–

### Phase 2: Transformeré›†æˆ
- [ ] ä¿®æ”¹ `CmtTransformer.forward`
- [ ] ä¿®æ”¹ `PETRTransformerDecoder.forward`
- [ ] ä¿®æ”¹ `PETRTransformerDecoderLayer.forward`
- [ ] ä¿®æ”¹ `MultiheadAttention.forward`

### Phase 3: CmtHeadé›†æˆ
- [ ] æ·»åŠ  `attention_bias_config` å‚æ•°
- [ ] åˆå§‹åŒ– `AttentionBiasGenerator`
- [ ] åœ¨ `forward_single` ä¸­ç”Ÿæˆbias
- [ ] ä¼ é€’biasåˆ°Transformer

### Phase 4: æµ‹è¯•ä¸Žä¼˜åŒ–
- [ ] å•å…ƒæµ‹è¯•ï¼šçª—å£ç”Ÿæˆæ­£ç¡®æ€§
- [ ] é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯å‰å‘ä¼ æ’­
- [ ] æ€§èƒ½æµ‹è¯•ï¼šå†…å­˜å’Œé€Ÿåº¦
- [ ] å¯è§†åŒ–ï¼šbiasåˆ†å¸ƒå›¾

---

## ðŸš¨ æ½œåœ¨é£Žé™©

1. **å†…å­˜å ç”¨**
   - é£Žé™©ï¼š387MBé¢å¤–å†…å­˜å¯èƒ½å¯¼è‡´OOM
   - ç¼“è§£ï¼šä½¿ç”¨fp16ã€åˆ†å—è®¡ç®—

2. **æ•°å€¼ç¨³å®šæ€§**
   - é£Žé™©ï¼šbiasè¿‡å¤§å¯¼è‡´softmaxé¥±å’Œ
   - ç¼“è§£ï¼šæ·»åŠ `bias_scale`å‚æ•°æŽ§åˆ¶å¹…åº¦

3. **Flash Attentionå…¼å®¹æ€§**
   - é£Žé™©ï¼šFlash Attentionå¯èƒ½ä¸æ”¯æŒcustom bias
   - ç¼“è§£ï¼šé€€å›žæ ‡å‡†attentionæˆ–ä¿®æ”¹biasåº”ç”¨æ–¹å¼

4. **è®­ç»ƒä¸ç¨³å®š**
   - é£Žé™©ï¼šåˆæœŸbiaså¯èƒ½æ‰°ä¹±å·²æœ‰çš„attention pattern
   - ç¼“è§£ï¼šæ¸è¿›å¼å¯ç”¨ï¼ˆwarmupï¼‰ã€è¾ƒå°çš„`bias_scale`

---

## ðŸ“– å‚è€ƒæ–‡çŒ®

1. **Deformable DETR**: Conditional spatial attention
2. **DN-DETR**: Noise-based query denoising with positional bias
3. **Relative Position Bias**: Swin Transformerçš„ç›¸å¯¹ä½ç½®ç¼–ç 
4. **Local Attention**: Longformerçš„æ»‘åŠ¨çª—å£æœºåˆ¶

---

**ä¸»äººï¼Œè¿™ä¸ªè®¾è®¡æ–¹æ¡ˆå‡†å¤‡å¥½äº†ï¼æŽ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹å®žçŽ°æ ¸å¿ƒä»£ç ï¼** ðŸš€âœ¨

