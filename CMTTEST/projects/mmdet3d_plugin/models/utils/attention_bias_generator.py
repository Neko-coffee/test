# Copyright (c) 2024 CMT-AQR Team. All rights reserved.
"""
Attention Bias Generator for Local Spatial Modulation
å±€éƒ¨ç©ºé—´æ³¨æ„åŠ›biasç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
1. æ ¹æ®queryçš„ç©ºé—´æŠ•å½±ä½ç½®ç”Ÿæˆå±€éƒ¨çª—å£
2. å°†per-queryæƒé‡æ‰©æ•£åˆ°å±€éƒ¨çª—å£å†…çš„ç‰¹å¾
3. ç”Ÿæˆç»†ç²’åº¦çš„attention biasçŸ©é˜µç”¨äºTransformer

è®¾è®¡æ€è·¯ï¼š
- ä¸ç›´æ¥ä¿®æ”¹ç‰¹å¾å›¾ï¼Œè€Œæ˜¯å½±å“attentionè®¡ç®—
- ä¿æŒç‰¹å¾è¯­ä¹‰ä¸å˜ï¼Œåªè°ƒæ•´queryå¯¹ä¸åŒåŒºåŸŸçš„å…³æ³¨ç¨‹åº¦
- ä½¿ç”¨å±€éƒ¨çª—å£è€Œéå…¨å±€ï¼Œæä¾›ç©ºé—´å…ˆéªŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from mmcv.runner import BaseModule, force_fp32
import numpy as np


class AttentionBiasGenerator(BaseModule):
    """
    å±€éƒ¨æ³¨æ„åŠ›Biasç”Ÿæˆå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è¾“å…¥ï¼šper-queryæƒé‡ + ç©ºé—´ä½ç½®
    - è¾“å‡ºï¼š[bs, num_queries, num_features] çš„biasçŸ©é˜µ
    
    å®ç°ç­–ç•¥ï¼š
    - å‘é‡åŒ–è®¡ç®—ï¼Œé¿å…å¾ªç¯
    - å±€éƒ¨çª—å£æ§åˆ¶biasèŒƒå›´
    - æ”¯æŒBEVå’ŒCameraä¸¤ç§ç‰¹å¾å›¾
    """
    
    def __init__(self,
                 bev_feature_shape=(180, 180),      # ğŸ”¥ 1600x640é»˜è®¤: (180, 180)
                 pers_feature_shape=(6, 40, 100),   # ğŸ”¥ 1600x640é»˜è®¤: (6, 40, 100)
                 window_size=15,                    # ğŸ”¥ 1600x640é»˜è®¤: 15
                 bias_scale=2.5,
                 learnable_scale=True,              # ğŸ”¥ é»˜è®¤å¯ç”¨å¯å­¦ä¹ scale
                 min_scale=0.5,
                 max_scale=5.0,
                 use_local_bias=True,
                 use_gaussian_window=False,
                 gaussian_sigma=2.5,                # ğŸ”¥ 1600x640é»˜è®¤: 2.5
                 debug_print=True,                  # ğŸ”¥ é»˜è®¤å¯ç”¨è°ƒè¯•æ‰“å°
                 print_interval=1000,               # ğŸ”¥ é»˜è®¤1000ä¸ªiterationæ‰“å°ä¸€æ¬¡
                 fp16=True,                         # ğŸ”¥ é»˜è®¤å¯ç”¨FP16
                 init_cfg=None):
        """
        Args:
            bev_feature_shape (tuple): BEVç‰¹å¾å›¾å°ºå¯¸ (H, W)
            pers_feature_shape (tuple): é€è§†ç‰¹å¾å›¾å°ºå¯¸ (num_views, H, W)
            window_size (int): å±€éƒ¨çª—å£å¤§å°ï¼ˆæ­£æ–¹å½¢çª—å£çš„è¾¹é•¿ï¼‰
            bias_scale (float): biasç¼©æ”¾å› å­çš„åˆå§‹å€¼
            learnable_scale (bool): æ˜¯å¦è®©bias_scaleå¯å­¦ä¹ 
            min_scale (float): bias_scaleçš„æœ€å°å€¼ï¼ˆé˜²æ­¢é€€åŒ–ï¼‰
            max_scale (float): bias_scaleçš„æœ€å¤§å€¼ï¼ˆé˜²æ­¢softmaxé¥±å’Œï¼‰
            use_local_bias (bool): True=å±€éƒ¨çª—å£bias, False=å…¨å±€bias
            use_gaussian_window (bool): True=é«˜æ–¯è¡°å‡çª—å£, False=å‡åŒ€çª—å£ï¼ˆä»…å½“use_local_bias=Trueæ—¶ç”Ÿæ•ˆï¼‰
            gaussian_sigma (float): é«˜æ–¯æ ¸çš„æ ‡å‡†å·®ï¼ˆä»…å½“use_gaussian_window=Trueæ—¶ç”Ÿæ•ˆï¼‰
            debug_print (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆbiasç»Ÿè®¡ã€æƒé‡åˆ†å¸ƒç­‰ï¼‰
            print_interval (int): æ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªiterationæ‰“å°ä¸€æ¬¡ï¼‰ï¼Œä»…å½“debug_print=Trueæ—¶ç”Ÿæ•ˆ
            fp16 (bool): æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦å­˜å‚¨biasçŸ©é˜µ
            init_cfg (dict): åˆå§‹åŒ–é…ç½®
        """
        super(AttentionBiasGenerator, self).__init__(init_cfg=init_cfg)
        
        self.bev_h, self.bev_w = bev_feature_shape
        self.num_views, self.pers_h, self.pers_w = pers_feature_shape
        self.window_size = window_size
        self.use_local_bias = use_local_bias
        self.use_gaussian_window = use_gaussian_window
        self.gaussian_sigma = gaussian_sigma
        self.debug_print = debug_print
        self.print_interval = print_interval
        self._iter_count = 0  # è¿­ä»£è®¡æ•°å™¨
        self.fp16 = fp16
        self.learnable_scale = learnable_scale
        self.min_scale = min_scale
        self.max_scale = max_scale
        
        # ğŸ”¥ å¯å­¦ä¹ çš„bias_scale
        if learnable_scale:
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ FloatTensor å¹¶æ˜ç¡®æŒ‡å®š dtypeï¼Œé¿å…æ¢¯åº¦è®¡ç®—é—®é¢˜
            self.bias_scale = nn.Parameter(torch.tensor([bias_scale], dtype=torch.float32))
        else:
            self.register_buffer('bias_scale', torch.tensor([bias_scale], dtype=torch.float32))
        
        # é¢„è®¡ç®—çª—å£åç§»é‡ï¼ˆåŠ é€Ÿï¼‰
        self._init_window_offsets()
        
        print(f"âœ… AttentionBiasGenerator initialized:")
        print(f"   BEV shape: {bev_feature_shape}")
        print(f"   Pers shape: {pers_feature_shape}")
        print(f"   Window size: {window_size} ({'local' if use_local_bias else 'global'})")
        print(f"   Bias scale: {bias_scale} ({'learnable' if learnable_scale else 'fixed'})")
        if learnable_scale:
            print(f"   Scale range: [{min_scale}, {max_scale}]")
        print(f"   FP16: {fp16}")
    
    def _init_window_offsets(self):
        """é¢„è®¡ç®—çª—å£åç§»é‡"""
        # ğŸ”¥ ç¡®ä¿window_sizeå°±æ˜¯å®é™…çª—å£å¤§å°ï¼ˆä¾‹å¦‚window_size=8 â†’ 8x8çª—å£ï¼‰
        # çª—å£èŒƒå›´ï¼š[-half_window+1, half_window]ï¼Œå…±window_sizeä¸ªå…ƒç´ 
        half_window = self.window_size // 2
        if self.window_size % 2 == 0:
            # å¶æ•°çª—å£ï¼šä¾‹å¦‚8 â†’ [-3, -2, -1, 0, 1, 2, 3, 4]
            offsets = torch.arange(-half_window + 1, half_window + 1)
        else:
            # å¥‡æ•°çª—å£ï¼šä¾‹å¦‚9 â†’ [-4, -3, -2, -1, 0, 1, 2, 3, 4]
            offsets = torch.arange(-half_window, half_window + 1)
        
        # 2Dç½‘æ ¼åç§»ï¼ˆç”¨äºBEVï¼‰
        # å…¼å®¹æ—§ç‰ˆPyTorchï¼ˆ<1.10ï¼‰ï¼šä¸ä½¿ç”¨indexingå‚æ•°
        try:
            y_offsets, x_offsets = torch.meshgrid(offsets, offsets, indexing='ij')
        except TypeError:
            # PyTorch < 1.10ï¼šé»˜è®¤å°±æ˜¯'ij'ç´¢å¼•æ–¹å¼
            y_offsets, x_offsets = torch.meshgrid(offsets, offsets)
        self.register_buffer('y_offsets', y_offsets.reshape(-1))  # [window_size^2]
        self.register_buffer('x_offsets', x_offsets.reshape(-1))  # [window_size^2]
        
        # 1Dç´¢å¼•åç§»ï¼ˆç”¨äºå±•å¹³åçš„ç‰¹å¾ï¼‰
        window_offsets_bev = y_offsets * self.bev_w + x_offsets
        self.register_buffer('window_offsets_bev', window_offsets_bev.reshape(-1))  # [window_size^2]
        
        window_offsets_pers = y_offsets * self.pers_w + x_offsets
        self.register_buffer('window_offsets_pers', window_offsets_pers.reshape(-1))  # [window_size^2]
        
        # ğŸ”¥ é¢„è®¡ç®—é«˜æ–¯æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gaussian_window:
            # è®¡ç®—çª—å£å†…æ¯ä¸ªä½ç½®åˆ°ä¸­å¿ƒçš„è·ç¦»
            distances = torch.sqrt(self.y_offsets.float()**2 + self.x_offsets.float()**2)
            # [window_size^2]
            
            # é«˜æ–¯è¡°å‡ï¼šexp(-distance^2 / (2 * sigma^2))
            gaussian_weights = torch.exp(-distances**2 / (2 * self.gaussian_sigma**2))
            # å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼Œè®©æ€»å’Œä¸º1ï¼‰
            # gaussian_weights = gaussian_weights / gaussian_weights.sum()
            
            self.register_buffer('gaussian_weights', gaussian_weights)  # [window_size^2]
        else:
            # å‡åŒ€æƒé‡
            uniform_weights = torch.ones(self.window_size**2)
            self.register_buffer('gaussian_weights', uniform_weights)
    
    @force_fp32(apply_to=('lidar_weights', 'camera_weights'))
    def forward(self,
                lidar_weights,      # [bs, num_queries] LiDARæ¨¡æ€æƒé‡
                camera_weights,     # [bs, num_queries] Cameraæ¨¡æ€æƒé‡
                pts_bev_indices,    # [bs, num_queries] BEVç‰¹å¾å›¾ä½ç½®ç´¢å¼•
                pts_pers_indices):  # [bs, num_queries, 3] é€è§†ç‰¹å¾å›¾ä½ç½®ç´¢å¼• (view, h, w)
        """
        ç”Ÿæˆå±€éƒ¨attention bias
        
        Args:
            lidar_weights: [bs, num_queries] AQRç”Ÿæˆçš„LiDARæƒé‡
            camera_weights: [bs, num_queries] AQRç”Ÿæˆçš„Cameraæƒé‡
            pts_bev_indices: [bs, num_queries] queryåœ¨BEVç‰¹å¾å›¾ä¸­çš„ä½ç½®ï¼ˆ1Dç´¢å¼•ï¼‰
            pts_pers_indices: [bs, num_queries, 3] queryåœ¨é€è§†ç‰¹å¾å›¾ä¸­çš„ä½ç½®ï¼ˆview, h, wï¼‰
        
        Returns:
            attention_bias: [bs, num_queries, total_features]
                å…¶ä¸­ total_features = bev_h*bev_w + num_views*pers_h*pers_w
                
        ç¤ºä¾‹ï¼š
            bs=2, num_queries=900
            bev_features = 180*180 = 32400
            pers_features = 6*40*100 = 24000
            total_features = 56400
            
            è¾“å‡ºï¼š[2, 900, 56400]
        """
        batch_size, num_queries = lidar_weights.shape
        device = lidar_weights.device
        
        # ğŸ”¥ æƒé‡å·²ç»æ˜¯[-1, 1]èŒƒå›´ï¼ˆæ¥è‡ªAQRWeightGeneratorçš„tanhè¾“å‡ºï¼‰
        # weight=+1.0 â†’ æ­£bias â†’ å¢å¼ºattention â†’ çº¢è‰²
        # weight=-1.0 â†’ è´Ÿbias â†’ æŠ‘åˆ¶attention â†’ è“è‰²
        
        # 1. ç”ŸæˆBEV biasï¼ˆç›´æ¥ä½¿ç”¨æƒé‡ï¼Œå·²ç»æ˜¯[-1, 1]èŒƒå›´ï¼‰
        bev_bias = self._generate_bev_bias(
            lidar_weights,      # [bs, num_queries], èŒƒå›´[-1, 1]
            pts_bev_indices     # [bs, num_queries]
        )  # â†’ [bs, num_queries, bev_h*bev_w], èŒƒå›´[-1, 1]
        
        # 2. ç”ŸæˆCamera biasï¼ˆç›´æ¥ä½¿ç”¨æƒé‡ï¼Œå·²ç»æ˜¯[-1, 1]èŒƒå›´ï¼‰
        camera_bias = self._generate_camera_bias(
            camera_weights,     # [bs, num_queries], èŒƒå›´[-1, 1]
            pts_pers_indices    # [bs, num_queries, 3]
        )  # â†’ [bs, num_queries, num_views*pers_h*pers_w], èŒƒå›´[-1, 1]
        
        # 3. æ‹¼æ¥æˆå®Œæ•´çš„biasçŸ©é˜µ
        attention_bias = torch.cat([bev_bias, camera_bias], dim=-1)
        # â†’ [bs, num_queries, total_features], èŒƒå›´[-1, 1]
        
        # 4. ğŸ”¥ åº”ç”¨ç¼©æ”¾å› å­ï¼ˆå¸¦çº¦æŸï¼‰
        if self.learnable_scale:
            # ğŸ”¥ Step 1: Clamp scaleåˆ°å®‰å…¨èŒƒå›´
            scale = torch.clamp(self.bias_scale[0], min=self.min_scale, max=self.max_scale)  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            
        else:
            scale = self.bias_scale[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        
        attention_bias = attention_bias * scale
        # ğŸ”¥ æƒé‡å·²ç»æ˜¯[-1, 1]ï¼Œä¹˜ä»¥scaleåbiasæ˜¯[-scale, +scale]
        # weight=+1.0 â†’ bias=+scaleï¼ˆæ­£biasï¼Œå¢å¼ºattentionï¼Œçº¢è‰²ï¼‰
        # weight=-1.0 â†’ bias=-scaleï¼ˆè´Ÿbiasï¼ŒæŠ‘åˆ¶attentionï¼Œè“è‰²ï¼‰
        # ä¾‹å¦‚ï¼šscale=3.0 â†’ biasèŒƒå›´[-3.0, +3.0]
        
        # 5. ğŸ”¥ å®æ—¶ç›‘æ§biasç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é…ç½®ï¼‰
        if self.debug_print:  # ğŸ”¥ ç§»é™¤trainingé™åˆ¶ï¼Œè®­ç»ƒå’ŒéªŒè¯éƒ½æ‰“å°
            self._iter_count += 1
            if self._iter_count % self.print_interval == 0:
                # ğŸ”¥ åœ¨ no_grad() ä¹‹å‰ä¿å­˜æ¢¯åº¦ä¿¡æ¯ï¼ˆå› ä¸ºæ¢¯åº¦å¯èƒ½åœ¨åç»­è¢«æ¸…é›¶ï¼‰
                grad_value = None
                if self.learnable_scale and self.bias_scale.grad is not None:
                    grad_value = self.bias_scale.grad[0].item()
                
                with torch.no_grad():
                    # åˆ†ç¦»BEVå’ŒCameraéƒ¨åˆ†
                    bev_part = attention_bias[:, :, :self.bev_h * self.bev_w]
                    cam_part = attention_bias[:, :, self.bev_h * self.bev_w:]
                    
                    print(f"\n{'='*70}")
                    print(f"ğŸ“Š [AttentionBias] Monitoring Report (Iter {self._iter_count}):")
                    
                    # Scaleä¿¡æ¯
                    if self.learnable_scale:
                        current_scale = scale.item()
                        print(f"   ğŸ”§ Learnable Bias Scale: {current_scale:.4f} (range: [{self.min_scale}, {self.max_scale}])")
                        
                        # ğŸ”¥ æ‰“å°ä¿å­˜çš„æ¢¯åº¦ä¿¡æ¯
                        if grad_value is not None:
                            print(f"      ğŸ“‰ Gradient: {grad_value:+.8f}")
                        else:
                            print(f"      âš ï¸  Gradient: None (æœªè®¡ç®—æˆ–å·²æ¸…é›¶)")
                        
                        # æ£€æŸ¥ requires_grad
                        print(f"      ğŸ” requires_grad: {self.bias_scale.requires_grad}")
                        
                        if current_scale > 0.9 * self.max_scale:
                            print(f"      âš ï¸  WARNING: Scaleæ¥è¿‘ä¸Šé™ ({current_scale:.4f} / {self.max_scale})!")
                        elif current_scale < 1.1 * self.min_scale:
                            print(f"      âš ï¸  WARNING: Scaleæ¥è¿‘ä¸‹é™ ({current_scale:.4f} / {self.min_scale})!")
                    else:
                        print(f"   ğŸ”§ Fixed Bias Scale: {self.bias_scale:.4f}")
                    
                    # Biasç»Ÿè®¡ä¿¡æ¯
                    print(f"   ğŸ“ˆ Bias Statistics:")
                    print(f"      Overall  - Mean: {attention_bias.mean().item():+.4f}, Std: {attention_bias.std().item():.4f}")
                    print(f"                 Range: [{attention_bias.min().item():+.4f}, {attention_bias.max().item():+.4f}]")
                    print(f"      BEV      - Mean: {bev_part.mean().item():+.4f}, Std: {bev_part.std().item():.4f}")
                    print(f"      Camera   - Mean: {cam_part.mean().item():+.4f}, Std: {cam_part.std().item():.4f}")
                    
                    # åˆ†å¸ƒåˆ†æ
                    positive_ratio = (attention_bias > 0).float().mean().item()
                    strong_positive = (attention_bias > 2.0).float().mean().item()
                    strong_negative = (attention_bias < -2.0).float().mean().item()
                    near_zero = (attention_bias.abs() < 0.1).float().mean().item()
                    print(f"      Distribution:")
                    print(f"         Positive: {positive_ratio*100:.1f}% | Negative: {(1-positive_ratio)*100:.1f}%")
                    print(f"         Strong+ (>+2): {strong_positive*100:.2f}% | Strong- (<-2): {strong_negative*100:.2f}%")
                    print(f"         Near-zero (|bias|<0.1): {near_zero*100:.1f}%")
                    
                    # è¾“å…¥æƒé‡åˆ†æ
                    print(f"   ğŸ“Š Input Weights (from AQR):")
                    print(f"      LiDAR    - Mean: {lidar_weights.mean().item():+.4f}, Std: {lidar_weights.std().item():.4f}")
                    print(f"                 Range: [{lidar_weights.min().item():+.4f}, {lidar_weights.max().item():+.4f}]")
                    print(f"      Camera   - Mean: {camera_weights.mean().item():+.4f}, Std: {camera_weights.std().item():.4f}")
                    print(f"                 Range: [{camera_weights.min().item():+.4f}, {camera_weights.max().item():+.4f}]")
                    
                # æ¨¡æ€åå¥½åˆ†æ
                lidar_prefer_ratio = (lidar_weights > camera_weights).float().mean().item()
                camera_prefer_ratio = (camera_weights > lidar_weights).float().mean().item()
                balanced_ratio = ((lidar_weights - camera_weights).abs() < 0.2).float().mean().item()
                
                print(f"      Modality Preference (per query):")
                print(f"         Prefer LiDAR: {lidar_prefer_ratio*100:.1f}% (lidar_w > camera_w)")
                print(f"         Prefer Camera: {camera_prefer_ratio*100:.1f}% (camera_w > lidar_w)")
                print(f"         Balanced: {balanced_ratio*100:.1f}% (|diff| < 0.2)")
                print(f"{'='*70}\n")
        
        # 5. ğŸ”¥ åŒé‡ä¿é™©ï¼šè£å‰ªæœ€ç»ˆbiasèŒƒå›´
        # å³ä½¿scaleè¢«çº¦æŸï¼Œä»ç„¶clampä¸€æ¬¡ç¡®ä¿ä¸ä¼šè¶…å‡ºsoftmaxæ•æ„ŸåŒºé—´
        # Softmaxæ•æ„ŸåŒºé—´ï¼š[-3, +3]æœ€ä¼˜ï¼Œ[-5, +5]å®‰å…¨
        max_bias = min(5.0, self.max_scale)  # å–max_scaleå’Œ5.0çš„è¾ƒå°å€¼
        attention_bias = torch.clamp(attention_bias, min=-max_bias, max=max_bias)
        
        # 6. è½¬æ¢ä¸ºfp16ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.fp16:
            attention_bias = attention_bias.half()
        
        return attention_bias
    
    def _generate_bev_bias(self, weights, positions):
        """
        ç”ŸæˆBEVç‰¹å¾å›¾çš„å±€éƒ¨bias
        
        Args:
            weights: [bs, num_queries] æƒé‡
            positions: [bs, num_queries] 1Dä½ç½®ç´¢å¼•
        
        Returns:
            bias: [bs, num_queries, bev_h*bev_w]
        """
        batch_size, num_queries = weights.shape
        device = weights.device
        total_features = self.bev_h * self.bev_w
        
        if not self.use_local_bias:
            # å…¨å±€biasï¼šæ¯ä¸ªqueryå¯¹æ‰€æœ‰BEVç‰¹å¾æ–½åŠ ç›¸åŒbias
            return weights.unsqueeze(-1).expand(batch_size, num_queries, total_features)
        
        # === å±€éƒ¨biasï¼šå‘é‡åŒ–å®ç° ===
        
        # 1. è®¡ç®—æ‰€æœ‰queryçš„å±€éƒ¨çª—å£ç´¢å¼•
        # positions: [bs, num_queries]
        # window_offsets_bev: [window_size^2]
        query_indices = positions.unsqueeze(-1) + self.window_offsets_bev.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        # 2. è¾¹ç•Œæ£€æŸ¥ï¼ˆ2Dç½‘æ ¼ï¼‰
        query_y = torch.div(positions, self.bev_w, rounding_mode='floor').long()  # [bs, num_queries]
        query_x = positions % self.bev_w   # [bs, num_queries]
        
        window_y = query_y.unsqueeze(-1) + self.y_offsets.unsqueeze(0).unsqueeze(0)
        window_x = query_x.unsqueeze(-1) + self.x_offsets.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        valid_y = (window_y >= 0) & (window_y < self.bev_h)
        valid_x = (window_x >= 0) & (window_x < self.bev_w)
        valid_mask = valid_y & valid_x  # [bs, num_queries, window_size^2]
        
        # 3. åˆ›å»ºbiasçŸ©é˜µ
        bias = torch.zeros(batch_size, num_queries, total_features, 
                          device=device, dtype=weights.dtype)
        
        # 4. æ‰©å±•æƒé‡åˆ°çª—å£
        weights_expanded = weights.unsqueeze(-1).expand(-1, -1, self.window_size**2)
        # â†’ [bs, num_queries, window_size^2]
        
        # ğŸ”¥ 4.5. åº”ç”¨é«˜æ–¯è¡°å‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gaussian_window:
            # gaussian_weights: [window_size^2]
            # weights_expanded: [bs, num_queries, window_size^2]
            weights_expanded = weights_expanded * self.gaussian_weights.unsqueeze(0).unsqueeze(0)
        
        # 5. åº”ç”¨valid mask
        weights_masked = torch.where(valid_mask, weights_expanded, 
                                     torch.zeros_like(weights_expanded))
        
        # 6. å‘é‡åŒ–å¡«å……ï¼ˆä½¿ç”¨scatter_addï¼‰
        # å°†çª—å£å†…çš„ç´¢å¼•clipåˆ°æœ‰æ•ˆèŒƒå›´
        query_indices_clamped = query_indices.clamp(0, total_features - 1).long()  # ğŸ”¥ å¼ºåˆ¶è½¬æ¢ä¸ºint64
        
        # é€batchå¤„ç†ï¼ˆé¿å…scatter_addçš„ç»´åº¦é—®é¢˜ï¼‰
        for b in range(batch_size):
            bias[b].scatter_add_(
                dim=1,  # åœ¨featureç»´åº¦scatter
                index=query_indices_clamped[b],  # [num_queries, window_size^2]
                src=weights_masked[b]             # [num_queries, window_size^2]
            )
        
        return bias
    
    def _generate_camera_bias(self, weights, positions):
        """
        ç”ŸæˆCameraé€è§†ç‰¹å¾å›¾çš„å±€éƒ¨bias
        
        Args:
            weights: [bs, num_queries] æƒé‡
            positions: [bs, num_queries, 3] 3Dä½ç½®ç´¢å¼• (view, h, w)
        
        Returns:
            bias: [bs, num_queries, num_views*pers_h*pers_w]
        """
        batch_size, num_queries = weights.shape
        device = weights.device
        total_features = self.num_views * self.pers_h * self.pers_w
        
        if not self.use_local_bias:
            # å…¨å±€bias
            return weights.unsqueeze(-1).expand(batch_size, num_queries, total_features)
        
        # === å±€éƒ¨biasï¼šå‘é‡åŒ–å®ç° ===
        
        # 1. è§£æ3Dä½ç½®
        view_indices = positions[..., 0].long()  # [bs, num_queries]
        h_indices = positions[..., 1].long()     # [bs, num_queries]
        w_indices = positions[..., 2].long()     # [bs, num_queries]
        
        # 2. è®¡ç®—1Dç´¢å¼•
        positions_1d = (view_indices * self.pers_h * self.pers_w + 
                       h_indices * self.pers_w + 
                       w_indices)  # [bs, num_queries]
        
        # 3. è®¡ç®—çª—å£ç´¢å¼•ï¼ˆåªåœ¨åŒä¸€è§†è§’å†…ï¼‰
        query_indices = positions_1d.unsqueeze(-1) + self.window_offsets_pers.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        # 4. è¾¹ç•Œæ£€æŸ¥ï¼ˆ2Dç½‘æ ¼ + è§†è§’ä¸€è‡´æ€§ï¼‰
        window_h = h_indices.unsqueeze(-1) + self.y_offsets.unsqueeze(0).unsqueeze(0)
        window_w = w_indices.unsqueeze(-1) + self.x_offsets.unsqueeze(0).unsqueeze(0)
        # â†’ [bs, num_queries, window_size^2]
        
        valid_h = (window_h >= 0) & (window_h < self.pers_h)
        valid_w = (window_w >= 0) & (window_w < self.pers_w)
        
        # ç¡®ä¿çª—å£ä¸è·¨è§†è§’ï¼ˆæ£€æŸ¥çª—å£ç´¢å¼•æ˜¯å¦åœ¨åŒä¸€è§†è§’å†…ï¼‰
        window_view = torch.div(query_indices, self.pers_h * self.pers_w, rounding_mode='floor').long()
        valid_view = (window_view == view_indices.unsqueeze(-1))
        
        valid_mask = valid_h & valid_w & valid_view  # [bs, num_queries, window_size^2]
        
        # 5. åˆ›å»ºbiasçŸ©é˜µ
        bias = torch.zeros(batch_size, num_queries, total_features,
                          device=device, dtype=weights.dtype)
        
        # 6. æ‰©å±•æƒé‡åˆ°çª—å£
        weights_expanded = weights.unsqueeze(-1).expand(-1, -1, self.window_size**2)
        
        # ğŸ”¥ 6.5. åº”ç”¨é«˜æ–¯è¡°å‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gaussian_window:
            weights_expanded = weights_expanded * self.gaussian_weights.unsqueeze(0).unsqueeze(0)
        
        weights_masked = torch.where(valid_mask, weights_expanded,
                                     torch.zeros_like(weights_expanded))
        
        # 7. å‘é‡åŒ–å¡«å……
        query_indices_clamped = query_indices.clamp(0, total_features - 1).long()  # ğŸ”¥ å¼ºåˆ¶è½¬æ¢ä¸ºint64
        
        for b in range(batch_size):
            bias[b].scatter_add_(
                dim=1,
                index=query_indices_clamped[b],
                src=weights_masked[b]
            )
        
        return bias
    
    def get_memory_usage(self, batch_size, num_queries):
        """
        ä¼°ç®—å†…å­˜å ç”¨
        
        Args:
            batch_size: batchå¤§å°
            num_queries: queryæ•°é‡
        
        Returns:
            dict: å†…å­˜ä½¿ç”¨ä¿¡æ¯
        """
        total_features = self.bev_h * self.bev_w + self.num_views * self.pers_h * self.pers_w
        
        # biasçŸ©é˜µå¤§å°
        bias_elements = batch_size * num_queries * total_features
        bias_memory_fp32 = bias_elements * 4 / (1024**2)  # MB
        bias_memory_fp16 = bias_elements * 2 / (1024**2)  # MB
        
        return {
            'total_features': total_features,
            'bias_shape': (batch_size, num_queries, total_features),
            'memory_fp32_mb': bias_memory_fp32,
            'memory_fp16_mb': bias_memory_fp16,
            'current_dtype': 'fp16' if self.fp16 else 'fp32',
            'estimated_memory_mb': bias_memory_fp16 if self.fp16 else bias_memory_fp32
        }


# === é»˜è®¤é…ç½® ===
DEFAULT_ATTENTION_BIAS_CONFIG = dict(
    type='AttentionBiasGenerator',
    bev_feature_shape=(180, 180),
    pers_feature_shape=(6, 40, 100),
    window_size=15,                # æ¨èå€¼ï¼šä¸LAMçš„camera windowä¸€è‡´
    bias_scale=1.0,                # åˆå§‹å»ºè®®ï¼š1.0ï¼ˆæ— ç¼©æ”¾ï¼‰
    use_local_bias=True,           # æ¨èï¼šTrueï¼ˆå±€éƒ¨biasï¼‰
    fp16=True                      # æ¨èï¼šTrueï¼ˆèŠ‚çœå†…å­˜ï¼‰
)

