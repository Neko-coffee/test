import math
import copy
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as cp

from einops import rearrange
from mmcv.cnn.bricks.drop import build_dropout
from mmcv.runner.base_module import BaseModule

from mmcv.cnn.bricks.transformer import (
    BaseTransformerLayer,
    TransformerLayerSequence,
    build_transformer_layer_sequence
)
from mmcv.cnn import (
    build_activation_layer,
    build_conv_layer,
    build_norm_layer,
    xavier_init
)
from mmcv.cnn.bricks.registry import (
    ATTENTION,TRANSFORMER_LAYER,
    TRANSFORMER_LAYER_SEQUENCE
)
from mmcv.utils import (
    ConfigDict,
    build_from_cfg,
    deprecated_api_warning,
    to_2tuple
)
from mmdet.models.utils.builder import TRANSFORMER


@ATTENTION.register_module()
class PETRMultiheadAttention(BaseModule):
    """A wrapper for ``torch.nn.MultiheadAttention``.
    This module implements MultiheadAttention with identity connection,
    and positional encoding  is also passed as input.
    Args:
        embed_dims (int): The embedding dimension.
        num_heads (int): Parallel attention heads.
        attn_drop (float): A Dropout layer on attn_output_weights.
            Default: 0.0.
        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.
            Default: 0.0.
        dropout_layer (obj:`ConfigDict`): The dropout_layer used
            when adding the shortcut.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
        batch_first (bool): When it is True,  Key, Query and Value are shape of
            (batch, n, embed_dim), otherwise (n, batch, embed_dim).
             Default to False.
    """

    def __init__(self,
                 embed_dims,
                 num_heads,
                 attn_drop=0.,
                 proj_drop=0.,
                 dropout_layer=dict(type='Dropout', drop_prob=0.),
                 init_cfg=None,
                 batch_first=False,
                 **kwargs):
        super(PETRMultiheadAttention, self).__init__(init_cfg)
        if 'dropout' in kwargs:
            warnings.warn(
                'The arguments `dropout` in MultiheadAttention '
                'has been deprecated, now you can separately '
                'set `attn_drop`(float), proj_drop(float), '
                'and `dropout_layer`(dict) ', DeprecationWarning)
            attn_drop = kwargs['dropout']
            dropout_layer['drop_prob'] = kwargs.pop('dropout')

        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.batch_first = batch_first

        self.attn = nn.MultiheadAttention(embed_dims, num_heads, attn_drop,
                                          **kwargs)

        self.proj_drop = nn.Dropout(proj_drop)
        self.dropout_layer = build_dropout(
            dropout_layer) if dropout_layer else nn.Identity()

    @deprecated_api_warning({'residual': 'identity'},
                            cls_name='MultiheadAttention')
    def forward(self,
                query,
                key=None,
                value=None,
                identity=None,
                query_pos=None,
                key_pos=None,
                attn_mask=None,
                key_padding_mask=None,
                attention_bias=None,  # ğŸ”¥ æ–°å¢ï¼šAQR attention bias
                **kwargs):
        """Forward function for `MultiheadAttention`.
        **kwargs allow passing a more general data flow when combining
        with other operations in `transformerlayer`.
        Args:
            query (Tensor): The input query with shape [num_queries, bs,
                embed_dims] if self.batch_first is False, else
                [bs, num_queries embed_dims].
            key (Tensor): The key tensor with shape [num_keys, bs,
                embed_dims] if self.batch_first is False, else
                [bs, num_keys, embed_dims] .
                If None, the ``query`` will be used. Defaults to None.
            value (Tensor): The value tensor with same shape as `key`.
                Same in `nn.MultiheadAttention.forward`. Defaults to None.
                If None, the `key` will be used.
            identity (Tensor): This tensor, with the same shape as x,
                will be used for the identity link.
                If None, `x` will be used. Defaults to None.
            query_pos (Tensor): The positional encoding for query, with
                the same shape as `x`. If not None, it will
                be added to `x` before forward function. Defaults to None.
            key_pos (Tensor): The positional encoding for `key`, with the
                same shape as `key`. Defaults to None. If not None, it will
                be added to `key` before forward function. If None, and
                `query_pos` has the same shape as `key`, then `query_pos`
                will be used for `key_pos`. Defaults to None.
            attn_mask (Tensor): ByteTensor mask with shape [num_queries,
                num_keys]. Same in `nn.MultiheadAttention.forward`.
                Defaults to None.
            key_padding_mask (Tensor): ByteTensor with shape [bs, num_keys].
                Defaults to None.
            attention_bias (Tensor): AQR attention bias with shape [num_queries, bs, num_keys].
                Will be added to attention scores before softmax. Only applied in cross-attention.
                Defaults to None.
        Returns:
            Tensor: forwarded results with shape
            [num_queries, bs, embed_dims]
            if self.batch_first is False, else
            [bs, num_queries embed_dims].
        """

        if key is None:
            key = query
        if value is None:
            value = key
        if identity is None:
            identity = query
        if key_pos is None:
            if query_pos is not None:
                # use query_pos if key_pos is not available
                if query_pos.shape == key.shape:
                    key_pos = query_pos
                else:
                    warnings.warn(f'position encoding of key is'
                                  f'missing in {self.__class__.__name__}.')
        if query_pos is not None:
            query = query + query_pos
        if key_pos is not None:
            key = key + key_pos

        # ğŸ”¥ å¤„ç†attention_bias
        final_attn_mask = attn_mask
        
        if attention_bias is not None:
            # Step 1: åˆ¤æ–­æ˜¯self-attnè¿˜æ˜¯cross-attn
            # Self-attn: keyæ¥è‡ªquery (key.shape[0] == query.shape[0])
            # Cross-attn: keyæ¥è‡ªmemory (key.shape[0] != query.shape[0])
            is_cross_attn = (key.shape[0] != query.shape[0])
            
            if is_cross_attn:
                # åªåœ¨Cross-Attentionä¸­åº”ç”¨attention_bias
                # attention_bias: [num_queries, bs, num_features]
                
                # Step 2: è½¬æ¢ä¸ºPyTorch MultiheadAttentionæœŸæœ›çš„æ ¼å¼
                # éœ€è¦æ‰©å±•åˆ°å¤šå¤´: [bs*num_heads, num_queries, num_features]
                num_queries, bs, num_features = attention_bias.shape
                
                # [num_queries, bs, num_features] â†’ [bs, num_queries, num_features]
                bias = attention_bias.transpose(0, 1)
                
                # æ‰©å±•åˆ°å¤šå¤´
                bias = bias.unsqueeze(1)  # [bs, 1, num_queries, num_features]
                bias = bias.expand(-1, self.num_heads, -1, -1)  # [bs, num_heads, num_queries, num_features]
                bias = bias.reshape(bs * self.num_heads, num_queries, num_features)
                # â†’ [bs*num_heads, num_queries, num_features]
                
                # Step 3: ä¸åŸæœ‰attn_maskåˆå¹¶
                if final_attn_mask is not None:
                    if final_attn_mask.dtype == torch.bool:
                        # Bool maskè½¬ä¸ºfloat: True â†’ -inf
                        mask_float = torch.zeros_like(bias)
                        
                        # å¤„ç†ç»´åº¦ä¸åŒ¹é…
                        if final_attn_mask.dim() == 2:
                            # [num_queries, num_features] â†’ [bs*num_heads, num_queries, num_features]
                            final_attn_mask = final_attn_mask.unsqueeze(0).expand(bs * self.num_heads, -1, -1)
                        
                        mask_float.masked_fill_(final_attn_mask, float('-inf'))
                        final_attn_mask = mask_float + bias
                    else:
                        # Float maskç›´æ¥åŠ 
                        final_attn_mask = final_attn_mask + bias
                else:
                    final_attn_mask = bias

        # Because the dataflow('key', 'query', 'value') of
        # ``torch.nn.MultiheadAttention`` is (num_query, batch,
        # embed_dims), We should adjust the shape of dataflow from
        # batch_first (batch, num_query, embed_dims) to num_query_first
        # (num_query ,batch, embed_dims), and recover ``attn_output``
        # from num_query_first to batch_first.
        if self.batch_first:
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†attn_maskå’Œkey_padding_maskçš„å…¼å®¹æ€§
        # PyTorchæ—§ç‰ˆæœ¬è¦æ±‚ï¼šå¦‚æœattn_maskæ˜¯floatï¼ˆåŠ æ€§maskï¼‰ï¼Œä¸èƒ½åŒæ—¶ä¼ é€’key_padding_mask
        # å› ä¸ºattention_biaså·²ç»æ˜¯åŠ æ€§maskï¼Œæ‰€ä»¥å½“æœ‰attention_biasæ—¶ä¸ä¼ é€’key_padding_mask
        if attention_bias is not None and final_attn_mask is not None:
            # æœ‰attention_biasæ—¶ï¼šåªä½¿ç”¨attn_maskï¼ˆå·²åŒ…å«biasï¼‰ï¼Œå¿½ç•¥key_padding_mask
            attn_output, attn_weights = self.attn(
                query=query,
                key=key,
                value=value,
                attn_mask=final_attn_mask,  # ğŸ”¥ floatç±»å‹çš„åŠ æ€§maskï¼ˆåŒ…å«attention_biasï¼‰
                key_padding_mask=None,      # ğŸ”¥ ä¸ä¼ é€’ï¼Œé¿å…ç±»å‹å†²çª
                need_weights=True
            )
        else:
            # æ— attention_biasæ—¶ï¼šæ­£å¸¸ä¼ é€’key_padding_mask
            attn_output, attn_weights = self.attn(
                query=query,
                key=key,
                value=value,
                attn_mask=final_attn_mask,
                key_padding_mask=key_padding_mask,
                need_weights=True
            )
        
        # ğŸ”¥ ä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºå¯è§†åŒ–ï¼‰
        if hasattr(self, 'save_attn_weights') and self.save_attn_weights:
            self.last_attn_weights = attn_weights  # [bs*num_heads, num_queries, num_features]
            if attention_bias is not None:
                self.last_attention_bias = attention_bias  # ä¿å­˜å¯¹åº”çš„bias

        if self.batch_first:
            attn_output = attn_output.transpose(0, 1)

        return identity + self.dropout_layer(self.proj_drop(attn_output))


from .attention import FlashMHA

@ATTENTION.register_module()
class PETRMultiheadFlashAttention(BaseModule):
    """A wrapper for ``torch.nn.MultiheadAttention``.
    This module implements MultiheadAttention with identity connection,
    and positional encoding  is also passed as input.
    Args:
        embed_dims (int): The embedding dimension.
        num_heads (int): Parallel attention heads.
        attn_drop (float): A Dropout layer on attn_output_weights.
            Default: 0.0.
        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.
            Default: 0.0.
        dropout_layer (obj:`ConfigDict`): The dropout_layer used
            when adding the shortcut.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
        batch_first (bool): When it is True,  Key, Query and Value are shape of
            (batch, n, embed_dim), otherwise (n, batch, embed_dim).
             Default to False.
    """

    def __init__(self,
                 embed_dims,
                 num_heads,
                 attn_drop=0.,
                 proj_drop=0.,
                 dropout_layer=dict(type='Dropout', drop_prob=0.),
                 init_cfg=None,
                 batch_first=True,
                 **kwargs):
        super(PETRMultiheadFlashAttention, self).__init__(init_cfg)
        if 'dropout' in kwargs:
            warnings.warn(
                'The arguments `dropout` in MultiheadAttention '
                'has been deprecated, now you can separately '
                'set `attn_drop`(float), proj_drop(float), '
                'and `dropout_layer`(dict) ', DeprecationWarning)
            attn_drop = kwargs['dropout']
            dropout_layer['drop_prob'] = kwargs.pop('dropout')

        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.batch_first = True

        # ğŸ”¥ æ·»åŠ use_flashbiaså‚æ•°
        self.use_flashbias = kwargs.pop('use_flashbias', True)
        
        # æ ¹æ®FlashBiaså¯ç”¨æ€§é€‰æ‹©æ³¨æ„åŠ›å®ç°
        from .attention import FLASHBIAS_AVAILABLE, FlashBiasAttention
        
        if self.use_flashbias and FLASHBIAS_AVAILABLE:
            print("âœ… ä½¿ç”¨ FlashBias æ³¨æ„åŠ›")
            self.attn = FlashBiasAttention(embed_dims, num_heads, dropout=attn_drop, **kwargs)
        else:
            print("âš ï¸ ä½¿ç”¨æ ‡å‡† FlashBias æ³¨æ„åŠ›ï¼ˆå›é€€æ¨¡å¼ï¼‰")
            self.attn = FlashBiasAttention(embed_dims, num_heads, dropout=attn_drop, **kwargs)

        self.proj_drop = nn.Dropout(proj_drop)
        self.dropout_layer = build_dropout(
            dropout_layer) if dropout_layer else nn.Identity()

    @deprecated_api_warning({'residual': 'identity'},
                            cls_name='MultiheadAttention')
    def forward(self,
                query,
                key=None,
                value=None,
                identity=None,
                query_pos=None,
                key_pos=None,
                attn_mask=None,
                key_padding_mask=None,
                attention_bias=None,  # ğŸ”¥ æ·»åŠ attention_biaså‚æ•°
                **kwargs):
        """Forward function for `MultiheadAttention`.
        **kwargs allow passing a more general data flow when combining
        with other operations in `transformerlayer`.
        Args:
            query (Tensor): The input query with shape [num_queries, bs,
                embed_dims] if self.batch_first is False, else
                [bs, num_queries embed_dims].
            key (Tensor): The key tensor with shape [num_keys, bs,
                embed_dims] if self.batch_first is False, else
                [bs, num_keys, embed_dims] .
                If None, the ``query`` will be used. Defaults to None.
            value (Tensor): The value tensor with same shape as `key`.
                Same in `nn.MultiheadAttention.forward`. Defaults to None.
                If None, the `key` will be used.
            identity (Tensor): This tensor, with the same shape as x,
                will be used for the identity link.
                If None, `x` will be used. Defaults to None.
            query_pos (Tensor): The positional encoding for query, with
                the same shape as `x`. If not None, it will
                be added to `x` before forward function. Defaults to None.
            key_pos (Tensor): The positional encoding for `key`, with the
                same shape as `key`. Defaults to None. If not None, it will
                be added to `key` before forward function. If None, and
                `query_pos` has the same shape as `key`, then `query_pos`
                will be used for `key_pos`. Defaults to None.
            attn_mask (Tensor): ByteTensor mask with shape [num_queries,
                num_keys]. Same in `nn.MultiheadAttention.forward`.
                Defaults to None.
            key_padding_mask (Tensor): ByteTensor with shape [bs, num_keys].
                Defaults to None.
        Returns:
            Tensor: forwarded results with shape
            [num_queries, bs, embed_dims]
            if self.batch_first is False, else
            [bs, num_queries embed_dims].
        """

        if key is None:
            key = query
        if value is None:
            value = key
        if identity is None:
            identity = query
        if key_pos is None:
            if query_pos is not None:
                # use query_pos if key_pos is not available
                if query_pos.shape == key.shape:
                    key_pos = query_pos
                else:
                    warnings.warn(f'position encoding of key is'
                                  f'missing in {self.__class__.__name__}.')
        if query_pos is not None:
            query = query + query_pos
        if key_pos is not None:
            key = key + key_pos

        # Because the dataflow('key', 'query', 'value') of
        # ``torch.nn.MultiheadAttention`` is (num_query, batch,
        # embed_dims), We should adjust the shape of dataflow from
        # batch_first (batch, num_query, embed_dims) to num_query_first
        # (num_query ,batch, embed_dims), and recover ``attn_output``
        # from num_query_first to batch_first.
        if self.batch_first:
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)
        
        # ğŸ”¥ ä¼ é€’attention_biasåˆ°FlashMHA
        out = self.attn(
            q=query,
            k=key,
            v=value,
            key_padding_mask=None,
            attn_bias=attention_bias  # ğŸ”¥ ä¼ é€’attention_bias
        )[0]

        if self.batch_first:
            out = out.transpose(0, 1)

        return identity + self.dropout_layer(self.proj_drop(out))


@TRANSFORMER_LAYER_SEQUENCE.register_module()
class PETRTransformerDecoder(TransformerLayerSequence):
    """Implements the decoder in DETR transformer.
    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        post_norm_cfg (dict): Config of last normalization layer. Defaultï¼š
            `LN`.
    """

    def __init__(self,
                 *args,
                 post_norm_cfg=dict(type='LN'),
                 return_intermediate=False,
                 **kwargs):

        super(PETRTransformerDecoder, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        if post_norm_cfg is not None:
            self.post_norm = build_norm_layer(post_norm_cfg,
                                              self.embed_dims)[1]
        else:
            self.post_norm = None

    def forward(self, query, *args, **kwargs):
        """Forward function for `TransformerDecoder`.
        Args:
            query (Tensor): Input query with shape
                `(num_query, bs, embed_dims)`.
        Returns:
            Tensor: Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims].
        """
        if not self.return_intermediate:
            x = super().forward(query, *args, **kwargs)
            if self.post_norm:
                x = self.post_norm(x)[None]
            return x

        intermediate = []
        for layer in self.layers:
            query = layer(query, *args, **kwargs)
            if self.return_intermediate:
                if self.post_norm is not None:
                    intermediate.append(self.post_norm(query))
                else:
                    intermediate.append(query)
        return torch.stack(intermediate)


@TRANSFORMER_LAYER.register_module()
class PETRTransformerDecoderLayer(BaseTransformerLayer):
    """Implements decoder layer in DETR transformer.
    Args:
        attn_cfgs (list[`mmcv.ConfigDict`] | list[dict] | dict )):
            Configs for self_attention or cross_attention, the order
            should be consistent with it in `operation_order`. If it is
            a dict, it would be expand to the number of attention in
            `operation_order`.
        feedforward_channels (int): The hidden dimension for FFNs.
        ffn_dropout (float): Probability of an element to be zeroed
            in ffn. Default 0.0.
        operation_order (tuple[str]): The execution order of operation
            in transformer. Such as ('self_attn', 'norm', 'ffn', 'norm').
            Defaultï¼šNone
        act_cfg (dict): The activation config for FFNs. Default: `LN`
        norm_cfg (dict): Config dict for normalization layer.
            Default: `LN`.
        ffn_num_fcs (int): The number of fully-connected layers in FFNs.
            Defaultï¼š2.
    """

    def __init__(self,
                 attn_cfgs,
                 feedforward_channels,
                 ffn_dropout=0.0,
                 operation_order=None,
                 act_cfg=dict(type='ReLU', inplace=True),
                 norm_cfg=dict(type='LN'),
                 ffn_num_fcs=2,
                 with_cp=True,
                 **kwargs):
        super(PETRTransformerDecoderLayer, self).__init__(
            attn_cfgs=attn_cfgs,
            feedforward_channels=feedforward_channels,
            ffn_dropout=ffn_dropout,
            operation_order=operation_order,
            act_cfg=act_cfg,
            norm_cfg=norm_cfg,
            ffn_num_fcs=ffn_num_fcs,
            **kwargs)
        # ğŸ”¥ æ”¯æŒä¸¤ç§æ“ä½œé¡ºåºï¼š
        # 1. å®Œæ•´æ¨¡å¼ï¼ˆCMTä¸»Transformerï¼‰: 6ä¸ªæ“ä½œï¼ŒåŒ…å«self_attn
        # 2. ç®€åŒ–æ¨¡å¼ï¼ˆAQRæƒé‡ç”Ÿæˆå™¨ï¼‰: 4ä¸ªæ“ä½œï¼Œåªæœ‰cross_attn
        valid_ops = set(['self_attn', 'norm', 'cross_attn', 'ffn'])
        assert len(operation_order) in [4, 6], \
            f"operation_order length must be 4 or 6, got {len(operation_order)}"
        assert set(operation_order).issubset(valid_ops), \
            f"operation_order contains invalid operations: {set(operation_order) - valid_ops}"
        
        # æ£€æŸ¥æ¨¡å¼
        self.has_self_attn = 'self_attn' in operation_order
        if self.has_self_attn:
            assert len(operation_order) == 6, \
                "Full mode (with self_attn) requires 6 operations"
        else:
            assert len(operation_order) == 4, \
                "Simplified mode (without self_attn) requires 4 operations"
        
        self.use_checkpoint = with_cp
    
    def _forward(self, 
                query,
                key=None,
                value=None,
                query_pos=None,
                key_pos=None,
                attn_masks=None,
                query_key_padding_mask=None,
                key_padding_mask=None,
                attention_bias=None,  # ğŸ”¥ æ–°å¢
                ):
        """Forward function for `TransformerCoder`.
        Args:
            attention_bias (Tensor): Attention bias with shape [num_query, bs, num_features]. Default: None.
        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        x = super(PETRTransformerDecoderLayer, self).forward(
                query,
                key=key,
                value=value,
                query_pos=query_pos,
                key_pos=key_pos,
                attn_masks=attn_masks,
                query_key_padding_mask=query_key_padding_mask,
                key_padding_mask=key_padding_mask,
                attention_bias=attention_bias,  # ğŸ”¥ ä¼ é€’
                )

        return x

    def forward(self, 
                query,
                key=None,
                value=None,
                query_pos=None,
                key_pos=None,
                attn_masks=None,
                query_key_padding_mask=None,
                key_padding_mask=None,
                attention_bias=None,  # ğŸ”¥ æ–°å¢
                **kwargs
                ):
        """Forward function for `TransformerCoder`.
        Args:
            attention_bias (Tensor): Attention bias with shape [num_query, bs, num_features]. Default: None.
        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """

        if self.use_checkpoint and self.training:
            x = cp.checkpoint(
                self._forward, 
                query,
                key,
                value,
                query_pos,
                key_pos,
                attn_masks,
                query_key_padding_mask,
                key_padding_mask,
                attention_bias,  # ğŸ”¥ ä¼ é€’
                )
        else:
            x = self._forward(
            query,
            key=key,
            value=value,
            query_pos=query_pos,
            key_pos=key_pos,
            attn_masks=attn_masks,
            query_key_padding_mask=query_key_padding_mask,
            key_padding_mask=key_padding_mask,
            attention_bias=attention_bias  # ğŸ”¥ ä¼ é€’
            )
        
        return x
