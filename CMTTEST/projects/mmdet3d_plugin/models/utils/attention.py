# Copyright (c) 2023 megvii-model. All Rights Reserved.

import math
import torch
import torch.nn as nn
from torch.nn.init import (
    xavier_uniform_,
    constant_,
    xavier_normal_
)
from torch.nn.functional import linear
import sys
import os

from einops import rearrange
from mmcv.runner import auto_fp16
from mmcv.runner.base_module import BaseModule

# ğŸ”¥ FlashBias å¯¼å…¥
FLASHBIAS_AVAILABLE = False
FLASHBIAS_VERSION = None

try:
    # å°è¯•ä» external/FlashBias å¯¼å…¥
    flashbias_path = os.path.join(os.path.dirname(__file__), '../../../../external/FlashBias')
    flashbias_abs_path = os.path.abspath(flashbias_path)
    
    print(f"ğŸ” å°è¯•ä»è·¯å¾„å¯¼å…¥ FlashBias: {flashbias_abs_path}")
    
    if os.path.exists(flashbias_abs_path):
        print(f"âœ… FlashBias è·¯å¾„å­˜åœ¨: {flashbias_abs_path}")
        sys.path.insert(0, flashbias_abs_path)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        flash_bias_file = os.path.join(flashbias_abs_path, 'flash_bias_triton.py')
        if os.path.exists(flash_bias_file):
            print(f"âœ… FlashBias æ–‡ä»¶å­˜åœ¨: {flash_bias_file}")
            from flash_bias_triton import flash_bias_func
            FLASHBIAS_AVAILABLE = True
            FLASHBIAS_VERSION = "triton"
            print("âœ… FlashBias (Triton) loaded successfully!")
        else:
            print(f"âŒ FlashBias æ–‡ä»¶ä¸å­˜åœ¨: {flash_bias_file}")
            raise ImportError("flash_bias_triton.py not found")
    else:
        print(f"âŒ FlashBias è·¯å¾„ä¸å­˜åœ¨: {flashbias_abs_path}")
        # å°è¯•ä»ç³»ç»Ÿè·¯å¾„å¯¼å…¥
        try:
            from flash_bias_triton import flash_bias_func
            FLASHBIAS_AVAILABLE = True
            FLASHBIAS_VERSION = "system"
            print("âœ… FlashBias (System) loaded successfully!")
        except ImportError:
            print("âš ï¸ FlashBias not available in system path")
            raise ImportError("FlashBias not found in system path")
            
except ImportError as e:
    print(f"âš ï¸ FlashBias import failed: {e}")
    FLASHBIAS_AVAILABLE = False
    # æä¾›å ä½å‡½æ•°
    def flash_bias_func(*args, **kwargs):
        raise NotImplementedError("FlashBias not installed")


def _in_projection_packed(q, k, v, w, b=None):
    """è¾“å…¥æŠ•å½±çš„æ‰“åŒ…ç‰ˆæœ¬"""
    w_q, w_k, w_v = w.chunk(3)
    if b is None:
        b_q = b_k = b_v = None
    else:
        b_q, b_k, b_v = b.chunk(3)
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)


class FlashBiasAttention(nn.Module):
    """
    FlashBias æ³¨æ„åŠ›å®ç°
    ä¸“é—¨ä¸º FlashBias ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒ attention_bias
    
    ç‰¹æ€§ï¼š
    - æ”¯æŒ Triton-based FlashBiasï¼ˆæœ€ä½³æ€§èƒ½ï¼‰
    - æ”¯æŒ PyTorch-SDPA-based FlashBiasï¼ˆå…¼å®¹æ€§ï¼‰
    - è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†æ³¨æ„åŠ›ï¼ˆä¿åº•ï¼‰
    - æ™ºèƒ½åç½®è½¬æ¢ï¼ˆattn_bias â†’ q_bias + k_biasï¼‰
    """
    
    def __init__(self, embed_dims, num_heads, dropout=0.0, bias=True, **kwargs):
        super(FlashBiasAttention, self).__init__()
        
        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.head_dim = embed_dims // num_heads
        self.dropout = dropout
        
        assert self.head_dim * num_heads == embed_dims, "embed_dims must be divisible by num_heads"
        
        # è¾“å…¥æŠ•å½±
        self.in_proj_weight = nn.Parameter(torch.randn(3 * embed_dims, embed_dims))
        if bias:
            self.in_proj_bias = nn.Parameter(torch.randn(3 * embed_dims))
        else:
            self.register_parameter('in_proj_bias', None)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(embed_dims, embed_dims, bias=bias)
        
        # FlashBias å¯ç”¨æ€§æ£€æŸ¥
        if not FLASHBIAS_AVAILABLE:
            print("âš ï¸ FlashBias not available, will use standard attention")

    # def forward(self, query, key, value, attn_mask=None, key_padding_mask=None,
    #             attn_bias=None, **kwargs):
    def forward(self, query=None, key=None, value=None, q=None, k=None, v=None, 
                attn_mask=None, key_padding_mask=None, attn_bias=None, **kwargs):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query/q: [batch_size, seq_len, embed_dims] æŸ¥è¯¢
            key/k: [batch_size, seq_len, embed_dims] é”®
            value/v: [batch_size, seq_len, embed_dims] å€¼
            attn_mask: æ³¨æ„åŠ›æ©ç 
            key_padding_mask: é”®å¡«å……æ©ç 
            attn_bias: [batch_size, num_heads, seq_len, seq_len] æ³¨æ„åŠ›åç½®
            
        Returns:
            context: [batch_size, seq_len, embed_dims] è¾“å‡º
            attn_weights: æ³¨æ„åŠ›æƒé‡ï¼ˆFlashBias ä¸è¿”å›ï¼‰
        """
        # ğŸ”¥ å‚æ•°å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒ q/k/v å’Œ query/key/value ä¸¤ç§è°ƒç”¨æ–¹å¼
        if query is None and q is not None:
            query = q
        if key is None and k is not None:
            key = k
        if value is None and v is not None:
            value = v
            
        # éªŒè¯å¿…è¦å‚æ•°
        if query is None or key is None or value is None:
            raise ValueError("query/key/value æˆ– q/k/v å‚æ•°å¿…é¡»æä¾›")

        batch_size, seq_len_q, embed_dims = query.shape
        _, seq_len_k, _ = key.shape
        _, seq_len_v, _ = value.shape
        
        # ğŸ”¥ ä¿å­˜åŸå§‹ dtypeï¼ˆç”¨äºæœ€åæ¢å¤ï¼‰
        original_query_dtype = query.dtype

        # ğŸ”¥ è°ƒè¯•ä¿¡æ¯
        # print(f"ğŸ” FlashBiasAttention Debug:")
        # print(f"   Input shapes: query={query.shape}, key={key.shape}, value={value.shape}")
        # print(f"   embed_dims={embed_dims}, num_heads={self.num_heads}, head_dim={self.head_dim}")
        # print(f"   seq_len_q={seq_len_q}, seq_len_k={seq_len_k}, seq_len_v={seq_len_v}")
        
        
        # è¾“å…¥æŠ•å½±
        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight, self.in_proj_bias)

        # ğŸ”¥ æŠ•å½±åè°ƒè¯•ä¿¡æ¯
        # print(f"   After projection: q={q.shape}, k={k.shape}, v={v.shape}")
        
        # éªŒè¯ç»´åº¦åŒ¹é…
        if embed_dims != self.num_heads * self.head_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: embed_dims={embed_dims} != num_heads*head_dim={self.num_heads * self.head_dim}")
            # å°è¯•è‡ªåŠ¨è°ƒæ•´
            if embed_dims % self.num_heads == 0:
                self.head_dim = embed_dims // self.num_heads
                print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´ head_dim ä¸º: {self.head_dim}")
            else:
                raise ValueError(f"embed_dims ({embed_dims}) å¿…é¡»èƒ½è¢« num_heads ({self.num_heads}) æ•´é™¤")
        
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ - ä½¿ç”¨å„è‡ªçš„åºåˆ—é•¿åº¦
        try:
            q = q.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)
            k = k.view(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(1, 2)
            v = v.view(batch_size, seq_len_v, self.num_heads, self.head_dim).transpose(1, 2)
            # print(f"   After reshape: q={q.shape}, k={k.shape}, v={v.shape}")
        except RuntimeError as e:
            print(f"âŒ é‡å¡‘å¤±è´¥: {e}")
            print(f"   q size: {q.numel()}, expected: {batch_size * seq_len_q * self.num_heads * self.head_dim}")
            print(f"   k size: {k.numel()}, expected: {batch_size * seq_len_k * self.num_heads * self.head_dim}")
            print(f"   v size: {v.numel()}, expected: {batch_size * seq_len_v * self.num_heads * self.head_dim}")
            raise
        
        # ğŸ”¥ å¯¹äº FlashBiasï¼Œç¡®ä¿è¾“å…¥æ˜¯ fp16/bf16
        # ä½¿ç”¨ attn_bias çš„ dtype ä½œä¸ºç›®æ ‡ï¼ˆå› ä¸ºå®ƒæ¥è‡ª AQRï¼Œé€šå¸¸æ˜¯ fp16ï¼‰
        if attn_bias is not None and FLASHBIAS_AVAILABLE:
            # ä½¿ç”¨ attn_bias çš„ dtype ä½œä¸ºç›®æ ‡
            target_dtype = attn_bias.dtype
            
            # å¦‚æœ attn_bias æ˜¯ fp16/bf16ï¼Œå°† q/k/v ä¹Ÿè½¬æ¢ä¸ºç›¸åŒç±»å‹
            if target_dtype in [torch.float16, torch.bfloat16]:
                if q.dtype != target_dtype:
                    old_dtype = q.dtype
                    q = q.to(target_dtype)
                    k = k.to(target_dtype)
                    v = v.to(target_dtype)
                    print(f"ğŸ”§ å·²å°† q/k/v ä» {old_dtype} è½¬æ¢ä¸º {target_dtype} ä»¥åŒ¹é… FlashBias è¦æ±‚")
            # å¦‚æœ attn_bias ä¸æ˜¯ fp16/bf16ï¼Œå°è¯•è½¬æ¢ä¸º fp16
            elif q.dtype in [torch.float16, torch.bfloat16]:
                # q å·²ç»æ˜¯ fp16/bf16ï¼Œå°† attn_bias è½¬æ¢ä¸ºåŒ¹é…
                target_dtype = q.dtype
                attn_bias = attn_bias.to(target_dtype)
                print(f"ğŸ”§ å·²å°† attn_bias è½¬æ¢ä¸º {target_dtype} ä»¥åŒ¹é… q/k/v")
            else:
                # éƒ½ä¸æ˜¯ fp16/bf16ï¼Œé»˜è®¤è½¬æ¢ä¸º fp16
                target_dtype = torch.float16
                q = q.half()
                k = k.half()
                v = v.half()
                attn_bias = attn_bias.half()
                print(f"ğŸ”§ å·²å°† q/k/v/attn_bias éƒ½è½¬æ¢ä¸º fp16 ä»¥åŒ¹é… FlashBias è¦æ±‚")
        
        # ğŸ”¥ æ³¨æ„åŠ›è®¡ç®—ç­–ç•¥ï¼šAQR + FlashBias æ˜¯é¦–è¦ç›®æ ‡
        # ä¼˜å…ˆçº§ï¼š
        # 1. FlashBias (Triton) - æœ€ä¼˜æ€§èƒ½ + æ”¯æŒ attention_bias
        # 2. PyTorch SDPA FlashBias - å¤‡é€‰æ–¹æ¡ˆ
        # 3. PyTorch SDPA æ ‡å‡† - æœ€ç»ˆå›é€€
        
        # ğŸ”¥ é…ç½®é€‰é¡¹ï¼šæ˜¯å¦ä½¿ç”¨ SVD + FlashBiasï¼ˆTritonï¼‰
        # è®¾ç½®ä¸º False å¯ä»¥è·³è¿‡ SVDï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡† SDPAï¼ˆæ›´å¿«ï¼Œæ˜¾å­˜å ç”¨ç¨é«˜ï¼‰
        USE_SVD_FLASHBIAS = False  # â† æ”¹ä¸º True å¯ç”¨ SVD + FlashBias
        
        if FLASHBIAS_AVAILABLE and attn_bias is not None and USE_SVD_FLASHBIAS:
            # ğŸ¯ æ–¹æ¡ˆ1ï¼šAQR + FlashBias (Triton) - éœ€è¦ SVD
            try:
                print(f"ğŸš€ å¼€å§‹è°ƒç”¨ FlashBias (Triton)ï¼Œq.shape={q.shape}, attn_bias.shape={attn_bias.shape}")
                context = self._flashbias_attention(q, k, v, attn_bias)
                print("âœ… ä½¿ç”¨ FlashBias (Triton) + AQR bias")
            except Exception as e:
                print(f"âš ï¸ FlashBias (Triton) å¤±è´¥: {e}, å°è¯• PyTorch-SDPA FlashBias")
                # å¤‡é€‰æ–¹æ¡ˆï¼šPyTorch SDPA + concat æ–¹å¼
                try:
                    context = self._pytorch_sdpa_attention(q, k, v, attn_bias)
                    print("âœ… ä½¿ç”¨ PyTorch-SDPA FlashBias + AQR bias")
                except Exception as e2:
                    print(f"âš ï¸ PyTorch-SDPA FlashBias å¤±è´¥: {e2}, å›é€€åˆ°æ ‡å‡† SDPA")
                    # æœ€ç»ˆå›é€€ï¼šæ ‡å‡† SDPA + attn_mask
                    context = self._standard_attention(q, k, v, attn_bias)
                    print("âœ… ä½¿ç”¨æ ‡å‡† SDPA + AQR bias (å›é€€)")
        elif attn_bias is not None:
            # ğŸ¯ æ–¹æ¡ˆ2ï¼šç›´æ¥ä½¿ç”¨æ ‡å‡† SDPAï¼ˆå¿«é€Ÿï¼Œæ— éœ€ SVDï¼‰
            # PyTorch 2.1+ çš„ SDPA å·²ç»è‡ªåŠ¨ä½¿ç”¨ FlashAttention
            context = self._standard_attention(q, k, v, attn_bias)
            # print("âœ… ä½¿ç”¨æ ‡å‡† SDPA + AQR bias (æ—  SVD)")
        elif FLASHBIAS_AVAILABLE:
            # æ²¡æœ‰ bias æ—¶ï¼Œä¹Ÿä½¿ç”¨ FlashBias ä¼˜åŒ–æ€§èƒ½
            try:
                context = self._flashbias_attention(q, k, v, None)
            except Exception:
                context = self._standard_attention(q, k, v, None)
        else:
            # FlashBias ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›
            context = self._standard_attention(q, k, v, attn_bias)
        
        # ğŸ”¥ ç¡®ä¿ context çš„ dtype ä¸ out_proj æƒé‡åŒ¹é…
        # å¦‚æœæˆ‘ä»¬è½¬æ¢è¿‡ dtypeï¼ˆä¸ºäº† FlashBiasï¼‰ï¼Œéœ€è¦è½¬æ¢å›å»
        if context.dtype != self.out_proj.weight.dtype:
            context = context.to(self.out_proj.weight.dtype)
        
        # è¾“å‡ºæŠ•å½±
        return self.out_proj(context), None
    
    def _flashbias_attention(self, q, k, v, attn_bias):
        """
        Triton-based FlashBias å®ç°
        FlashBias éœ€è¦è¾“å…¥æ ¼å¼: [batch, seqlen, nheads, headdim]
        è¦æ±‚ï¼šæ‰€æœ‰è¾“å…¥å¿…é¡»æ˜¯ fp16 æˆ– bf16
        """
        batch_size, num_heads, seq_len_q, head_dim = q.shape
        _, _, seq_len_k, _ = k.shape
        
        # ğŸ”¥ ç¡®ä¿è¾“å…¥æ˜¯ fp16/bf16ï¼ˆFlashBias è¦æ±‚ï¼‰
        if q.dtype not in [torch.float16, torch.bfloat16]:
            raise TypeError(f"FlashBias requires fp16/bf16, but got {q.dtype}")
        
        # ğŸ”¥ å°† attn_bias è½¬æ¢ä¸º q_bias å’Œ k_biasï¼ˆé€šè¿‡ SVD ä½ç§©åˆ†è§£ï¼‰
        q_bias, k_bias = self._convert_attn_bias_to_qk_bias(attn_bias)
        # SVD è¾“å‡º: q_bias [batch, num_heads, seq_len_q, rank]
        #          k_bias [batch, num_heads, seq_len_k, rank]
        
        # ğŸ”¥ è½¬æ¢ä¸º FlashBias æœŸæœ›çš„æ ¼å¼: [batch, seqlen, nheads, headdim/rank]
        q_flash = q.transpose(1, 2)  # [batch, seq_len_q, num_heads, head_dim]
        k_flash = k.transpose(1, 2)  # [batch, seq_len_k, num_heads, head_dim]
        v_flash = v.transpose(1, 2)  # [batch, seq_len_k, num_heads, head_dim]
        q_bias_flash = q_bias.transpose(1, 2)  # [batch, seq_len_q, num_heads, rank]
        k_bias_flash = k_bias.transpose(1, 2)  # [batch, seq_len_k, num_heads, rank]
        
        # ğŸ”¥ è°ƒç”¨ FlashBiasï¼ˆTriton å®ç°ï¼‰
        # æ³¨æ„ï¼šflash_bias_func ä¸æ¥å—å…³é”®å­—å‚æ•°ï¼Œå¿…é¡»æŒ‰ä½ç½®ä¼ é€’
        context = flash_bias_func(
            q_flash,          # q
            k_flash,          # k
            v_flash,          # v
            q_bias_flash,     # q_bias
            k_bias_flash,     # k_bias
            None,             # mask
            False,            # causal
            1.0 / math.sqrt(head_dim)  # softmax_scale
        )
        # FlashBias è¾“å‡º: [batch, seq_len_q, num_heads, head_dim]
        
        # ğŸ”¥ è½¬å›æ ‡å‡†æ ¼å¼ [batch, num_heads, seq_len_q, head_dim]
        context = context.transpose(1, 2)
        
        # è½¬å› [batch, seq_len_q, embed_dims]
        context = context.contiguous().view(batch_size, seq_len_q, -1)
        return context
    
    def _pytorch_sdpa_attention(self, q, k, v, attn_bias=None):
        """
        PyTorch-SDPA-based FlashBias å®ç°ï¼ˆGitHubå®˜æ–¹æ–¹æ³•2ï¼‰
        ä½¿ç”¨ concat([q*scale, q_bias], [k, k_bias]) çš„æ–¹å¼
        è¦æ±‚ï¼šconcat åçš„ç»´åº¦èƒ½è¢«8æ•´é™¤ï¼Œæ‰èƒ½æ¿€æ´» FlashAttention åç«¯
        """
        batch_size, num_heads, seq_len_q, head_dim = q.shape
        _, _, seq_len_k, _ = k.shape
        
        if attn_bias is not None:
            # ğŸ”¥ å°† attn_bias é€šè¿‡ SVD åˆ†è§£ä¸º q_bias å’Œ k_bias
            # SVDè¾“å‡º: q_bias [batch, num_heads, seq_len_q, rank]
            #         k_bias [batch, num_heads, seq_len_k, rank]
            q_bias, k_bias = self._convert_attn_bias_to_qk_bias(attn_bias)
            
            # ğŸ”¥ PyTorch-SDPA-based FlashBias (GitHubå®˜æ–¹æ–¹æ³•)
            # è¦æ±‚: concat[q, q_bias] çš„æœ€åä¸€ç»´èƒ½è¢«8æ•´é™¤
            rank = q_bias.shape[-1]
            total_dim = head_dim + rank
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦padding
            if total_dim % 8 != 0:
                pad_size = 8 - (total_dim % 8)
                # å¯¹ q_bias å’Œ k_bias è¿›è¡Œpadding
                q_bias = torch.cat([q_bias, torch.zeros(batch_size, num_heads, seq_len_q, pad_size, device=q_bias.device, dtype=q_bias.dtype)], dim=-1)
                k_bias = torch.cat([k_bias, torch.zeros(batch_size, num_heads, seq_len_k, pad_size, device=k_bias.device, dtype=k_bias.dtype)], dim=-1)
            
            # è®¡ç®— softmax_scale
            softmax_scale = 1.0 / math.sqrt(head_dim)
            
            # ğŸ”¥ æŒ‰ç…§ FlashBias å®˜æ–¹æ–¹å¼æ‹¼æ¥: concat([q*scale, q_bias], [k, k_bias])
            q_concat = torch.cat([q * softmax_scale, q_bias], dim=-1)  # [batch, num_heads, seq_len_q, head_dim+rank]
            k_concat = torch.cat([k, k_bias], dim=-1)                   # [batch, num_heads, seq_len_k, head_dim+rank]
            
            # ä½¿ç”¨ PyTorch SDPAï¼ˆè‡ªåŠ¨ä½¿ç”¨ FlashAttentionï¼‰
            context = torch.nn.functional.scaled_dot_product_attention(
                query=q_concat,
                key=k_concat,
                value=v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0.0,
                scale=1.0,  # å·²ç»åœ¨ q ä¸Šä¹˜è¿‡ scale äº†
                is_causal=False
            )
            # è¾“å‡º: [batch, num_heads, seq_len_q, head_dim]
            
            # è½¬å› [batch, seq_len_q, embed_dims]
            context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)
            return context
        else:
            # æ²¡æœ‰åç½®ï¼Œä½¿ç”¨æ ‡å‡† SDPA
            # q, k, v å·²ç»æ˜¯ [batch, num_heads, seq_len, head_dim] æ ¼å¼
            context = torch.nn.functional.scaled_dot_product_attention(
                query=q,
                key=k,
                value=v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0.0,
                scale=1.0 / math.sqrt(head_dim),
                is_causal=False
            )
            # è¾“å‡º: [batch, num_heads, seq_len_q, head_dim]
            
            # è½¬å› [batch, seq_len_q, embed_dims]
            context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)
            return context
    
    def _standard_attention(self, q, k, v, attn_bias=None):
        """
        é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼ˆä½¿ç”¨ PyTorch SDPAï¼‰
        PyTorch 2.0+ çš„ SDPA ä¼šè‡ªåŠ¨ä½¿ç”¨ FlashAttentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        batch_size, num_heads, seq_len_q, head_dim = q.shape
        _, _, seq_len_k, _ = k.shape
        
        # ğŸ”¥ ä½¿ç”¨ PyTorch 2.0+ çš„ scaled_dot_product_attention
        # è¾“å…¥æ ¼å¼: [batch, num_heads, seq_len, head_dim]
        # q, k, v å·²ç»æ˜¯è¿™ä¸ªæ ¼å¼ï¼Œä¸éœ€è¦è½¬ç½®
        
        # å¤„ç† attention_bias
        attn_mask_sdpa = None
        if attn_bias is not None:
            # SDPA æœŸæœ› attn_mask çš„æ ¼å¼: [batch, num_heads, seq_len_q, seq_len_k]
            # æˆ– [batch, 1, seq_len_q, seq_len_k]ï¼ˆä¼šbroadcaståˆ°æ‰€æœ‰headsï¼‰
            if attn_bias.dim() == 3:
                # [batch, seq_len_q, seq_len_k] â†’ [batch, 1, seq_len_q, seq_len_k]
                attn_mask_sdpa = attn_bias.unsqueeze(1)
            elif attn_bias.dim() == 4:
                # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ [batch, num_heads, seq_len_q, seq_len_k]
                attn_mask_sdpa = attn_bias
            else:
                # å…¶ä»–æƒ…å†µï¼Œæ‰©å±•åˆ°4ç»´
                attn_mask_sdpa = attn_bias.unsqueeze(0).unsqueeze(0)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ attn_mask çš„ dtype ä¸ query åŒ¹é…
            # PyTorch SDPA è¦æ±‚ attn_mask çš„ dtype è¦ä¹ˆæ˜¯ boolï¼Œè¦ä¹ˆä¸ query ç›¸åŒ
            if attn_mask_sdpa.dtype != q.dtype:
                attn_mask_sdpa = attn_mask_sdpa.to(q.dtype)
        
        # ä½¿ç”¨ PyTorch SDPAï¼ˆè‡ªåŠ¨ä½¿ç”¨ FlashAttentionï¼‰
        # è¾“å…¥æ ¼å¼: [batch, num_heads, seq_len, head_dim]
        context = torch.nn.functional.scaled_dot_product_attention(
            query=q,
            key=k,
            value=v,
            attn_mask=attn_mask_sdpa,  # ğŸ”¥ ä¼ é€’ attention_bias
            dropout_p=self.dropout if self.training else 0.0,
            scale=None,  # ä½¿ç”¨é»˜è®¤ scale (1/sqrt(head_dim))
            is_causal=False
        )
        # SDPA è¾“å‡º: [batch, num_heads, seq_len_q, head_dim]
        
        # è½¬å› [batch, seq_len_q, embed_dims]
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)
        
        return context
    
    def _convert_attn_bias_to_qk_bias(self, attn_bias, rank=None):
        """
        å°† attn_bias è½¬æ¢ä¸º q_bias å’Œ k_bias
        
        Args:
            attn_bias: [batch, num_heads, seq_len, seq_len] æ³¨æ„åŠ›åç½®çŸ©é˜µ
                      æˆ– [batch, seq_len, seq_len] ç®€åŒ–æ ¼å¼ï¼ˆä¼šè‡ªåŠ¨æ‰©å±•åˆ°å¤šå¤´ï¼‰
            rank: ä½ç§©è¿‘ä¼¼çš„ç§©ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            q_bias: [batch, num_heads, seq_len, rank] æŸ¥è¯¢åç½®
            k_bias: [batch, num_heads, rank, seq_len] é”®åç½®
        """
        # ğŸ”¥ SVD ä¸æ”¯æŒ FP16ï¼Œéœ€è¦è½¬ä¸º FP32
        original_dtype = attn_bias.dtype
        if attn_bias.dtype == torch.float16:
            attn_bias = attn_bias.float()
        
        # ğŸ”¥ å¤„ç† 3 ç»´è¾“å…¥ï¼ˆè‡ªåŠ¨æ‰©å±•åˆ°å¤šå¤´ï¼‰
        if attn_bias.dim() == 3:
            # [batch, seq_len_q, seq_len_k] â†’ [batch, num_heads, seq_len_q, seq_len_k]
            attn_bias = attn_bias.unsqueeze(1).expand(-1, self.num_heads, -1, -1)
        
        batch_size, num_heads, seq_len_q, seq_len_k = attn_bias.shape
        
        # ğŸ”¥ ä½¿ç”¨å›ºå®šçš„å° rank ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆé¿å… OOMï¼‰
        # åŸæ¥çš„è‡ªåŠ¨è®¡ç®— rank éå¸¸å æ˜¾å­˜ï¼Œä¸”åœ¨å¤§çŸ©é˜µä¸Šä¼šå¯¼è‡´ OOM
        if rank is None:
            # ä½¿ç”¨å›ºå®šçš„å° rankï¼Œåœ¨æ€§èƒ½å’Œæ˜¾å­˜ä¹‹é—´å–å¹³è¡¡
            # rank=8 é€šå¸¸è¶³å¤Ÿæ•æ‰ä¸»è¦çš„æ³¨æ„åŠ›æ¨¡å¼
            rank = min(8, min(seq_len_q, seq_len_k) // 4)
        
        # ğŸ”¥ åœ¨ GPU ä¸Šè¿›è¡Œ SVD åˆ†è§£ï¼ˆFlashBias ä¼šèŠ‚çœæ˜¾å­˜ï¼‰
        device = attn_bias.device
        print(f"ğŸ”„ å¼€å§‹ SVD åˆ†è§£ (GPU)ï¼šbatch={batch_size}, heads={num_heads}, seq_q={seq_len_q}, seq_k={seq_len_k}, rank={rank}")
        
        # ğŸ”¥ æ‰¹é‡ SVDï¼šå°† batch å’Œ heads ç»´åº¦åˆå¹¶
        # [batch, num_heads, seq_q, seq_k] -> [batch*num_heads, seq_q, seq_k]
        attn_bias_flat = attn_bias.reshape(batch_size * num_heads, seq_len_q, seq_len_k)
        
        # æ‰¹é‡ SVD åˆ†è§£ï¼ˆåœ¨ GPU ä¸Šï¼‰
        U, S, V = torch.svd(attn_bias_flat)
        
        # é€‰æ‹©å‰ rank ä¸ªå¥‡å¼‚å€¼
        U_trunc = U[:, :, :rank]  # [batch*num_heads, seq_len_q, rank]
        S_trunc = S[:, :rank]     # [batch*num_heads, rank]
        V_trunc = V[:, :, :rank]  # [batch*num_heads, seq_len_k, rank]
        
        # ğŸ”¥ é‡æ„åç½®ï¼šattn_bias = q_bias @ k_bias.T
        # ä½¿ç”¨ sqrt(S) æ¥åˆ†é…å¥‡å¼‚å€¼çš„æƒé‡
        sqrt_S = torch.sqrt(S_trunc)  # [batch*num_heads, rank]
        q_bias = U_trunc * sqrt_S.unsqueeze(1)  # [batch*num_heads, seq_len_q, rank]
        k_bias = V_trunc * sqrt_S.unsqueeze(1)  # [batch*num_heads, seq_len_k, rank]
        
        # ğŸ”¥ é‡æ–°ç»„ç»‡ä¸º FlashBias æœŸæœ›çš„æ ¼å¼
        # [batch*num_heads, seq_len, rank] -> [batch, num_heads, seq_len, rank]
        q_bias = q_bias.view(batch_size, num_heads, seq_len_q, rank)
        k_bias = k_bias.view(batch_size, num_heads, seq_len_k, rank)
        
        print(f"âœ… SVD åˆ†è§£å®Œæˆ")
        
        # ğŸ”¥ è½¬å›åŸå§‹ dtype
        if original_dtype == torch.float16:
            q_bias = q_bias.half()
            k_bias = k_bias.half()
        
        return q_bias, k_bias


# ğŸ”¥ ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŸæœ‰çš„ FlashMHA ç±»å
class FlashMHA(FlashBiasAttention):
    """
    ä¸ºäº†å…¼å®¹æ€§ä¿ç•™çš„åˆ«å
    """
    pass


# ğŸ”¥ ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŸæœ‰çš„ FlashAttention ç±»å  
class FlashAttention(FlashBiasAttention):
    """
    ä¸ºäº†å…¼å®¹æ€§ä¿ç•™çš„åˆ«å
    """
    pass