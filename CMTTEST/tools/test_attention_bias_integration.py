#!/usr/bin/env python
"""
æµ‹è¯• Attention Bias é›†æˆ
éªŒè¯æ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£ç¡®è¿æ¥
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def test_attention_bias_generator():
    """æµ‹è¯•AttentionBiasGenerator"""
    from projects.mmdet3d_plugin.models.utils.attention_bias_generator import AttentionBiasGenerator
    
    print("ğŸ”¥ æµ‹è¯• AttentionBiasGenerator...")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = AttentionBiasGenerator(
        bev_feature_shape=(128, 128),
        pers_feature_shape=(6, 20, 50),
        window_size=8,
        bias_scale=1.0,
        use_local_bias=True,
        fp16=False
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥
    bs, num_queries = 2, 900
    lidar_weights = torch.rand(bs, num_queries)
    camera_weights = torch.rand(bs, num_queries)
    pts_bev = torch.randint(0, 128*128, (bs, num_queries))
    pts_pers = torch.randint(0, 6*20*50, (bs, num_queries))
    
    # å‰å‘ä¼ æ’­
    attention_bias = generator(lidar_weights, camera_weights, pts_bev, pts_pers, img_metas=None)
    
    # éªŒè¯è¾“å‡º
    expected_shape = (bs, num_queries, 128*128 + 6*20*50)
    assert attention_bias.shape == expected_shape, f"å½¢çŠ¶é”™è¯¯ï¼š{attention_bias.shape} vs {expected_shape}"
    
    print(f"   âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {attention_bias.shape}")
    print(f"   âœ… BiasèŒƒå›´: [{attention_bias.min():.4f}, {attention_bias.max():.4f}]")
    print(f"   âœ… Biaså‡å€¼: {attention_bias.mean():.4f}")
    
    return True


def test_petr_multihead_attention():
    """æµ‹è¯•PETRMultiheadAttentionæ”¯æŒattention_bias"""
    from projects.mmdet3d_plugin.models.utils.petr_transformer import PETRMultiheadAttention
    
    print("\nğŸ”¥ æµ‹è¯• PETRMultiheadAttention...")
    
    # åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
    attn = PETRMultiheadAttention(
        embed_dims=256,
        num_heads=8,
        dropout=0.1
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥
    num_queries, bs, embed_dims = 900, 2, 256
    num_features = 128*128 + 6*20*50
    
    query = torch.randn(num_queries, bs, embed_dims)
    key = torch.randn(num_features, bs, embed_dims)
    value = torch.randn(num_features, bs, embed_dims)
    
    # ğŸ”¥ æµ‹è¯•ï¼šä¸ä½¿ç”¨attention_bias
    out1 = attn(query, key=key, value=value, attention_bias=None)
    print(f"   âœ… ä¸ä½¿ç”¨biasè¾“å‡ºå½¢çŠ¶: {out1.shape}")
    
    # ğŸ”¥ æµ‹è¯•ï¼šä½¿ç”¨attention_bias
    attention_bias = torch.randn(num_queries, bs, num_features)
    out2 = attn(query, key=key, value=value, attention_bias=attention_bias)
    print(f"   âœ… ä½¿ç”¨biasè¾“å‡ºå½¢çŠ¶: {out2.shape}")
    
    # éªŒè¯è¾“å‡ºä¸åŒ
    assert not torch.equal(out1, out2), "ä½¿ç”¨biasåè¾“å‡ºåº”è¯¥ä¸åŒ"
    print(f"   âœ… Attention biasç”Ÿæ•ˆï¼ˆè¾“å‡ºå‘ç”Ÿå˜åŒ–ï¼‰")
    
    return True


def test_cmt_transformer():
    """æµ‹è¯•CmtTransformeræ”¯æŒattention_bias"""
    from projects.mmdet3d_plugin.models.utils.cmt_transformer import CmtTransformer
    from projects.mmdet3d_plugin.models.utils.petr_transformer import PETRTransformerDecoder
    
    print("\nğŸ”¥ æµ‹è¯• CmtTransformer...")
    
    # åˆ›å»ºTransformer
    decoder_config = dict(
        type='PETRTransformerDecoder',
        return_intermediate=True,
        num_layers=2,
        transformerlayers=dict(
            type='PETRTransformerDecoderLayer',
            attn_cfgs=[
                dict(type='MultiheadAttention', embed_dims=256, num_heads=8),
                dict(type='MultiheadAttention', embed_dims=256, num_heads=8)
            ],
            ffn_cfgs=dict(type='FFN', embed_dims=256, feedforward_channels=1024),
            feedforward_channels=1024,
            operation_order=('self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm')
        )
    )
    
    transformer = CmtTransformer(decoder=decoder_config)
    
    # æ¨¡æ‹Ÿè¾“å…¥
    bs, c, h, w = 2, 256, 128, 128
    bev_feat = torch.randn(bs, c, h, w)
    
    bs_cam, c_cam, h_cam, w_cam = bs * 6, 256, 20, 50
    cam_feat = torch.randn(bs_cam, c_cam, h_cam, w_cam)
    
    num_queries = 900
    query_embed = torch.randn(bs, num_queries, 256)
    
    bev_pos = torch.randn(h * w, bs, 256)
    rv_pos = torch.randn(6 * h_cam * w_cam, bs, 256)
    
    # ğŸ”¥ æµ‹è¯•ï¼šä¸ä½¿ç”¨attention_bias
    out1, _ = transformer(bev_feat, cam_feat, query_embed, bev_pos, rv_pos, attention_bias=None)
    print(f"   âœ… ä¸ä½¿ç”¨biasè¾“å‡ºå½¢çŠ¶: {out1.shape}")
    
    # ğŸ”¥ æµ‹è¯•ï¼šä½¿ç”¨attention_bias
    attention_bias = torch.randn(bs, num_queries, h*w + 6*h_cam*w_cam)
    out2, _ = transformer(bev_feat, cam_feat, query_embed, bev_pos, rv_pos, attention_bias=attention_bias)
    print(f"   âœ… ä½¿ç”¨biasè¾“å‡ºå½¢çŠ¶: {out2.shape}")
    
    # éªŒè¯è¾“å‡ºä¸åŒ
    assert not torch.equal(out1, out2), "ä½¿ç”¨biasåè¾“å‡ºåº”è¯¥ä¸åŒ"
    print(f"   âœ… Attention biasåœ¨Transformerä¸­ç”Ÿæ•ˆ")
    
    return True


def test_integration():
    """é›†æˆæµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹ Attention Bias é›†æˆæµ‹è¯•")
    print("="*60 + "\n")
    
    try:
        # æµ‹è¯•å„ä¸ªç»„ä»¶
        test_attention_bias_generator()
        test_petr_multihead_attention()
        test_cmt_transformer()
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Attention Biasé›†æˆæˆåŠŸï¼")
        print("="*60)
        print("\nğŸ“ åç»­æ­¥éª¤ï¼š")
        print("   1. åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ enable_aqr=True")
        print("   2. æ·»åŠ  attention_bias_config é…ç½®")
        print("   3. è¿è¡Œè®­ç»ƒè„šæœ¬éªŒè¯ç«¯åˆ°ç«¯æµç¨‹")
        print("   4. å¯¹æ¯”æ—§æ–¹æ¡ˆï¼ˆç‰¹å¾è°ƒåˆ¶ï¼‰å’Œæ–°æ–¹æ¡ˆï¼ˆAttention Biasï¼‰çš„æ€§èƒ½")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = test_integration()
    sys.exit(0 if success else 1)

