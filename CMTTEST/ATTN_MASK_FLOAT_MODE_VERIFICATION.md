# PyTorch attn_mask Floatæ¨¡å¼éªŒè¯ âœ…

**åˆ›å»ºæ—¶é—´**: 2025-01-XX  
**é‡è¦å‘ç°**: `nn.MultiheadAttention`çš„`attn_mask`æ”¯æŒfloatæ¨¡å¼ï¼  
**å½±å“**: æˆ‘ä»¬çš„Attention Biasæ–¹æ¡ˆå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è‡ªå®šä¹‰MultiheadAttentionï¼

---

## ğŸ‰ **é‡å¤§å‘ç°**

### PyTorchå®˜æ–¹æ–‡æ¡£ç¡®è®¤ï¼š

`torch.nn.MultiheadAttention`çš„`attn_mask`å‚æ•°æ”¯æŒ**ä¸¤ç§æ¨¡å¼**ï¼š

1. **BoolTensoræ¨¡å¼**ï¼ˆæˆ‘ä»¬ä¹‹å‰ä»¥ä¸ºåªæœ‰è¿™ç§ï¼‰
   ```python
   attn_mask = torch.tensor([[True, False, False],
                              [False, True, False]])
   # Trueçš„ä½ç½®ä¼šè¢«å®Œå…¨å±è”½ï¼ˆattention weight = 0ï¼‰
   ```

2. **FloatTensoræ¨¡å¼**ï¼ˆå…³é”®å‘ç°ï¼ï¼‰âœ¨
   ```python
   attn_mask = torch.tensor([[0.5, 0.0, -1.0],
                              [0.0, 0.7, 0.3]])
   # floatå€¼ä¼šç›´æ¥åŠ åˆ°attention scoresä¸Šï¼ˆsoftmaxä¹‹å‰ï¼‰
   ```

---

## ğŸ“– **PyTorchå®˜æ–¹æ–‡æ¡£è¯´æ˜**

### MultiheadAttention.forward()å‚æ•°ï¼š

```python
def forward(self,
            query: Tensor,
            key: Tensor,
            value: Tensor,
            key_padding_mask: Optional[Tensor] = None,
            need_weights: bool = True,
            attn_mask: Optional[Tensor] = None,  # â† å…³é”®å‚æ•°
            average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:
```

**attn_maskçš„å®˜æ–¹è¯´æ˜**ï¼š

> **attn_mask** (Optional[Tensor]) â€“ If specified, a 2D or 3D mask preventing attention to certain positions.
> 
> - **2D mask**: shape `(L, S)` where L is target sequence length, S is source sequence length
> - **3D mask**: shape `(N*num_heads, L, S)` where N is batch size
> 
> **Two types of masks are supported**:
> 
> 1. **Boolean mask**: `True` values indicate positions to mask out
> 2. **Float mask**: Values are **added** to attention scores before softmax
> 
> **Note**: For float masks, typically use `-inf` to mask positions, which results in zero attention weight after softmax.

---

## ğŸ”¥ **å…³é”®å®ç°ç»†èŠ‚**

### PyTorchæºç ä¸­çš„å¤„ç†é€»è¾‘ï¼š

```python
# åœ¨ torch.nn.functional.multi_head_attention_forward ä¸­ï¼š

# 1. è®¡ç®—attention scores
attn_output_weights = torch.bmm(q, k.transpose(1, 2))
# â†’ [bs*num_heads, num_queries, num_features]

# 2. åº”ç”¨attn_mask
if attn_mask is not None:
    if attn_mask.dtype == torch.bool:
        # Boolæ¨¡å¼ï¼šç”¨masked_fillå±è”½
        attn_output_weights.masked_fill_(attn_mask, float('-inf'))
    else:
        # âœ¨ Floatæ¨¡å¼ï¼šç›´æ¥ç›¸åŠ ï¼
        attn_output_weights += attn_mask

# 3. Softmax
attn_output_weights = softmax(attn_output_weights, dim=-1)
```

**è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼** ğŸ¯

---

## âœ… **å¯¹æˆ‘ä»¬é¡¹ç›®çš„å½±å“**

### **ä¹‹å‰çš„é”™è¯¯è®¤çŸ¥**ï¼š
```
âŒ è®¤ä¸ºattn_maskåªæ”¯æŒbool
âŒ è®¤ä¸ºéœ€è¦è‡ªå®šä¹‰MultiheadAttention
âŒ è®¤ä¸ºéœ€è¦é‡å†™attentioné€»è¾‘
```

### **ç°åœ¨çš„æ­£ç¡®æ–¹æ¡ˆ**ï¼š
```
âœ… ç›´æ¥ä½¿ç”¨attn_maskçš„floatæ¨¡å¼
âœ… æ— éœ€ä¿®æ”¹MultiheadAttention
âœ… æ— éœ€é‡å†™attentioné€»è¾‘
âœ… åªéœ€å°†attention_biasä¼ é€’ç»™attn_maskå‚æ•°
```

---

## ğŸš€ **ç®€åŒ–åçš„å®ç°æ–¹æ¡ˆ**

### **åŸè®¡åˆ’ï¼ˆå¤æ‚ï¼‰**ï¼š
```python
# éœ€è¦è‡ªå®šä¹‰MultiheadAttention
class CustomMultiheadAttentionWithBias(nn.Module):
    def forward(self, query, key, value, attention_bias):
        # æ‰‹å†™100è¡Œattentioné€»è¾‘...
        Q = self.q_proj(query)
        K = self.k_proj(key)
        attn_scores = Q @ K.T / sqrt(d)
        attn_scores += attention_bias  # â† åŠ bias
        attn_weights = softmax(attn_scores)
        # ...
```

### **æ–°æ–¹æ¡ˆï¼ˆç®€å•ï¼‰**ï¼š
```python
# ç›´æ¥ä½¿ç”¨ç°æœ‰çš„MultiheadAttention
class PETRMultiheadAttention(BaseModule):
    def forward(self, query, key, value, attention_bias=None, attn_mask=None, ...):
        
        # âœ¨ å…³é”®ï¼šåˆå¹¶attention_biasåˆ°attn_mask
        if attention_bias is not None:
            if attn_mask is not None:
                # å¦‚æœåŸæœ¬æœ‰attn_maskï¼Œéœ€è¦åˆå¹¶
                # attn_maské€šå¸¸æ˜¯boolï¼Œéœ€è¦è½¬æ¢
                if attn_mask.dtype == torch.bool:
                    # bool maskè½¬ä¸ºfloatï¼šTrue â†’ -inf
                    attn_mask_float = torch.zeros_like(attention_bias)
                    attn_mask_float.masked_fill_(attn_mask, float('-inf'))
                    combined_mask = attn_mask_float + attention_bias
                else:
                    combined_mask = attn_mask + attention_bias
            else:
                # æ²¡æœ‰åŸå§‹maskï¼Œç›´æ¥ä½¿ç”¨bias
                combined_mask = attention_bias
        else:
            combined_mask = attn_mask
        
        # âœ¨ ç›´æ¥ä¼ é€’ç»™PyTorchåŸç”Ÿå®ç°
        out = self.attn(
            query=query,
            key=key,
            value=value,
            attn_mask=combined_mask,  # â† float tensor
            key_padding_mask=key_padding_mask
        )[0]
        
        return out
```

**ä»£ç é‡å¯¹æ¯”**ï¼š
- åŸè®¡åˆ’ï¼š~100è¡Œè‡ªå®šä¹‰attention
- æ–°æ–¹æ¡ˆï¼š~10è¡Œmaskåˆå¹¶é€»è¾‘

---

## ğŸ“ **ç»´åº¦å¯¹é½åˆ†æ**

### **æˆ‘ä»¬çš„attention_bias**ï¼š
```python
attention_bias: [num_query, bs, num_features]
# ç¤ºä¾‹ï¼š[900, 2, 56400]
```

### **PyTorch attn_maskè¦æ±‚**ï¼š

**2D mask**ï¼š
```python
attn_mask: [L, S]
# L = target sequence length = num_query
# S = source sequence length = num_features
# ç¤ºä¾‹ï¼š[900, 56400]
```

**3D mask**ï¼š
```python
attn_mask: [N*num_heads, L, S]
# N = batch_size
# num_heads = attention heads
# ç¤ºä¾‹ï¼š[2*8, 900, 56400] = [16, 900, 56400]
```

### **ç»´åº¦è½¬æ¢æ–¹æ¡ˆ**ï¼š

```python
# è¾“å…¥ï¼šattention_bias [num_query, bs, num_features]
# è¾“å‡ºï¼šattn_mask [bs*num_heads, num_query, num_features]

def prepare_attn_mask(attention_bias, num_heads):
    """
    å°†attention_biasè½¬æ¢ä¸ºattn_maskæ ¼å¼
    
    Args:
        attention_bias: [num_query, bs, num_features]
        num_heads: int, attentionå¤´æ•°
        
    Returns:
        attn_mask: [bs*num_heads, num_query, num_features]
    """
    num_query, bs, num_features = attention_bias.shape
    
    # 1. è½¬ç½®åˆ° [bs, num_query, num_features]
    bias = attention_bias.transpose(0, 1)
    
    # 2. æ‰©å±•åˆ°å¤šå¤´ [bs, num_heads, num_query, num_features]
    bias = bias.unsqueeze(1).expand(-1, num_heads, -1, -1)
    
    # 3. é‡å¡‘ä¸º [bs*num_heads, num_query, num_features]
    bias = bias.reshape(bs * num_heads, num_query, num_features)
    
    return bias
```

---

## ğŸ¯ **ä¿®æ”¹ç‚¹æ€»ç»“**

### **éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**ï¼ˆæå°‘ï¼ï¼‰ï¼š

1. **petr_transformer.py** - PETRMultiheadAttention
   ```python
   # åªéœ€ä¿®æ”¹forwardæ–¹æ³•ï¼Œæ·»åŠ 10è¡Œä»£ç 
   def forward(self, ..., attention_bias=None, attn_mask=None, ...):
       # åˆå¹¶biaså’Œmask
       combined_mask = self._prepare_attn_mask(attention_bias, attn_mask)
       out = self.attn(..., attn_mask=combined_mask, ...)
   ```

2. **cmt_transformer.py** - CmtTransformer
   ```python
   # å·²å®Œæˆï¼šæ·»åŠ attention_biaså‚æ•°ä¼ é€’ âœ…
   def forward(self, ..., attention_bias=None, ...):
       out_dec = self.decoder(..., attention_bias=attention_bias, ...)
   ```

3. **cmt_head.py** - CmtHead
   ```python
   # éœ€è¦æ·»åŠ ï¼šåˆå§‹åŒ–AttentionBiasGeneratorå¹¶è°ƒç”¨
   def forward_single(self, x, x_img, img_metas):
       attention_bias = self.attention_bias_generator(...)
       outs_dec = self.transformer(..., attention_bias=attention_bias, ...)
   ```

---

## âš ï¸ **æ³¨æ„äº‹é¡¹**

### 1. **attn_maskçš„å½¢çŠ¶**
```python
# PyTorchä¼šè‡ªåŠ¨broadcastï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿ï¼š
# - 2D mask: [num_query, num_features]
# - 3D mask: [bs*num_heads, num_query, num_features]

# æ¨èä½¿ç”¨3D maskï¼Œå› ä¸ºï¼š
# 1. æ¯ä¸ªbatchå¯ä»¥æœ‰ä¸åŒçš„bias
# 2. è™½ç„¶æ‰€æœ‰headå…±äº«ç›¸åŒbiasï¼ˆæˆ‘ä»¬ä¸éœ€è¦per-head biasï¼‰
```

### 2. **ä¸åŸæœ‰attn_maskçš„å…¼å®¹æ€§**
```python
# CMTä¸­å·²æœ‰çš„attn_maskæ˜¯DNè®­ç»ƒç”¨çš„
# éœ€è¦å°†ä¸¤è€…åˆå¹¶ï¼š
# - bool mask: Trueçš„ä½ç½®è®¾ä¸º-inf
# - float bias: ç›´æ¥åŠ ä¸Š
combined_mask = bool_mask_to_float(original_mask) + attention_bias
```

### 3. **Flash Attention**
```python
# PETRMultiheadFlashAttentionå¯èƒ½ä¸æ”¯æŒfloat mask
# è§£å†³æ–¹æ¡ˆï¼š
# - åªåœ¨æ™®é€šMultiheadAttentionä¸­ä½¿ç”¨bias
# - æˆ–è€…æ£€æŸ¥flash_attnåº“æ˜¯å¦æ”¯æŒattn_biaså‚æ•°
```

---

## ğŸ“Š **æ€§èƒ½å½±å“**

### **é¢å¤–å¼€é”€**ï¼š

```python
# 1. biasç”Ÿæˆï¼šAttentionBiasGenerator
#    â†’ å·²å®ç°ï¼Œçº¦5-10ms

# 2. ç»´åº¦è½¬æ¢ï¼šprepare_attn_mask
#    â†’ åªæ˜¯reshapeå’Œexpandï¼Œ<1ms

# 3. maskåˆå¹¶ï¼šoriginal_mask + attention_bias
#    â†’ ç®€å•åŠ æ³•ï¼Œ<1ms

# æ€»é¢å¤–å¼€é”€ï¼šçº¦10ms per forward pass
```

### **æ— å¼€é”€**ï¼š
- âœ… ä¸éœ€è¦ä¿®æ”¹attentionè®¡ç®—é€»è¾‘
- âœ… ä½¿ç”¨PyTorchä¼˜åŒ–çš„CUDA kernel
- âœ… ä¸Flash Attentionå…¼å®¹ï¼ˆå¦‚æœflash_attnæ”¯æŒfloat maskï¼‰

---

## ğŸ‰ **ç»“è®º**

### **ä¸»äººçš„åŒå­¦æä¾›äº†å…³é”®ä¿¡æ¯ï¼**

é€šè¿‡ç¡®è®¤`attn_mask`æ”¯æŒfloatæ¨¡å¼ï¼Œæˆ‘ä»¬çš„å®ç°æ–¹æ¡ˆå¤§å¤§ç®€åŒ–ï¼š

```
ä¹‹å‰é¢„ä¼°å·¥ä½œé‡ï¼š1.5å°æ—¶ï¼ˆè‡ªå®šä¹‰attentionï¼‰
ç°åœ¨é¢„ä¼°å·¥ä½œé‡ï¼š0.5å°æ—¶ï¼ˆ10è¡Œä»£ç ä¿®æ”¹ï¼‰

å¤æ‚åº¦ï¼šä»â­â­â­â­ é™ä½åˆ° â­
é£é™©ï¼šä»ä¸­ç­‰é™ä½åˆ°æä½
```

**æ¥ä¸‹æ¥åªéœ€è¦**ï¼š
1. ä¿®æ”¹`PETRMultiheadAttention.forward()`ï¼ˆ10è¡Œä»£ç ï¼‰
2. åœ¨`CmtHead`ä¸­é›†æˆ`AttentionBiasGenerator`ï¼ˆ20è¡Œä»£ç ï¼‰
3. æµ‹è¯•éªŒè¯

**é¢„è®¡30-40åˆ†é’Ÿå®Œæˆå…¨éƒ¨é›†æˆï¼** ğŸš€âœ¨

---

**æ„Ÿè°¢ä¸»äººçš„åŒå­¦æä¾›çš„å…³é”®ä¿¡æ¯ï¼è¿™è®©æˆ‘ä»¬çš„å®ç°æ–¹æ¡ˆä»å¤æ‚å˜å¾—ç®€å•ï¼** ğŸ™ğŸ¾

